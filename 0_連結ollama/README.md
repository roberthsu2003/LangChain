# Olloma

## å®‰è£olloma(æ–¹æ³•1):æœ¬æ©Ÿå®‰è£olloma

[å®˜ç¶²å®‰è£ç¶²å€](https://ollama.com/)

> [åƒè€ƒå®˜ç¶²çš„å‘½ä»¤åˆ—æ“ä½œæŒ‡ä»¤(CLI)](https://github.com/ollama/ollama/blob/main/README.md)

## å®‰è£olloma(æ–¹æ³•2):dockerå…§å®‰è£olloma

[å®˜ç¶²èªªæ˜](https://hub.docker.com/r/ollama/ollama)

### CUP only

```bash
docker run -d -v ollama:/root/.ollama -p 11434:11434 --name ollama ollama/ollama
```

#### åŸ·è¡Œæ¨¡å‹
- é€™è£æ˜¯åŸ·è¡ŒMetaçš„llama3:3b

```bash
docker exec -it ollama ollama run llama3:3b
```

### devcontainer.jsonå¿…éœ€åŠ ä¸Šé€™ä¸€è¡Œæ‰å¯ä»¥å®‰å…¨åŸ·è¡Œ

```
//ä½¿ç”¨--newwork=host,ä»£è¡¨ç›´æ¥ä½¿ç”¨hostçš„ç¶²è·¯
	"runArgs": ["--network=host","--name","python_langchain"]
```

---

## Ollama python API

### [ollama python APIå®˜æ–¹èªªæ˜é€£çµ](https://github.com/ollama/ollama-python)

### å®‰è£api

```bash
pip install ollama
```


### ä½¿ç”¨ollamaçš„api
#### æ–¹æ³•1:æ‡‰ç”¨ç¨‹å¼å’Œollamaéƒ½åœ¨æœ¬æ©Ÿç«¯

```python
#ä½¿ç”¨ollamçš„api
from ollama import chat
from ollama import ChatResponse
response: ChatResponse = chat(model='llama3.2:3b', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
print(response['message']['content'])
# or access fields directly from the response object
print(response.message.content)
```

### ä½¿ç”¨ollamaçš„api
#### æ–¹æ³•2:æ‡‰ç”¨ç¨‹å¼åœ¨dockerå®¹å™¨å…§,ollamaåœ¨ä¸»æ©Ÿä¸Š

```python
from ollama import Client

client = Client(
  host='http://host.docker.internal:11434',
  headers={'x-some-header': 'some-value'}
)

response = client.chat(model='gpt-oss:20b', messages=[
  {
    'role': 'user',
    'content': 'Why is the sky blue?',
  },
])
print(response['message']['content'])
```

#### æ–¹æ³•3:ä½¿ç”¨Ollama REST API

[Ollama RESTã€€API å®˜ç¶²èªªæ˜](https://github.com/ollama/ollama/blob/main/docs/api.md)

> å®˜ç¶²èªªæ˜å®Œæ•´,å»ºè­°ä¾å®˜ç¶²èªªæ˜ç·´ç¿’ä¸‹é¢ç¨‹å¼ç¢¼

#### Generate Mode(ç”Ÿæˆæ¨¡å¼)

**ç¯„ä¾‹1 Request (No streaming)**

- lesson1.ipynb

**Request**

- stream:å¿…éœ€è¨­å®šç‚ºfalse

**Response**

- å› ç‚ºstreamæ˜¯false,responseç‚ºjson object

```python
import requests

def chat_with_ollama(prompt: str):
    url = "http://host.docker.internal:11434/api/generate"
    payload = {
        "model": "llama3.2:latest",
        "prompt": prompt,
        "stream": False,
        "options": { #åƒè€ƒèªªæ˜1
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
        },
        "max_tokens": 100,
        "format": "json",
    }

    response = requests.post(url, json=payload)
    result = response.json()
    print("ğŸ’¬ AI å›æ‡‰ï¼š")
    # Print the whole result for debugging
    print(result)
    # Try to print the 'response' key if it exists, otherwise print possible keys
    if "response" in result:
        print(result["response"])
    elif "message" in result:
        print(result["message"])
    elif "content" in result:
        print(result["content"])
    else:
        print("No expected key found in response. Available keys:", result.keys())

#ç¯„ä¾‹è¼¸å…¥
chat_with_ollama("è«‹ç”¨ç°¡å–®çš„æ–¹å¼è§£é‡‹ä»€éº¼æ˜¯Pythonçš„å‡½å¼ï¼Ÿ")
```

#### Generate(ç”Ÿæˆæ¨¡å¼-No streaming)

**ç¯„ä¾‹2 Request (No streaming)**

- lesson1.ipynb

```python
def chat_with_ollama(prompt: str):
    url = "http://host.docker.internal:11434/api/generate"
    payload = {
        "model": "llama3.2:latest",
        "prompt": prompt,
        "stream": False,
        "options": { #åƒè€ƒèªªæ˜1
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
        },
        "max_tokens": 100,
        "format": "json",
    }

    response = requests.post(url, json=payload)
    result = response.json()
    print("ğŸ’¬ AI å›æ‡‰ï¼š")
    # Print the whole result for debugging
    print(result)
    # Try to print the 'response' key if it exists, otherwise print possible keys
    if "response" in result:
        print(result["response"])
    elif "message" in result:
        print(result["message"])
    elif "content" in result:
        print(result["content"])
    else:
        print("No expected key found in response. Available keys:", result.keys())

    
def chat_loop():
    print("æ­¡è¿ä½¿ç”¨æœ¬åœ°ç«¯ LLM èŠå¤©æ©Ÿå™¨äººï¼ˆè¼¸å…¥ q é›¢é–‹ï¼‰")
    while True:
        user_input = input("ğŸ‘¤ ä½ èªªï¼š")
        if user_input.lower() == 'q':
            break
        chat_with_ollama(user_input)

chat_loop()
```

#### chat mode(èŠå¤©æ¨¡å¼-streaming)

- lesson2.ipynb

```python
import requests
import json

def chat_with_ollama(prompt: str):
    url = "http://host.docker.internal:11434/api/chat"
    payload = {
        "model": "llama3.2:latest",
        "messages": [
            {
                "role": "user",
                "content": prompt
            }
        ],
        "stream": True,
        "options": {
            "temperature": 0.7,
            "top_p": 0.9,
            "top_k": 50,
        }
    }

    print("ğŸ’¬ AI å›æ‡‰ï¼š", end="", flush=True)
    
    try:
        response = requests.post(url, json=payload, stream=True)
        response.raise_for_status()
        
        for line in response.iter_lines():
            if line:
                try:
                    chunk = json.loads(line.decode('utf-8'))
                    
                    # æª¢æŸ¥æ˜¯å¦æœ‰è¨Šæ¯å…§å®¹
                    if 'message' in chunk and 'content' in chunk['message']:
                        content = chunk['message']['content']
                        print(content, end="", flush=True)
                    
                    # æª¢æŸ¥æ˜¯å¦å®Œæˆ
                    if chunk.get('done', False):
                        print()  # æ›è¡Œ
                        break
                        
                except json.JSONDecodeError:
                    continue
                    
    except requests.exceptions.RequestException as e:
        print(f"\nâŒ è«‹æ±‚éŒ¯èª¤: {e}")
    except Exception as e:
        print(f"\nâŒ è™•ç†éŒ¯èª¤: {e}")

def chat_loop():
    print("æ­¡è¿ä½¿ç”¨æœ¬åœ°ç«¯ LLM èŠå¤©æ©Ÿå™¨äººï¼ˆè¼¸å…¥ q é›¢é–‹ï¼‰")
    while True:
        user_input = input("ğŸ‘¤ ä½ èªªï¼š")
        if user_input.lower() == 'q':
            break
        chat_with_ollama(user_input)
        print()  # ç©ºè¡Œåˆ†éš”

chat_loop()
```




### ä½¿ç”¨langchain apiå‘¼å«ollama

```
from langchain_core.prompts import ChatPromptTemplate
from langchain_ollama.llms import OllamaLLM
template = """Question: {question}

Answer: Let's think step by step."""

prompt = ChatPromptTemplate.from_template(template)

model = OllamaLLM(model="llama3.2:3b")

chain = prompt | model

chain.invoke({"question": "What is LangChain?"})
```

