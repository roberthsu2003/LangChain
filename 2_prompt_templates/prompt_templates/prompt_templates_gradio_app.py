"""
LangChain Prompt Templates Gradio æ‡‰ç”¨ç¨‹å¼
å±•ç¤ºå„ç¨® Prompt Template çš„ä½¿ç”¨æ–¹å¼å’Œæ•ˆæœ
æ”¯æ´å¤šç¨® AI æ¨¡å‹å’Œæ¨¡æ¿é¡å‹
"""

import gradio as gr
import os
import json
import socket
from datetime import datetime
from typing import List, Dict, Any, Optional
from dotenv import load_dotenv

# LangChain imports
from langchain.prompts import PromptTemplate, ChatPromptTemplate
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_ollama import ChatOllama
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_google_genai import ChatGoogleGenerativeAI

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

class PromptTemplatesManager:
    """ç®¡ç† Prompt Templates çš„é¡åˆ¥"""
    
    def __init__(self):
        self.models = {}
        self.current_model = None
        self.templates = {}
        self.template_history = []
        
        # åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹
        self._initialize_models()
        
        # åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ¨¡æ¿
        self._initialize_templates()
    
    def _initialize_models(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ¨¡å‹"""
        try:
            # Ollama æ¨¡å‹
            self.models["Ollama (llama3.2)"] = ChatOllama(
                model="llama3.2:latest", 
                base_url="http://localhost:11434"
            )
        except Exception as e:
            print(f"ç„¡æ³•åˆå§‹åŒ– Ollama æ¨¡å‹: {e}")
        
        try:
            # Google Gemini æ¨¡å‹
            if os.getenv("GOOGLE_API_KEY"):
                self.models["Gemini (gemini-2.5-flash)"] = ChatGoogleGenerativeAI(
                    model="gemini-2.5-flash"
                )
        except Exception as e:
            print(f"ç„¡æ³•åˆå§‹åŒ– Gemini æ¨¡å‹: {e}")
        
        try:
            # OpenAI æ¨¡å‹
            if os.getenv("OPENAI_API_KEY"):
                self.models["OpenAI (gpt-4o-mini)"] = ChatOpenAI(
                    model="gpt-4o-mini"
                )
        except Exception as e:
            print(f"ç„¡æ³•åˆå§‹åŒ– OpenAI æ¨¡å‹: {e}")
        
        try:
            # Anthropic æ¨¡å‹
            if os.getenv("ANTHROPIC_API_KEY"):
                self.models["Anthropic (claude-3-5-sonnet)"] = ChatAnthropic(
                    model="claude-3-5-sonnet-latest"
                )
        except Exception as e:
            print(f"ç„¡æ³•åˆå§‹åŒ– Anthropic æ¨¡å‹: {e}")
        
        # è¨­å®šé è¨­æ¨¡å‹
        if self.models:
            self.current_model = list(self.models.keys())[0]
    
    def _initialize_templates(self):
        """åˆå§‹åŒ–æ‰€æœ‰å¯ç”¨çš„æ¨¡æ¿"""
        
        # 1. ç¿»è­¯æ¨¡æ¿
        self.templates["å°ˆæ¥­ç¿»è­¯"] = {
            "name": "å°ˆæ¥­ç¿»è­¯",
            "description": "å°‡è‹±æ–‡ç¿»è­¯æˆç¹é«”ä¸­æ–‡",
            "template": """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¹é«”ä¸­æ–‡ç¿»è­¯å®¶ï¼Œå…·æœ‰è±å¯Œçš„èªè¨€å­¸èƒŒæ™¯ã€‚
è«‹å°‡ä½¿ç”¨è€…æä¾›çš„ä»¥ä¸‹è‹±æ–‡å¥å­ç¿»è­¯æˆæµæš¢ã€è‡ªç„¶çš„ç¹é«”ä¸­æ–‡ã€‚

è‹±æ–‡å¥å­ï¼š{english_sentence}
ç¹é«”ä¸­æ–‡ç¿»è­¯ï¼š
""",
            "input_variables": ["english_sentence"],
            "example": "Hello, how are you today?"
        }
        
        # 2. æ‘˜è¦æ¨¡æ¿
        self.templates["æ–‡ç« æ‘˜è¦"] = {
            "name": "æ–‡ç« æ‘˜è¦",
            "description": "å°‡é•·ç¯‡æ–‡ç« æ‘˜è¦æˆé‡é»",
            "template": """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å…§å®¹åˆ†æå¸«ï¼Œæ“…é•·æå–æ–‡ç« çš„æ ¸å¿ƒè¦é»ã€‚
è«‹å°‡ä»¥ä¸‹æ–‡ç« æ‘˜è¦æˆ 3-5 å€‹é‡é»ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

æ–‡ç« å…§å®¹ï¼š
{article_content}

é‡é»æ‘˜è¦ï¼š
""",
            "input_variables": ["article_content"],
            "example": "äººå·¥æ™ºæ…§æ­£åœ¨æ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»æ–¹å¼..."
        }
        
        # 3. ç¨‹å¼ç¢¼è§£é‡‹æ¨¡æ¿
        self.templates["ç¨‹å¼ç¢¼è§£é‡‹"] = {
            "name": "ç¨‹å¼ç¢¼è§£é‡‹",
            "description": "è§£é‡‹ç¨‹å¼ç¢¼çš„åŠŸèƒ½å’Œé‚è¼¯",
            "template": """
ä½ æ˜¯ä¸€ä½è³‡æ·±çš„è»Ÿé«”å·¥ç¨‹å¸«ï¼Œæ“…é•·ç¨‹å¼ç¢¼åˆ†æå’Œè§£é‡‹ã€‚
è«‹è©³ç´°è§£é‡‹ä»¥ä¸‹ç¨‹å¼ç¢¼çš„åŠŸèƒ½ã€é‚è¼¯å’Œç”¨é€”ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

ç¨‹å¼ç¢¼ï¼š
{code_content}

ç¨‹å¼ç¢¼è§£é‡‹ï¼š
""",
            "input_variables": ["code_content"],
            "example": "def fibonacci(n):\n    if n <= 1:\n        return n\n    return fibonacci(n-1) + fibonacci(n-2)"
        }
        
        # 4. å‰µæ„å¯«ä½œæ¨¡æ¿
        self.templates["å‰µæ„å¯«ä½œ"] = {
            "name": "å‰µæ„å¯«ä½œ",
            "description": "æ ¹æ“šä¸»é¡Œå‰µä½œå‰µæ„å…§å®¹",
            "template": """
ä½ æ˜¯ä¸€ä½å¯Œæœ‰å‰µæ„çš„ä½œå®¶ï¼Œæ“…é•·å„ç¨®æ–‡é«”çš„å‰µä½œã€‚
è«‹æ ¹æ“šä»¥ä¸‹ä¸»é¡Œå‰µä½œä¸€æ®µå‰µæ„å…§å®¹ï¼Œå¯ä»¥æ˜¯æ•…äº‹ã€è©©æ­Œã€æ•£æ–‡ç­‰ï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

ä¸»é¡Œï¼š{writing_topic}

å‰µæ„å…§å®¹ï¼š
""",
            "input_variables": ["writing_topic"],
            "example": "æœªä¾†åŸå¸‚çš„äº¤é€š"
        }
        
        # 5. å•é¡Œè§£ç­”æ¨¡æ¿
        self.templates["å•é¡Œè§£ç­”"] = {
            "name": "å•é¡Œè§£ç­”",
            "description": "å›ç­”å„ç¨®å•é¡Œä¸¦æä¾›è©³ç´°è§£é‡‹",
            "template": """
ä½ æ˜¯ä¸€ä½çŸ¥è­˜æ·µåšçš„å°ˆå®¶ï¼Œèƒ½å¤ å›ç­”å„ç¨®å•é¡Œä¸¦æä¾›è©³ç´°çš„è§£é‡‹ã€‚
è«‹è©³ç´°å›ç­”ä»¥ä¸‹å•é¡Œï¼Œæä¾›æº–ç¢ºã€æœ‰ç”¨çš„è³‡è¨Šï¼Œä½¿ç”¨ç¹é«”ä¸­æ–‡ã€‚

å•é¡Œï¼š{question}

è©³ç´°å›ç­”ï¼š
""",
            "input_variables": ["question"],
            "example": "ä»€éº¼æ˜¯é‡å­è¨ˆç®—ï¼Ÿ"
        }
        
        # 6. å¤šè®Šæ•¸è¤‡é›œæ¨¡æ¿
        self.templates["å¤šè®Šæ•¸æ¨¡æ¿"] = {
            "name": "å¤šè®Šæ•¸æ¨¡æ¿",
            "description": "åŒ…å«å¤šå€‹è®Šæ•¸çš„è¤‡é›œæ¨¡æ¿",
            "template": """
ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„{role}ï¼Œå…·æœ‰{experience}å¹´çš„ç¶“é©—ã€‚
è«‹æ ¹æ“šä»¥ä¸‹è¦æ±‚è™•ç†{task_type}ï¼š

è¦æ±‚ï¼š{requirements}
è¼¸å…¥å…§å®¹ï¼š{input_content}
è¼¸å‡ºæ ¼å¼ï¼š{output_format}

è™•ç†çµæœï¼š
""",
            "input_variables": ["role", "experience", "task_type", "requirements", "input_content", "output_format"],
            "example": "role=ç¿»è­¯å“¡, experience=10, task_type=æ–‡ä»¶ç¿»è­¯, requirements=ä¿æŒå°ˆæ¥­è¡“èªæº–ç¢ºæ€§, input_content=Hello World, output_format=ç¹é«”ä¸­æ–‡"
        }
    
    def get_available_models(self) -> List[str]:
        """å–å¾—å¯ç”¨çš„æ¨¡å‹åˆ—è¡¨"""
        return list(self.models.keys())
    
    def get_available_templates(self) -> List[str]:
        """å–å¾—å¯ç”¨çš„æ¨¡æ¿åˆ—è¡¨"""
        return list(self.templates.keys())
    
    def get_template_info(self, template_name: str) -> Dict[str, Any]:
        """å–å¾—æ¨¡æ¿è³‡è¨Š"""
        return self.templates.get(template_name, {})
    
    def set_model(self, model_name: str) -> str:
        """åˆ‡æ›æ¨¡å‹"""
        if model_name in self.models:
            self.current_model = model_name
            return f"âœ… å·²åˆ‡æ›è‡³ {model_name}"
        return f"âŒ æ¨¡å‹ {model_name} ä¸å¯ç”¨"
    
    def process_template(self, template_name: str, input_data: Dict[str, str], model_name: str = None) -> Dict[str, Any]:
        """è™•ç†æ¨¡æ¿ä¸¦å–å¾—å›æ‡‰"""
        if template_name not in self.templates:
            return {"error": f"æ¨¡æ¿ {template_name} ä¸å­˜åœ¨"}
        
        # ä½¿ç”¨æŒ‡å®šçš„æ¨¡å‹æˆ–ç•¶å‰æ¨¡å‹
        selected_model = model_name if model_name and model_name in self.models else self.current_model
        
        if not selected_model or selected_model not in self.models:
            return {"error": "æ²’æœ‰å¯ç”¨çš„æ¨¡å‹ï¼Œè«‹æª¢æŸ¥æ¨¡å‹è¨­å®š"}
        
        try:
            # å–å¾—æ¨¡æ¿è³‡è¨Š
            template_info = self.templates[template_name]
            
            # å»ºç«‹ PromptTemplate
            prompt_template = PromptTemplate(
                input_variables=template_info["input_variables"],
                template=template_info["template"]
            )
            
            # æ ¼å¼åŒ–æç¤º
            formatted_prompt = prompt_template.format(**input_data)
            
            # å‘¼å«æ¨¡å‹
            model = self.models[selected_model]
            response = model.invoke(formatted_prompt)
            
            # å–å¾—å›æ‡‰å…§å®¹
            if hasattr(response, 'content'):
                ai_response = response.content
            else:
                ai_response = str(response)
            
            # ä¿å­˜è™•ç†è¨˜éŒ„
            record = {
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "template": template_name,
                "model": selected_model,
                "input_data": input_data,
                "formatted_prompt": formatted_prompt,
                "response": ai_response
            }
            self.template_history.append(record)
            
            return {
                "success": True,
                "formatted_prompt": formatted_prompt,
                "response": ai_response,
                "template_info": template_info
            }
            
        except Exception as e:
            error_msg = f"âŒ ç™¼ç”ŸéŒ¯èª¤: {str(e)}"
            print(error_msg)
            return {"error": error_msg}
    
    def get_template_history(self) -> List[Dict[str, Any]]:
        """å–å¾—æ¨¡æ¿è™•ç†æ­·å²"""
        return self.template_history
    
    def clear_history(self):
        """æ¸…é™¤æ­·å²è¨˜éŒ„"""
        self.template_history = []
        return "âœ… æ­·å²è¨˜éŒ„å·²æ¸…é™¤"

# å»ºç«‹å…¨åŸŸçš„ PromptTemplatesManager å¯¦ä¾‹
template_manager = PromptTemplatesManager()

def process_template_function(template_name: str, input_data: str, model_name: str) -> tuple:
    """Gradio æ¨¡æ¿è™•ç†å‡½æ•¸"""
    if not template_name or not input_data.strip():
        return "", "", ""
    
    # è§£æè¼¸å…¥è³‡æ–™
    try:
        if template_name == "å¤šè®Šæ•¸æ¨¡æ¿":
            # å¤šè®Šæ•¸æ¨¡æ¿éœ€è¦ç‰¹æ®Šè™•ç†
            variables = {}
            for line in input_data.split('\n'):
                if '=' in line:
                    key, value = line.split('=', 1)
                    variables[key.strip()] = value.strip()
            input_dict = variables
        else:
            # å–®è®Šæ•¸æ¨¡æ¿
            template_info = template_manager.get_template_info(template_name)
            if template_info:
                input_dict = {template_info["input_variables"][0]: input_data}
            else:
                return "", "", "âŒ æ¨¡æ¿è³‡è¨Šä¸å­˜åœ¨"
    except Exception as e:
        return "", "", f"âŒ è¼¸å…¥è³‡æ–™è§£æéŒ¯èª¤: {e}"
    
    # è™•ç†æ¨¡æ¿
    result = template_manager.process_template(template_name, input_dict, model_name)
    
    if "error" in result:
        return "", "", result["error"]
    
    return result["formatted_prompt"], result["response"], "âœ… è™•ç†å®Œæˆ"

def clear_history_function():
    """æ¸…é™¤æ­·å²è¨˜éŒ„"""
    template_manager.clear_history()
    return "âœ… æ­·å²è¨˜éŒ„å·²æ¸…é™¤"

def get_template_example(template_name: str) -> str:
    """å–å¾—æ¨¡æ¿ç¯„ä¾‹"""
    template_info = template_manager.get_template_info(template_name)
    if template_info:
        return template_info.get("example", "")
    return ""

def export_history():
    """åŒ¯å‡ºæ­·å²è¨˜éŒ„"""
    history = template_manager.get_template_history()
    if not history:
        return "âŒ æ²’æœ‰æ­·å²è¨˜éŒ„å¯åŒ¯å‡º"
    
    # å»ºç«‹åŒ¯å‡ºå…§å®¹
    export_content = f"# Prompt Templates è™•ç†æ­·å²\n"
    export_content += f"åŒ¯å‡ºæ™‚é–“: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}\n\n"
    
    for i, record in enumerate(history, 1):
        export_content += f"## {i}. {record['template']}\n"
        export_content += f"**æ™‚é–“**: {record['timestamp']}\n"
        export_content += f"**æ¨¡å‹**: {record['model']}\n"
        export_content += f"**è¼¸å…¥è³‡æ–™**: {record['input_data']}\n\n"
        export_content += f"**æ ¼å¼åŒ–æç¤º**:\n```\n{record['formatted_prompt']}\n```\n\n"
        export_content += f"**AI å›æ‡‰**:\n```\n{record['response']}\n```\n\n"
        export_content += "---\n\n"
    
    return export_content

def find_free_port(start_port=7860, max_port=7900):
    """å°‹æ‰¾å¯ç”¨çš„ç«¯å£"""
    for port in range(start_port, max_port):
        try:
            with socket.socket(socket.AF_INET, socket.SOCK_STREAM) as s:
                s.bind(('localhost', port))
                return port
        except OSError:
            continue
    return None

# å»ºç«‹ Gradio ä»‹é¢
def create_gradio_interface():
    """å»ºç«‹ Gradio ä»‹é¢"""
    
    # å–å¾—å¯ç”¨æ¨¡å‹å’Œæ¨¡æ¿
    available_models = template_manager.get_available_models()
    available_templates = template_manager.get_available_templates()
    
    with gr.Blocks(
        title="LangChain Prompt Templates æ‡‰ç”¨ç¨‹å¼",
        theme=gr.themes.Soft(),
        css="""
        .gradio-container {
            max-width: 1400px !important;
            margin: auto !important;
        }
        
        .template-card {
            border: 1px solid #e0e0e0;
            border-radius: 8px;
            padding: 16px;
            margin: 8px 0;
            background-color: #fafafa;
        }
        
        .template-title {
            font-weight: bold;
            color: #2196f3;
            margin-bottom: 8px;
        }
        
        .template-description {
            color: #666;
            font-size: 14px;
            margin-bottom: 8px;
        }
        
        .response-box {
            background-color: #f5f5f5;
            border-radius: 8px;
            padding: 12px;
            margin: 8px 0;
            border-left: 4px solid #4caf50;
        }
        """
    ) as interface:
        
        gr.Markdown("""
        # ğŸ¯ LangChain Prompt Templates æ‡‰ç”¨ç¨‹å¼
        
        é€™æ˜¯ä¸€å€‹å±•ç¤º LangChain Prompt Templates åŠŸèƒ½çš„æ‡‰ç”¨ç¨‹å¼ï¼Œæ”¯æ´ï¼š
        - ğŸ”„ **å¤šç¨®æ¨¡æ¿é¡å‹**ï¼šç¿»è­¯ã€æ‘˜è¦ã€ç¨‹å¼ç¢¼è§£é‡‹ã€å‰µæ„å¯«ä½œç­‰
        - ğŸ¤– **å¤šæ¨¡å‹æ”¯æ´**ï¼šOllamaã€Geminiã€OpenAIã€Anthropic
        - ğŸ“ **å‹•æ…‹è®Šæ•¸æ’å…¥**ï¼šæ”¯æ´å–®è®Šæ•¸å’Œå¤šè®Šæ•¸æ¨¡æ¿
        - ğŸ“Š **è™•ç†æ­·å²è¨˜éŒ„**ï¼šä¿å­˜å’ŒåŒ¯å‡ºè™•ç†è¨˜éŒ„
        """)
        
        with gr.Row():
            with gr.Column(scale=3):
                # æ¨¡æ¿é¸æ“‡
                gr.Markdown("### ğŸ¯ é¸æ“‡æ¨¡æ¿")
                template_dropdown = gr.Dropdown(
                    choices=available_templates,
                    value=available_templates[0] if available_templates else None,
                    label="é¸æ“‡æ¨¡æ¿é¡å‹",
                    interactive=True
                )
                
                # æ¨¡æ¿èªªæ˜
                template_description = gr.Markdown("")
                
                # è¼¸å…¥å€åŸŸ
                gr.Markdown("### ğŸ“ è¼¸å…¥å…§å®¹")
                input_textbox = gr.Textbox(
                    label="è¼¸å…¥å…§å®¹",
                    lines=4,
                    placeholder="è«‹è¼¸å…¥è¦è™•ç†çš„å…§å®¹..."
                )
                
                # ç¯„ä¾‹æŒ‰éˆ•
                example_btn = gr.Button("ğŸ“‹ è¼‰å…¥ç¯„ä¾‹", variant="secondary")
                
                # è™•ç†æŒ‰éˆ•
                process_btn = gr.Button("ğŸš€ è™•ç†æ¨¡æ¿", variant="primary")
                
                # çµæœé¡¯ç¤º
                gr.Markdown("### ğŸ“Š è™•ç†çµæœ")
                formatted_prompt_output = gr.Textbox(
                    label="æ ¼å¼åŒ–æç¤º",
                    lines=8,
                    interactive=False
                )
                
                ai_response_output = gr.Textbox(
                    label="AI å›æ‡‰",
                    lines=8,
                    interactive=False
                )
                
                status_output = gr.Textbox(
                    label="ç‹€æ…‹",
                    lines=1,
                    interactive=False
                )
            
            with gr.Column(scale=2):
                # æ¨¡å‹é¸æ“‡
                gr.Markdown("### ğŸ¤– æ¨¡å‹è¨­å®š")
                model_dropdown = gr.Dropdown(
                    choices=available_models,
                    value=template_manager.current_model,
                    label="é¸æ“‡æ¨¡å‹",
                    interactive=True
                )
                
                # æ¨¡æ¿è³‡è¨Š
                gr.Markdown("### ğŸ“‹ æ¨¡æ¿è³‡è¨Š")
                template_info_output = gr.Markdown("")
                
                # æ­·å²è¨˜éŒ„
                gr.Markdown("### ğŸ“š æ­·å²è¨˜éŒ„")
                history_output = gr.Textbox(
                    label="è™•ç†æ­·å²",
                    lines=10,
                    interactive=False
                )
                
                with gr.Row():
                    clear_history_btn = gr.Button("ğŸ—‘ï¸ æ¸…é™¤æ­·å²", variant="secondary")
                    export_btn = gr.Button("ğŸ“¤ åŒ¯å‡ºæ­·å²", variant="secondary")
                
                # åŒ¯å‡ºçµæœ
                gr.Markdown("### ğŸ“¤ åŒ¯å‡ºçµæœ")
                export_output = gr.Textbox(
                    label="åŒ¯å‡ºçš„å…§å®¹",
                    lines=8,
                    interactive=False
                )
        
        # äº‹ä»¶è™•ç†
        def update_template_info(template_name):
            """æ›´æ–°æ¨¡æ¿è³‡è¨Š"""
            if not template_name:
                return ""
            
            template_info = template_manager.get_template_info(template_name)
            if template_info:
                info = f"""
**æ¨¡æ¿åç¨±**: {template_info['name']}
**æè¿°**: {template_info['description']}
**è¼¸å…¥è®Šæ•¸**: {', '.join(template_info['input_variables'])}
**ç¯„ä¾‹**: {template_info['example']}
"""
                return info
            return ""
        
        def update_history_display():
            """æ›´æ–°æ­·å²è¨˜éŒ„é¡¯ç¤º"""
            history = template_manager.get_template_history()
            if not history:
                return "æ²’æœ‰è™•ç†è¨˜éŒ„"
            
            history_text = ""
            for i, record in enumerate(history[-5:], 1):  # åªé¡¯ç¤ºæœ€è¿‘ 5 æ¢
                history_text += f"{i}. {record['template']} ({record['timestamp']})\n"
            
            return history_text
        
        def handle_template_change(template_name):
            """è™•ç†æ¨¡æ¿è®Šæ›´"""
            info = update_template_info(template_name)
            history = update_history_display()
            return info, history
        
        def handle_example_load(template_name):
            """è¼‰å…¥ç¯„ä¾‹"""
            return get_template_example(template_name)
        
        def handle_process(template_name, input_data, model_name):
            """è™•ç†æ¨¡æ¿"""
            formatted_prompt, response, status = process_template_function(template_name, input_data, model_name)
            history = update_history_display()
            return formatted_prompt, response, status, history
        
        def handle_export():
            """åŒ¯å‡ºæ­·å²"""
            return export_history()
        
        def handle_clear_history():
            """æ¸…é™¤æ­·å²"""
            template_manager.clear_history()
            return "âœ… æ­·å²è¨˜éŒ„å·²æ¸…é™¤", ""
        
        # ç¶å®šäº‹ä»¶
        template_dropdown.change(
            handle_template_change,
            inputs=[template_dropdown],
            outputs=[template_info_output, history_output]
        )
        
        example_btn.click(
            handle_example_load,
            inputs=[template_dropdown],
            outputs=[input_textbox]
        )
        
        process_btn.click(
            handle_process,
            inputs=[template_dropdown, input_textbox, model_dropdown],
            outputs=[formatted_prompt_output, ai_response_output, status_output, history_output]
        )
        
        clear_history_btn.click(
            handle_clear_history,
            outputs=[status_output, history_output]
        )
        
        export_btn.click(
            handle_export,
            outputs=[export_output]
        )
        
        # åˆå§‹åŒ–
        if available_templates:
            initial_info = update_template_info(available_templates[0])
            initial_history = update_history_display()
            template_info_output.value = initial_info
            history_output.value = initial_history
    
    return interface

def main():
    """ä¸»å‡½æ•¸"""
    print("ğŸš€ å•Ÿå‹• LangChain Prompt Templates Gradio æ‡‰ç”¨ç¨‹å¼...")
    
    # æª¢æŸ¥å¯ç”¨æ¨¡å‹
    available_models = template_manager.get_available_models()
    if not available_models:
        print("âŒ æ²’æœ‰å¯ç”¨çš„æ¨¡å‹ï¼è«‹æª¢æŸ¥ï¼š")
        print("   1. Ollama æ˜¯å¦æ­£åœ¨é‹è¡Œ")
        print("   2. API é‡‘é‘°æ˜¯å¦æ­£ç¢ºè¨­å®š")
        print("   3. ç¶²è·¯é€£ç·šæ˜¯å¦æ­£å¸¸")
        return
    
    print(f"âœ… æ‰¾åˆ° {len(available_models)} å€‹å¯ç”¨æ¨¡å‹:")
    for model in available_models:
        print(f"   - {model}")
    
    print(f"âœ… æ‰¾åˆ° {len(template_manager.get_available_templates())} å€‹å¯ç”¨æ¨¡æ¿:")
    for template in template_manager.get_available_templates():
        print(f"   - {template}")
    
    # å°‹æ‰¾å¯ç”¨ç«¯å£
    free_port = find_free_port()
    if not free_port:
        print("âŒ ç„¡æ³•æ‰¾åˆ°å¯ç”¨ç«¯å£ (7860-7900)")
        return
    
    print(f"ğŸŒ ä½¿ç”¨ç«¯å£: {free_port}")
    
    # å»ºç«‹ä¸¦å•Ÿå‹•ä»‹é¢
    interface = create_gradio_interface()
    
    print("ğŸŒ å•Ÿå‹• Web ä»‹é¢...")
    try:
        interface.launch(
            server_name="0.0.0.0",
            server_port=free_port,
            share=False,
            show_error=True
        )
    except Exception as e:
        print(f"âŒ å•Ÿå‹•å¤±æ•—: {e}")
        print("ğŸ’¡ è«‹æª¢æŸ¥æ˜¯å¦æœ‰å…¶ä»–æ‡‰ç”¨ç¨‹å¼ä½”ç”¨ç«¯å£")

if __name__ == "__main__":
    main()
