{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. åŸºç¤éˆ (Chains Basics) - Gemini ç‰ˆæœ¬\n",
        "\n",
        "æœ¬ç¯„ä¾‹å±•ç¤ºæœ€åŸºæœ¬çš„ LangChain Expression Language (LCEL) éˆçµ„åˆï¼ŒåŒ…å«ï¼š\n",
        "- Prompt Template â†’ LLM â†’ Output Parser\n",
        "\n",
        "## å­¸ç¿’é‡é»\n",
        "- ç†è§£ LCEL çš„ç®¡é“ç¬¦è™Ÿ `|` ç”¨æ³•\n",
        "- å­¸ç¿’åŸºæœ¬çš„éˆçµ„åˆæ–¹å¼\n",
        "- äº†è§£å¦‚ä½•å°‡å¤šå€‹å…ƒä»¶ä¸²è¯èµ·ä¾†\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å°å…¥å¿…è¦çš„å¥—ä»¶\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
        "load_dotenv()\n",
        "\n",
        "# å»ºç«‹ Gemini æ¨¡å‹\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®šç¾©æç¤ºæ¨¡æ¿\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ä½ æ˜¯ä¸€å€‹å–œåŠ‡æ¼”å“¡ï¼Œå°ˆé–€è¬›é—œæ–¼ {topic} çš„ç¬‘è©±ã€‚\"),\n",
        "        (\"human\", \"è«‹å‘Šè¨´æˆ‘ {joke_count} å€‹ç¬‘è©±ã€‚\"),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½¿ç”¨ LangChain Expression Language (LCEL) å»ºç«‹çµ„åˆéˆ\n",
        "# ç®¡é“ç¬¦è™Ÿ | è¡¨ç¤ºï¼šå°‡å·¦é‚Šå…ƒä»¶çš„è¼¸å‡ºå‚³éçµ¦å³é‚Šçš„å…ƒä»¶\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "print(\"éˆå·²å»ºç«‹å®Œæˆï¼\")\n",
        "print(\"éˆçš„çµæ§‹ï¼šPrompt Template â†’ LLM â†’ Output Parser\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŸ·è¡Œéˆ\n",
        "result = chain.invoke({\"topic\": \"å¾‹å¸«\", \"joke_count\": 3})\n",
        "\n",
        "# è¼¸å‡ºçµæœ\n",
        "print(\"=\" * 50)\n",
        "print(\"ç¬‘è©±çµæœï¼š\")\n",
        "print(\"=\" * 50)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’¡ é‡é»èªªæ˜\n",
        "\n",
        "1. **LCEL èªæ³•**: `prompt_template | model | StrOutputParser()` ä½¿ç”¨ç®¡é“ç¬¦è™Ÿ `|` ä¾†ä¸²è¯å…ƒä»¶\n",
        "2. **è³‡æ–™æµå‘**: è¼¸å…¥ â†’ Prompt Template â†’ LLM â†’ Output Parser â†’ è¼¸å‡º\n",
        "3. **è‡ªå‹•åŒ–**: ä¸€æ—¦å»ºç«‹éˆï¼Œæ•´å€‹æµç¨‹å°±å¯ä»¥è‡ªå‹•åŸ·è¡Œ\n",
        "4. **æ¨¡çµ„åŒ–**: æ¯å€‹å…ƒä»¶éƒ½å¯ä»¥ç¨ç«‹æ›¿æ›æˆ–ä¿®æ”¹\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
