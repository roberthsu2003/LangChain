{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "dfed2453",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ åŸºç¤éˆ (Basic Chains) - Gemini ç‰ˆæœ¬\n",
    "\n",
    "> ğŸ“– **å­¸ç¿’ç›®æ¨™**ï¼šæŒæ¡ LangChain åŸºç¤éˆçš„æ ¸å¿ƒæ¦‚å¿µèˆ‡ LCEL èªæ³•\n",
    "> \n",
    "> ğŸ¯ **é©åˆå°è±¡**ï¼šLangChain åˆå­¸è€…\n",
    "> \n",
    "> â±ï¸ **é ä¼°æ™‚é–“**ï¼š15-20 åˆ†é˜\n",
    "\n",
    "## ğŸ“š ä»€éº¼æ˜¯åŸºç¤éˆï¼Ÿ\n",
    "\n",
    "**åŸºç¤éˆ**æ˜¯ LangChain ä¸­æœ€ç°¡å–®ä¹Ÿæœ€é‡è¦çš„æ¦‚å¿µï¼Œå®ƒä½¿ç”¨ **LCEL (LangChain Expression Language)** èªæ³•å°‡å¤šå€‹å…ƒä»¶ä¸²è¯èµ·ä¾†ã€‚\n",
    "\n",
    "### ğŸ”— åŸºç¤éˆçš„çµ„æˆ\n",
    "```\n",
    "ä½¿ç”¨è€…è¼¸å…¥ â†’ Prompt Template â†’ LLM â†’ Output Parser â†’ æœ€çµ‚çµæœ\n",
    "```\n",
    "\n",
    "### âœ¨ LCEL èªæ³•çš„æ ¸å¿ƒ\n",
    "- ä½¿ç”¨ç®¡é“ç¬¦è™Ÿ `|` é€£æ¥å…ƒä»¶\n",
    "- è‡ªå‹•è™•ç†è³‡æ–™æµè½‰æ›\n",
    "- æ”¯æ´ä¸²æµã€æ‰¹æ¬¡è™•ç†ç­‰é€²éšåŠŸèƒ½\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ è®“æˆ‘å€‘é–‹å§‹å¯¦ä½œï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9f2d177",
   "metadata": {},
   "source": [
    "## ğŸ“¦ æ­¥é©Ÿ 1ï¼šå®‰è£èˆ‡å°å…¥å¿…è¦çš„å¥—ä»¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d172e112",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„å¥—ä»¶\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "import os\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Šè¨Šæ¯\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"âœ… å¥—ä»¶å°å…¥æˆåŠŸï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3058beb4",
   "metadata": {},
   "source": [
    "## ğŸ”‘ æ­¥é©Ÿ 2ï¼šè¨­å®š Google API Key\n",
    "\n",
    "> **æ³¨æ„**ï¼šä½¿ç”¨ Gemini éœ€è¦ Google API Key\n",
    "> \n",
    "> å¦‚æœæ‚¨é‚„æ²’æœ‰ API Keyï¼Œè«‹ï¼š\n",
    "> 1. å‰å¾€ [Google AI Studio](https://makersuite.google.com/app/apikey)\n",
    "> 2. å»ºç«‹æ–°çš„ API Key\n",
    "> 3. å°‡ API Key è¨­å®šç‚ºç’°å¢ƒè®Šæ•¸\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20128aae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥ API Key æ˜¯å¦å·²è¨­å®š\n",
    "api_key = os.getenv(\"GOOGLE_API_KEY\")\n",
    "\n",
    "if not api_key:\n",
    "    print(\"âŒ è«‹å…ˆè¨­å®š GOOGLE_API_KEY ç’°å¢ƒè®Šæ•¸\")\n",
    "    print(\"ğŸ’¡ æ‚¨å¯ä»¥ï¼š\")\n",
    "    print(\"   1. åœ¨çµ‚ç«¯æ©ŸåŸ·è¡Œï¼šexport GOOGLE_API_KEY='your_api_key_here'\")\n",
    "    print(\"   2. æˆ–åœ¨ .env æª”æ¡ˆä¸­è¨­å®šï¼šGOOGLE_API_KEY=your_api_key_here\")\n",
    "    print(\"   3. æˆ–ç›´æ¥åœ¨ç¨‹å¼ä¸­è¨­å®šï¼šos.environ['GOOGLE_API_KEY'] = 'your_api_key_here'\")\n",
    "else:\n",
    "    print(\"âœ… Google API Key å·²è¨­å®š\")\n",
    "    print(f\"ğŸ”‘ API Key å‰ 10 å­—å…ƒï¼š{api_key[:10]}...\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4acd5bdc",
   "metadata": {},
   "source": [
    "## ğŸ¤– æ­¥é©Ÿ 3ï¼šå»ºç«‹ Gemini æ¨¡å‹\n",
    "\n",
    "Gemini æ˜¯ Google çš„æœ€æ–°å¤§å‹èªè¨€æ¨¡å‹ï¼Œå…·æœ‰å¼·å¤§çš„å¤šèªè¨€èƒ½åŠ›å’Œå‰µæ„ç”Ÿæˆèƒ½åŠ›ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8556854",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ Gemini æ¨¡å‹\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",  # ä½¿ç”¨ Gemini 1.5 Flash æ¨¡å‹\n",
    "    temperature=0.7,           # æ§åˆ¶å›æ‡‰çš„å‰µé€ æ€§ (0-1)\n",
    "    top_p=0.9,                # æ§åˆ¶å›æ‡‰çš„å¤šæ¨£æ€§\n",
    "    max_output_tokens=1000,   # é™åˆ¶å›æ‡‰é•·åº¦\n",
    ")\n",
    "\n",
    "print(\"âœ… Gemini æ¨¡å‹å»ºç«‹æˆåŠŸï¼\")\n",
    "print(f\"ğŸ“‹ ä½¿ç”¨æ¨¡å‹ï¼šgemini-1.5-flash\")\n",
    "print(f\"ğŸŒ¡ï¸ æº«åº¦è¨­å®šï¼š{model.temperature}\")\n",
    "print(f\"ğŸ¯ Top-p è¨­å®šï¼š{model.top_p}\")\n",
    "print(f\"ğŸ“ æœ€å¤§è¼¸å‡ºé•·åº¦ï¼š{model.max_output_tokens} tokens\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "acfb9ded",
   "metadata": {},
   "source": [
    "## ğŸ“ æ­¥é©Ÿ 4ï¼šå»ºç«‹æç¤ºæ¨¡æ¿ (Prompt Template)\n",
    "\n",
    "æç¤ºæ¨¡æ¿æ˜¯ LangChain ä¸­éå¸¸é‡è¦çš„å…ƒä»¶ï¼Œå®ƒè®“æˆ‘å€‘å¯ä»¥ï¼š\n",
    "- å‹•æ…‹æ’å…¥è®Šæ•¸\n",
    "- ä¿æŒæç¤ºçš„ä¸€è‡´æ€§\n",
    "- é‡è¤‡ä½¿ç”¨ç›¸åŒçš„æç¤ºæ ¼å¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09b2bf35",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹æç¤ºæ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„{role}ï¼Œè«‹ç”¨{style}çš„é¢¨æ ¼ä¾†ä»‹ç´¹{topic}ã€‚\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. å…§å®¹è¦æº–ç¢ºä¸”æ˜“æ‡‚\n",
    "2. é•·åº¦æ§åˆ¶åœ¨200å­—ä»¥å…§\n",
    "3. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”\n",
    "4. åŠ å…¥ä¸€äº›å¯¦ç”¨çš„ä¾‹å­æˆ–å»ºè­°\n",
    "\n",
    "è«‹é–‹å§‹ä»‹ç´¹ï¼š\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… æç¤ºæ¨¡æ¿å»ºç«‹æˆåŠŸï¼\")\n",
    "print(\"ğŸ“‹ æ¨¡æ¿å…§å®¹ï¼š\")\n",
    "print(prompt.template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "048604ef",
   "metadata": {},
   "source": [
    "## ğŸ”§ æ­¥é©Ÿ 5ï¼šå»ºç«‹è¼¸å‡ºè§£æå™¨ (Output Parser)\n",
    "\n",
    "è¼¸å‡ºè§£æå™¨ç”¨æ–¼å°‡ LLM çš„å›æ‡‰è½‰æ›æˆæˆ‘å€‘éœ€è¦çš„æ ¼å¼ã€‚é€™è£¡æˆ‘å€‘ä½¿ç”¨ `StrOutputParser` ä¾†å–å¾—ç´”æ–‡å­—å›æ‡‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47262d6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è¼¸å‡ºè§£æå™¨\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "print(\"âœ… è¼¸å‡ºè§£æå™¨å»ºç«‹æˆåŠŸï¼\")\n",
    "print(\"ğŸ“‹ è§£æå™¨é¡å‹ï¼šStrOutputParser\")\n",
    "print(\"ğŸ¯ åŠŸèƒ½ï¼šå°‡ LLM å›æ‡‰è½‰æ›ç‚ºç´”æ–‡å­—å­—ä¸²\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5534819c",
   "metadata": {},
   "source": [
    "## ğŸ”— æ­¥é©Ÿ 6ï¼šä½¿ç”¨ LCEL èªæ³•å»ºç«‹åŸºç¤éˆ\n",
    "\n",
    "é€™æ˜¯æ•´å€‹æ•™å­¸çš„æ ¸å¿ƒï¼æˆ‘å€‘ä½¿ç”¨ç®¡é“ç¬¦è™Ÿ `|` å°‡æ‰€æœ‰å…ƒä»¶ä¸²è¯èµ·ä¾†ã€‚\n",
    "\n",
    "### ğŸ’¡ LCEL èªæ³•èªªæ˜\n",
    "```python\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "**è³‡æ–™æµå‘**ï¼š\n",
    "1. `prompt` æ¥æ”¶è¼¸å…¥ä¸¦æ ¼å¼åŒ–\n",
    "2. `model` è™•ç†æ ¼å¼åŒ–å¾Œçš„æç¤º\n",
    "3. `output_parser` è§£ææ¨¡å‹å›æ‡‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d8d3755",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ LCEL èªæ³•å»ºç«‹åŸºç¤éˆ\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "print(\"âœ… åŸºç¤éˆå»ºç«‹æˆåŠŸï¼\")\n",
    "print(\"ğŸ”— éˆçš„çµæ§‹ï¼š\")\n",
    "print(\"   prompt â†’ model â†’ output_parser\")\n",
    "print(\"\\nğŸ“‹ éˆçš„é¡å‹ï¼š\", type(chain))\n",
    "print(\"ğŸ¯ æº–å‚™å¥½åŸ·è¡Œç¬¬ä¸€å€‹åŸºç¤éˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5cd1c32",
   "metadata": {},
   "source": [
    "## ğŸš€ æ­¥é©Ÿ 7ï¼šåŸ·è¡ŒåŸºç¤éˆ\n",
    "\n",
    "ç¾åœ¨è®“æˆ‘å€‘ä¾†æ¸¬è©¦æˆ‘å€‘å»ºç«‹çš„åŸºç¤éˆï¼æˆ‘å€‘æœƒä½¿ç”¨ `invoke()` æ–¹æ³•ä¾†åŸ·è¡Œéˆã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c73f7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æº–å‚™è¼¸å…¥è³‡æ–™\n",
    "input_data = {\n",
    "    \"role\": \"AI å°ˆå®¶\",\n",
    "    \"style\": \"ç°¡æ½”æ˜ç­\",\n",
    "    \"topic\": \"äººå·¥æ™ºæ…§\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ è¼¸å…¥è³‡æ–™ï¼š\")\n",
    "for key, value in input_data.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ”„ æ­£åœ¨åŸ·è¡ŒåŸºç¤éˆ...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# åŸ·è¡ŒåŸºç¤éˆ\n",
    "result = chain.invoke(input_data)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… åŸºç¤éˆåŸ·è¡Œå®Œæˆï¼\")\n",
    "print(\"\\nğŸ“‹ å›æ‡‰çµæœï¼š\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8152cef2",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ­¥é©Ÿ 8ï¼šå˜—è©¦ä¸åŒçš„è¼¸å…¥\n",
    "\n",
    "è®“æˆ‘å€‘å˜—è©¦ä¸åŒçš„ä¸»é¡Œï¼Œçœ‹çœ‹åŸºç¤éˆå¦‚ä½•è™•ç†ä¸åŒçš„è¼¸å…¥ï¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18cea103",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å˜—è©¦ä¸åŒçš„ä¸»é¡Œ\n",
    "topics_to_try = [\n",
    "    {\n",
    "        \"role\": \"ç§‘æŠ€è¨˜è€…\",\n",
    "        \"style\": \"ç”Ÿå‹•æœ‰è¶£\",\n",
    "        \"topic\": \"å€å¡ŠéˆæŠ€è¡“\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"æ•™è‚²å°ˆå®¶\",\n",
    "        \"style\": \"å°ˆæ¥­è©³ç´°\",\n",
    "        \"topic\": \"æ©Ÿå™¨å­¸ç¿’\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, input_data in enumerate(topics_to_try, 1):\n",
    "    print(f\"\\nğŸ¯ æ¸¬è©¦ {i}: {input_data['topic']}\")\n",
    "    print(f\"ğŸ‘¤ è§’è‰²: {input_data['role']}\")\n",
    "    print(f\"ğŸ¨ é¢¨æ ¼: {input_data['style']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # åŸ·è¡ŒåŸºç¤éˆ\n",
    "    result = chain.invoke(input_data)\n",
    "    print(result)\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244de465",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ­¥é©Ÿ 9ï¼šæ¢ç´¢é€²éšåŠŸèƒ½\n",
    "\n",
    "åŸºç¤éˆä¸åƒ…æ”¯æ´ `invoke()`ï¼Œé‚„æ”¯æ´å…¶ä»–å¼·å¤§çš„åŠŸèƒ½ï¼š\n",
    "\n",
    "### ğŸ“Š æ‰¹æ¬¡è™•ç† (Batch Processing)\n",
    "åŒæ™‚è™•ç†å¤šå€‹è¼¸å…¥ï¼Œæé«˜æ•ˆç‡\n",
    "\n",
    "### ğŸŒŠ ä¸²æµè¼¸å‡º (Streaming)\n",
    "å³æ™‚é¡¯ç¤ºå›æ‡‰ï¼Œæå‡ä½¿ç”¨è€…é«”é©—\n",
    "\n",
    "### âš¡ éåŒæ­¥è™•ç† (Async)\n",
    "æ”¯æ´éåŒæ­¥èª¿ç”¨ï¼Œé©åˆé«˜ä½µç™¼å ´æ™¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c4616a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹æ¬¡è™•ç†ç¯„ä¾‹\n",
    "print(\"ğŸ“Š æ‰¹æ¬¡è™•ç†ç¯„ä¾‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "batch_inputs = [\n",
    "    {\"role\": \"æ­·å²å­¸å®¶\", \"style\": \"å­¸è¡“åš´è¬¹\", \"topic\": \"å¤åŸƒåŠæ–‡æ˜\"},\n",
    "    {\"role\": \"æ—…éŠé”äºº\", \"style\": \"è¼•é¬†æ„‰å¿«\", \"topic\": \"æ—¥æœ¬æ–‡åŒ–\"},\n",
    "    {\"role\": \"ç§‘å­¸å®¶\", \"style\": \"é‚è¼¯æ¸…æ™°\", \"topic\": \"é‡å­ç‰©ç†\"}\n",
    "]\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨æ‰¹æ¬¡è™•ç†...\")\n",
    "batch_results = chain.batch(batch_inputs)\n",
    "\n",
    "for i, (input_data, result) in enumerate(zip(batch_inputs, batch_results), 1):\n",
    "    print(f\"\\nğŸ“ æ‰¹æ¬¡ {i}: {input_data['topic']}\")\n",
    "    print(f\"ğŸ‘¤ è§’è‰²: {input_data['role']}\")\n",
    "    print(f\"ğŸ¨ é¢¨æ ¼: {input_data['style']}\")\n",
    "    print(\"ğŸ“‹ å›æ‡‰:\")\n",
    "    print(result[:100] + \"...\" if len(result) > 100 else result)\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6304908e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸²æµè¼¸å‡ºç¯„ä¾‹\n",
    "print(\"\\nğŸŒŠ ä¸²æµè¼¸å‡ºç¯„ä¾‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "stream_input = {\n",
    "    \"role\": \"è©©äºº\",\n",
    "    \"style\": \"å„ªç¾è©©æ„\",\n",
    "    \"topic\": \"æ˜¥å¤©çš„èŠ±æœµ\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨ä¸²æµè¼¸å‡º...\")\n",
    "print(\"ğŸ“‹ å³æ™‚å›æ‡‰:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ä¸²æµè¼¸å‡º\n",
    "for chunk in chain.stream(stream_input):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(\"âœ… ä¸²æµè¼¸å‡ºå®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e692d05d",
   "metadata": {},
   "source": [
    "## ğŸ“ å­¸ç¿’ç¸½çµ\n",
    "\n",
    "æ­å–œï¼æ‚¨å·²ç¶“æˆåŠŸå­¸æœƒäº† LangChain åŸºç¤éˆçš„æ ¸å¿ƒæ¦‚å¿µï¼š\n",
    "\n",
    "### âœ… æ‚¨å­¸æœƒäº†ä»€éº¼ï¼Ÿ\n",
    "1. **LCEL èªæ³•**ï¼šä½¿ç”¨ `|` ç®¡é“ç¬¦è™Ÿä¸²è¯å…ƒä»¶\n",
    "2. **åŸºç¤éˆçµæ§‹**ï¼šPrompt â†’ Model â†’ Parser\n",
    "3. **ä¸‰ç¨®åŸ·è¡Œæ–¹å¼**ï¼š\n",
    "   - `invoke()` - å–®æ¬¡åŸ·è¡Œ\n",
    "   - `batch()` - æ‰¹æ¬¡è™•ç†\n",
    "   - `stream()` - ä¸²æµè¼¸å‡º\n",
    "\n",
    "### ğŸ”‘ é—œéµæ¦‚å¿µ\n",
    "- **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šæ¯å€‹å…ƒä»¶éƒ½æœ‰ç‰¹å®šåŠŸèƒ½\n",
    "- **è³‡æ–™æµè½‰æ›**ï¼šè‡ªå‹•è™•ç†ä¸åŒæ ¼å¼é–“çš„è½‰æ›\n",
    "- **éˆæ´»æ€§**ï¼šè¼•é¬†æ›¿æ›æˆ–çµ„åˆä¸åŒå…ƒä»¶\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°\n",
    "1. å­¸ç¿’ **æ“´å±•éˆ** - åŠ å…¥è‡ªå®šç¾©è™•ç†é‚è¼¯\n",
    "2. æ¢ç´¢ **ä¸¦è¡Œéˆ** - åŒæ™‚åŸ·è¡Œå¤šå€‹ä»»å‹™\n",
    "3. å˜—è©¦ **åˆ†æ”¯éˆ** - æ ¹æ“šæ¢ä»¶é¸æ“‡ä¸åŒè·¯å¾‘\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ å¯¦ç”¨æŠ€å·§\n",
    "\n",
    "### ğŸ”§ é™¤éŒ¯æŠ€å·§\n",
    "```python\n",
    "# æª¢æŸ¥éˆçš„çµæ§‹\n",
    "print(chain.get_graph().draw_mermaid())\n",
    "\n",
    "# æŸ¥çœ‹ä¸­é–“æ­¥é©Ÿ\n",
    "for step in chain.stream(input_data, config={\"debug\": True}):\n",
    "    print(step)\n",
    "```\n",
    "\n",
    "### âš¡ æ•ˆèƒ½å„ªåŒ–\n",
    "```python\n",
    "# èª¿æ•´æ¨¡å‹åƒæ•¸\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    temperature=0.3,           # é™ä½å‰µé€ æ€§ï¼Œæé«˜ä¸€è‡´æ€§\n",
    "    max_output_tokens=500     # é™åˆ¶å›æ‡‰é•·åº¦\n",
    ")\n",
    "```\n",
    "\n",
    "### ğŸŒŸ Gemini ç‰¹æ®ŠåŠŸèƒ½\n",
    "```python\n",
    "# ä½¿ç”¨ä¸åŒçš„ Gemini æ¨¡å‹\n",
    "model_gemini_pro = ChatGoogleGenerativeAI(model=\"gemini-1.5-pro\")\n",
    "model_gemini_flash = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash\")\n",
    "\n",
    "# èª¿æ•´å®‰å…¨è¨­å®š\n",
    "model = ChatGoogleGenerativeAI(\n",
    "    model=\"gemini-1.5-flash\",\n",
    "    safety_settings={\n",
    "        \"HARM_CATEGORY_HARASSMENT\": \"BLOCK_MEDIUM_AND_ABOVE\",\n",
    "        \"HARM_CATEGORY_HATE_SPEECH\": \"BLOCK_MEDIUM_AND_ABOVE\"\n",
    "    }\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ‚¨å·²ç¶“æŒæ¡äº† LangChain åŸºç¤éˆï¼æº–å‚™å¥½é€²å…¥ä¸‹ä¸€å€‹å­¸ç¿’éšæ®µäº†å—ï¼Ÿ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
