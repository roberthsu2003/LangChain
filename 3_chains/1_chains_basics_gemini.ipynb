{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 1. åŸºç¤éˆ (Chains Basics) - Gemini ç‰ˆæœ¬\n",
        "\n",
        "æœ¬ç¯„ä¾‹å±•ç¤ºæœ€åŸºæœ¬çš„ LangChain Expression Language (LCEL) éˆçµ„åˆï¼Œå»ºç«‹ä¸€å€‹ç°¡å–®çš„å•ç­”ç³»çµ±ã€‚\n",
        "\n",
        "## å­¸ç¿’é‡é»\n",
        "- ç†è§£ LCEL çš„ç®¡é“ç¬¦è™Ÿ `|` ç”¨æ³•\n",
        "- å­¸ç¿’åŸºæœ¬çš„éˆçµ„åˆæ–¹å¼\n",
        "- äº†è§£å¦‚ä½•å°‡å¤šå€‹å…ƒä»¶ä¸²è¯èµ·ä¾†\n",
        "- å»ºç«‹å¯¦ç”¨çš„å•ç­”ç³»çµ±\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å°å…¥å¿…è¦çš„å¥—ä»¶\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.output_parser import StrOutputParser\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
        "load_dotenv()\n",
        "\n",
        "# å»ºç«‹ Gemini æ¨¡å‹\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®šç¾©æç¤ºæ¨¡æ¿ - å»ºç«‹ä¸€å€‹å‹å–„çš„å•ç­”åŠ©æ‰‹\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [ \n",
        "        (\"system\", \"ä½ æ˜¯ä¸€å€‹å‹å–„ä¸”å°ˆæ¥­çš„åŠ©æ‰‹ï¼Œå°ˆé–€å›ç­”é—œæ–¼ {topic} çš„å•é¡Œã€‚è«‹ç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼è§£é‡‹ã€‚\"),\n",
        "        (\"human\", \"è«‹å›ç­”é€™å€‹å•é¡Œï¼š{question}\"),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½¿ç”¨ LangChain Expression Language (LCEL) å»ºç«‹çµ„åˆéˆ\n",
        "# ç®¡é“ç¬¦è™Ÿ | è¡¨ç¤ºï¼šå°‡å·¦é‚Šå…ƒä»¶çš„è¼¸å‡ºå‚³éçµ¦å³é‚Šçš„å…ƒä»¶\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "\n",
        "print(\"éˆå·²å»ºç«‹å®Œæˆï¼\")\n",
        "print(\"éˆçš„çµæ§‹ï¼šPrompt Template â†’ LLM â†’ Output Parser\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŸ·è¡Œéˆ - æ¸¬è©¦å•ç­”ç³»çµ±\n",
        "result = chain.invoke({\n",
        "    \"topic\": \"äººå·¥æ™ºæ…§\", \n",
        "    \"question\": \"ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ\"\n",
        "})\n",
        "\n",
        "# è¼¸å‡ºçµæœ\n",
        "print(\"=" * 50)\n",
        "print(\"å•ç­”ç³»çµ±å›æ‡‰ï¼š\")\n",
        "print(\"=" * 50)\n",
        "print(result)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’¡ é‡é»èªªæ˜\n",
        "\n",
        "1. **LCEL èªæ³•**: `prompt_template | model | StrOutputParser()` ä½¿ç”¨ç®¡é“ç¬¦è™Ÿ `|` ä¾†ä¸²è¯å…ƒä»¶\n",
        "2. **è³‡æ–™æµå‘**: è¼¸å…¥ â†’ Prompt Template â†’ LLM â†’ Output Parser â†’ è¼¸å‡º\n",
        "3. **è‡ªå‹•åŒ–**: ä¸€æ—¦å»ºç«‹éˆï¼Œæ•´å€‹æµç¨‹å°±å¯ä»¥è‡ªå‹•åŸ·è¡Œ\n",
        "4. **æ¨¡çµ„åŒ–**: æ¯å€‹å…ƒä»¶éƒ½å¯ä»¥ç¨ç«‹æ›¿æ›æˆ–ä¿®æ”¹\n",
        "\n",
        "## ğŸ”§ å¯¦éš›æ‡‰ç”¨å ´æ™¯\n",
        "\n",
        "- **å®¢æœç³»çµ±**: å›ç­”å¸¸è¦‹å•é¡Œ\n",
        "- **å­¸ç¿’åŠ©æ‰‹**: è§£é‡‹å­¸ç§‘æ¦‚å¿µ\n",
        "- **æŠ€è¡“æ”¯æ´**: æä¾›è§£æ±ºæ–¹æ¡ˆ\n",
        "- **å…§å®¹ç”Ÿæˆ**: æ ¹æ“šä¸»é¡Œå›ç­”å•é¡Œ\n"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}