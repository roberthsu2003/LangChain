{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46d383c6",
   "metadata": {},
   "source": [
    "# 1ï¸âƒ£ åŸºç¤éˆ (Basic Chains) - Ollama ç‰ˆæœ¬\n",
    "\n",
    "> ğŸ“– **å­¸ç¿’ç›®æ¨™**ï¼šæŒæ¡ LangChain åŸºç¤éˆçš„æ ¸å¿ƒæ¦‚å¿µèˆ‡ LCEL èªæ³•\n",
    "> \n",
    "> ğŸ¯ **é©åˆå°è±¡**ï¼šLangChain åˆå­¸è€…\n",
    "> \n",
    "> â±ï¸ **é ä¼°æ™‚é–“**ï¼š15-20 åˆ†é˜\n",
    "\n",
    "## ğŸ“š ä»€éº¼æ˜¯åŸºç¤éˆï¼Ÿ\n",
    "\n",
    "**åŸºç¤éˆ**æ˜¯ LangChain ä¸­æœ€ç°¡å–®ä¹Ÿæœ€é‡è¦çš„æ¦‚å¿µï¼Œå®ƒä½¿ç”¨ **LCEL (LangChain Expression Language)** èªæ³•å°‡å¤šå€‹å…ƒä»¶ä¸²è¯èµ·ä¾†ã€‚\n",
    "\n",
    "### ğŸ”— åŸºç¤éˆçš„çµ„æˆ\n",
    "```\n",
    "ä½¿ç”¨è€…è¼¸å…¥ â†’ Prompt Template â†’ LLM â†’ Output Parser â†’ æœ€çµ‚çµæœ\n",
    "```\n",
    "\n",
    "### âœ¨ LCEL èªæ³•çš„æ ¸å¿ƒ\n",
    "- ä½¿ç”¨ç®¡é“ç¬¦è™Ÿ `|` é€£æ¥å…ƒä»¶\n",
    "- è‡ªå‹•è™•ç†è³‡æ–™æµè½‰æ›\n",
    "- æ”¯æ´ä¸²æµã€æ‰¹æ¬¡è™•ç†ç­‰é€²éšåŠŸèƒ½\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸš€ è®“æˆ‘å€‘é–‹å§‹å¯¦ä½œï¼\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e4ff1b",
   "metadata": {},
   "source": [
    "## ğŸ“¦ æ­¥é©Ÿ 1ï¼šå®‰è£èˆ‡å°å…¥å¿…è¦çš„å¥—ä»¶\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a9952f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„å¥—ä»¶\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "import warnings\n",
    "\n",
    "# å¿½ç•¥è­¦å‘Šè¨Šæ¯\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "print(\"âœ… å¥—ä»¶å°å…¥æˆåŠŸï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0b0711",
   "metadata": {},
   "source": [
    "## ğŸ¤– æ­¥é©Ÿ 2ï¼šå»ºç«‹ Ollama æ¨¡å‹\n",
    "\n",
    "> **æ³¨æ„**ï¼šè«‹ç¢ºä¿æ‚¨çš„ç³»çµ±å·²å®‰è£ä¸¦é‹è¡Œ Ollamaï¼Œä¸”å·²ä¸‹è¼‰ llama3.2 æ¨¡å‹\n",
    "> \n",
    "> å¦‚æœå°šæœªå®‰è£ï¼Œè«‹åŸ·è¡Œï¼š\n",
    "> ```bash\n",
    "> # å®‰è£ Ollama\n",
    "> curl -fsSL https://ollama.ai/install.sh | sh\n",
    "> \n",
    "> # ä¸‹è¼‰æ¨¡å‹\n",
    "> ollama pull llama3.2:latest\n",
    "> ```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d769240",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹ Ollama æ¨¡å‹\n",
    "model = OllamaLLM(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0.7,  # æ§åˆ¶å›æ‡‰çš„å‰µé€ æ€§ (0-1)\n",
    "    top_p=0.9,       # æ§åˆ¶å›æ‡‰çš„å¤šæ¨£æ€§\n",
    ")\n",
    "\n",
    "print(\"âœ… Ollama æ¨¡å‹å»ºç«‹æˆåŠŸï¼\")\n",
    "print(f\"ğŸ“‹ ä½¿ç”¨æ¨¡å‹ï¼šllama3.2:latest\")\n",
    "print(f\"ğŸŒ¡ï¸ æº«åº¦è¨­å®šï¼š{model.temperature}\")\n",
    "print(f\"ğŸ¯ Top-p è¨­å®šï¼š{model.top_p}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a983fd96",
   "metadata": {},
   "source": [
    "## ğŸ“ æ­¥é©Ÿ 3ï¼šå»ºç«‹æç¤ºæ¨¡æ¿ (Prompt Template)\n",
    "\n",
    "æç¤ºæ¨¡æ¿æ˜¯ LangChain ä¸­éå¸¸é‡è¦çš„å…ƒä»¶ï¼Œå®ƒè®“æˆ‘å€‘å¯ä»¥ï¼š\n",
    "- å‹•æ…‹æ’å…¥è®Šæ•¸\n",
    "- ä¿æŒæç¤ºçš„ä¸€è‡´æ€§\n",
    "- é‡è¤‡ä½¿ç”¨ç›¸åŒçš„æç¤ºæ ¼å¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "161db7a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹æç¤ºæ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_template(\n",
    "    \"\"\"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„{role}ï¼Œè«‹ç”¨{style}çš„é¢¨æ ¼ä¾†ä»‹ç´¹{topic}ã€‚\n",
    "\n",
    "è¦æ±‚ï¼š\n",
    "1. å…§å®¹è¦æº–ç¢ºä¸”æ˜“æ‡‚\n",
    "2. é•·åº¦æ§åˆ¶åœ¨200å­—ä»¥å…§\n",
    "3. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”\n",
    "\n",
    "è«‹é–‹å§‹ä»‹ç´¹ï¼š\"\"\"\n",
    ")\n",
    "\n",
    "print(\"âœ… æç¤ºæ¨¡æ¿å»ºç«‹æˆåŠŸï¼\")\n",
    "print(\"ğŸ“‹ æ¨¡æ¿å…§å®¹ï¼š\")\n",
    "print(prompt.template)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e399f3",
   "metadata": {},
   "source": [
    "## ğŸ”§ æ­¥é©Ÿ 4ï¼šå»ºç«‹è¼¸å‡ºè§£æå™¨ (Output Parser)\n",
    "\n",
    "è¼¸å‡ºè§£æå™¨ç”¨æ–¼å°‡ LLM çš„å›æ‡‰è½‰æ›æˆæˆ‘å€‘éœ€è¦çš„æ ¼å¼ã€‚é€™è£¡æˆ‘å€‘ä½¿ç”¨ `StrOutputParser` ä¾†å–å¾—ç´”æ–‡å­—å›æ‡‰ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eac5e3d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å»ºç«‹è¼¸å‡ºè§£æå™¨\n",
    "output_parser = StrOutputParser()\n",
    "\n",
    "print(\"âœ… è¼¸å‡ºè§£æå™¨å»ºç«‹æˆåŠŸï¼\")\n",
    "print(\"ğŸ“‹ è§£æå™¨é¡å‹ï¼šStrOutputParser\")\n",
    "print(\"ğŸ¯ åŠŸèƒ½ï¼šå°‡ LLM å›æ‡‰è½‰æ›ç‚ºç´”æ–‡å­—å­—ä¸²\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "271bb295",
   "metadata": {},
   "source": [
    "## ğŸ”— æ­¥é©Ÿ 5ï¼šä½¿ç”¨ LCEL èªæ³•å»ºç«‹åŸºç¤éˆ\n",
    "\n",
    "é€™æ˜¯æ•´å€‹æ•™å­¸çš„æ ¸å¿ƒï¼æˆ‘å€‘ä½¿ç”¨ç®¡é“ç¬¦è™Ÿ `|` å°‡æ‰€æœ‰å…ƒä»¶ä¸²è¯èµ·ä¾†ã€‚\n",
    "\n",
    "### ğŸ’¡ LCEL èªæ³•èªªæ˜\n",
    "```python\n",
    "chain = prompt | model | output_parser\n",
    "```\n",
    "\n",
    "**è³‡æ–™æµå‘**ï¼š\n",
    "1. `prompt` æ¥æ”¶è¼¸å…¥ä¸¦æ ¼å¼åŒ–\n",
    "2. `model` è™•ç†æ ¼å¼åŒ–å¾Œçš„æç¤º\n",
    "3. `output_parser` è§£ææ¨¡å‹å›æ‡‰\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc6fd208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ LCEL èªæ³•å»ºç«‹åŸºç¤éˆ\n",
    "chain = prompt | model | output_parser\n",
    "\n",
    "print(\"âœ… åŸºç¤éˆå»ºç«‹æˆåŠŸï¼\")\n",
    "print(\"ğŸ”— éˆçš„çµæ§‹ï¼š\")\n",
    "print(\"   prompt â†’ model â†’ output_parser\")\n",
    "print(\"\\nğŸ“‹ éˆçš„é¡å‹ï¼š\", type(chain))\n",
    "print(\"ğŸ¯ æº–å‚™å¥½åŸ·è¡Œç¬¬ä¸€å€‹åŸºç¤éˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df064d27",
   "metadata": {},
   "source": [
    "## ğŸš€ æ­¥é©Ÿ 6ï¼šåŸ·è¡ŒåŸºç¤éˆ\n",
    "\n",
    "ç¾åœ¨è®“æˆ‘å€‘ä¾†æ¸¬è©¦æˆ‘å€‘å»ºç«‹çš„åŸºç¤éˆï¼æˆ‘å€‘æœƒä½¿ç”¨ `invoke()` æ–¹æ³•ä¾†åŸ·è¡Œéˆã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e790a1a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æº–å‚™è¼¸å…¥è³‡æ–™\n",
    "input_data = {\n",
    "    \"role\": \"AI å°ˆå®¶\",\n",
    "    \"style\": \"ç°¡æ½”æ˜ç­\",\n",
    "    \"topic\": \"äººå·¥æ™ºæ…§\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ“ è¼¸å…¥è³‡æ–™ï¼š\")\n",
    "for key, value in input_data.items():\n",
    "    print(f\"   {key}: {value}\")\n",
    "\n",
    "print(\"\\nğŸ”„ æ­£åœ¨åŸ·è¡ŒåŸºç¤éˆ...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# åŸ·è¡ŒåŸºç¤éˆ\n",
    "result = chain.invoke(input_data)\n",
    "\n",
    "print(\"=\" * 50)\n",
    "print(\"âœ… åŸºç¤éˆåŸ·è¡Œå®Œæˆï¼\")\n",
    "print(\"\\nğŸ“‹ å›æ‡‰çµæœï¼š\")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b2848dd",
   "metadata": {},
   "source": [
    "## ğŸ¯ æ­¥é©Ÿ 7ï¼šå˜—è©¦ä¸åŒçš„è¼¸å…¥\n",
    "\n",
    "è®“æˆ‘å€‘å˜—è©¦ä¸åŒçš„ä¸»é¡Œï¼Œçœ‹çœ‹åŸºç¤éˆå¦‚ä½•è™•ç†ä¸åŒçš„è¼¸å…¥ï¼\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b3e391b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# å˜—è©¦ä¸åŒçš„ä¸»é¡Œ\n",
    "topics_to_try = [\n",
    "    {\n",
    "        \"role\": \"ç§‘æŠ€è¨˜è€…\",\n",
    "        \"style\": \"ç”Ÿå‹•æœ‰è¶£\",\n",
    "        \"topic\": \"å€å¡ŠéˆæŠ€è¡“\"\n",
    "    },\n",
    "    {\n",
    "        \"role\": \"æ•™è‚²å°ˆå®¶\",\n",
    "        \"style\": \"å°ˆæ¥­è©³ç´°\",\n",
    "        \"topic\": \"æ©Ÿå™¨å­¸ç¿’\"\n",
    "    }\n",
    "]\n",
    "\n",
    "for i, input_data in enumerate(topics_to_try, 1):\n",
    "    print(f\"\\nğŸ¯ æ¸¬è©¦ {i}: {input_data['topic']}\")\n",
    "    print(f\"ğŸ‘¤ è§’è‰²: {input_data['role']}\")\n",
    "    print(f\"ğŸ¨ é¢¨æ ¼: {input_data['style']}\")\n",
    "    print(\"=\" * 50)\n",
    "    \n",
    "    # åŸ·è¡ŒåŸºç¤éˆ\n",
    "    result = chain.invoke(input_data)\n",
    "    print(result)\n",
    "    print(\"=\" * 50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bbf6aa",
   "metadata": {},
   "source": [
    "## ğŸ”„ æ­¥é©Ÿ 8ï¼šæ¢ç´¢é€²éšåŠŸèƒ½\n",
    "\n",
    "åŸºç¤éˆä¸åƒ…æ”¯æ´ `invoke()`ï¼Œé‚„æ”¯æ´å…¶ä»–å¼·å¤§çš„åŠŸèƒ½ï¼š\n",
    "\n",
    "### ğŸ“Š æ‰¹æ¬¡è™•ç† (Batch Processing)\n",
    "åŒæ™‚è™•ç†å¤šå€‹è¼¸å…¥ï¼Œæé«˜æ•ˆç‡\n",
    "\n",
    "### ğŸŒŠ ä¸²æµè¼¸å‡º (Streaming)\n",
    "å³æ™‚é¡¯ç¤ºå›æ‡‰ï¼Œæå‡ä½¿ç”¨è€…é«”é©—\n",
    "\n",
    "### âš¡ éåŒæ­¥è™•ç† (Async)\n",
    "æ”¯æ´éåŒæ­¥èª¿ç”¨ï¼Œé©åˆé«˜ä½µç™¼å ´æ™¯\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565a4658",
   "metadata": {},
   "outputs": [],
   "source": [
    "# æ‰¹æ¬¡è™•ç†ç¯„ä¾‹\n",
    "print(\"ğŸ“Š æ‰¹æ¬¡è™•ç†ç¯„ä¾‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "batch_inputs = [\n",
    "    {\"role\": \"æ­·å²å­¸å®¶\", \"style\": \"å­¸è¡“åš´è¬¹\", \"topic\": \"å¤åŸƒåŠæ–‡æ˜\"},\n",
    "    {\"role\": \"æ—…éŠé”äºº\", \"style\": \"è¼•é¬†æ„‰å¿«\", \"topic\": \"æ—¥æœ¬æ–‡åŒ–\"},\n",
    "    {\"role\": \"ç§‘å­¸å®¶\", \"style\": \"é‚è¼¯æ¸…æ™°\", \"topic\": \"é‡å­ç‰©ç†\"}\n",
    "]\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨æ‰¹æ¬¡è™•ç†...\")\n",
    "batch_results = chain.batch(batch_inputs)\n",
    "\n",
    "for i, (input_data, result) in enumerate(zip(batch_inputs, batch_results), 1):\n",
    "    print(f\"\\nğŸ“ æ‰¹æ¬¡ {i}: {input_data['topic']}\")\n",
    "    print(f\"ğŸ‘¤ è§’è‰²: {input_data['role']}\")\n",
    "    print(f\"ğŸ¨ é¢¨æ ¼: {input_data['style']}\")\n",
    "    print(\"ğŸ“‹ å›æ‡‰:\")\n",
    "    print(result[:100] + \"...\" if len(result) > 100 else result)\n",
    "    print(\"-\" * 30)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0dd2fa60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸²æµè¼¸å‡ºç¯„ä¾‹\n",
    "print(\"\\nğŸŒŠ ä¸²æµè¼¸å‡ºç¯„ä¾‹\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "stream_input = {\n",
    "    \"role\": \"è©©äºº\",\n",
    "    \"style\": \"å„ªç¾è©©æ„\",\n",
    "    \"topic\": \"æ˜¥å¤©çš„èŠ±æœµ\"\n",
    "}\n",
    "\n",
    "print(\"ğŸ”„ æ­£åœ¨ä¸²æµè¼¸å‡º...\")\n",
    "print(\"ğŸ“‹ å³æ™‚å›æ‡‰:\")\n",
    "print(\"-\" * 30)\n",
    "\n",
    "# ä¸²æµè¼¸å‡º\n",
    "for chunk in chain.stream(stream_input):\n",
    "    print(chunk, end=\"\", flush=True)\n",
    "\n",
    "print(\"\\n\" + \"-\" * 30)\n",
    "print(\"âœ… ä¸²æµè¼¸å‡ºå®Œæˆï¼\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b07f537c",
   "metadata": {},
   "source": [
    "## ğŸ“ å­¸ç¿’ç¸½çµ\n",
    "\n",
    "æ­å–œï¼æ‚¨å·²ç¶“æˆåŠŸå­¸æœƒäº† LangChain åŸºç¤éˆçš„æ ¸å¿ƒæ¦‚å¿µï¼š\n",
    "\n",
    "### âœ… æ‚¨å­¸æœƒäº†ä»€éº¼ï¼Ÿ\n",
    "1. **LCEL èªæ³•**ï¼šä½¿ç”¨ `|` ç®¡é“ç¬¦è™Ÿä¸²è¯å…ƒä»¶\n",
    "2. **åŸºç¤éˆçµæ§‹**ï¼šPrompt â†’ Model â†’ Parser\n",
    "3. **ä¸‰ç¨®åŸ·è¡Œæ–¹å¼**ï¼š\n",
    "   - `invoke()` - å–®æ¬¡åŸ·è¡Œ\n",
    "   - `batch()` - æ‰¹æ¬¡è™•ç†\n",
    "   - `stream()` - ä¸²æµè¼¸å‡º\n",
    "\n",
    "### ğŸ”‘ é—œéµæ¦‚å¿µ\n",
    "- **æ¨¡çµ„åŒ–è¨­è¨ˆ**ï¼šæ¯å€‹å…ƒä»¶éƒ½æœ‰ç‰¹å®šåŠŸèƒ½\n",
    "- **è³‡æ–™æµè½‰æ›**ï¼šè‡ªå‹•è™•ç†ä¸åŒæ ¼å¼é–“çš„è½‰æ›\n",
    "- **éˆæ´»æ€§**ï¼šè¼•é¬†æ›¿æ›æˆ–çµ„åˆä¸åŒå…ƒä»¶\n",
    "\n",
    "### ğŸš€ ä¸‹ä¸€æ­¥å»ºè­°\n",
    "1. å­¸ç¿’ **æ“´å±•éˆ** - åŠ å…¥è‡ªå®šç¾©è™•ç†é‚è¼¯\n",
    "2. æ¢ç´¢ **ä¸¦è¡Œéˆ** - åŒæ™‚åŸ·è¡Œå¤šå€‹ä»»å‹™\n",
    "3. å˜—è©¦ **åˆ†æ”¯éˆ** - æ ¹æ“šæ¢ä»¶é¸æ“‡ä¸åŒè·¯å¾‘\n",
    "\n",
    "---\n",
    "\n",
    "## ğŸ’¡ å¯¦ç”¨æŠ€å·§\n",
    "\n",
    "### ğŸ”§ é™¤éŒ¯æŠ€å·§\n",
    "```python\n",
    "# æª¢æŸ¥éˆçš„çµæ§‹\n",
    "print(chain.get_graph().draw_mermaid())\n",
    "\n",
    "# æŸ¥çœ‹ä¸­é–“æ­¥é©Ÿ\n",
    "for step in chain.stream(input_data, config={\"debug\": True}):\n",
    "    print(step)\n",
    "```\n",
    "\n",
    "### âš¡ æ•ˆèƒ½å„ªåŒ–\n",
    "```python\n",
    "# èª¿æ•´æ¨¡å‹åƒæ•¸\n",
    "model = OllamaLLM(\n",
    "    model=\"llama3.2:latest\",\n",
    "    temperature=0.3,  # é™ä½å‰µé€ æ€§ï¼Œæé«˜ä¸€è‡´æ€§\n",
    "    num_predict=100   # é™åˆ¶å›æ‡‰é•·åº¦\n",
    ")\n",
    "```\n",
    "\n",
    "---\n",
    "\n",
    "**ğŸ‰ æ‚¨å·²ç¶“æŒæ¡äº† LangChain åŸºç¤éˆï¼æº–å‚™å¥½é€²å…¥ä¸‹ä¸€å€‹å­¸ç¿’éšæ®µäº†å—ï¼Ÿ**\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
