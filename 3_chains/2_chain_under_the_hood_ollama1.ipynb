{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chain æ‹†è§£å¯¦ä½œç¯„ä¾‹ - ç°¡åŒ–ç‰ˆ\n",
    "\n",
    "æœ¬ç¯„ä¾‹å±•ç¤ºå¦‚ä½•å°‡ LCEL Chain æ‹†è§£æˆ RunnableLambdaï¼Œå¹«åŠ©ç†è§£ Chain çš„å…§éƒ¨é‹ä½œæ©Ÿåˆ¶ã€‚\n",
    "\n",
    "## å­¸ç¿’ç›®æ¨™\n",
    "- ç†è§£ Chain çš„çµ„æˆå…ƒä»¶\n",
    "- å­¸æœƒå°‡è™•ç†æ­¥é©Ÿæ‹†è§£æˆç¨ç«‹å‡½æ•¸\n",
    "- æŒæ¡ RunnableLambda çš„ä½¿ç”¨æ–¹æ³•\n",
    "- æ¯”è¼ƒ LCEL èªæ³•èˆ‡æ‰‹å‹•çµ„åˆçš„å·®ç•°"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 1: å°å…¥å¥—ä»¶ä¸¦è¨­å®šæ¨¡å‹"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
    "from langchain_ollama.llms import OllamaLLM\n",
    "\n",
    "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
    "load_dotenv()\n",
    "\n",
    "# å»ºç«‹æ¨¡å‹\n",
    "model = OllamaLLM(model=\"llama3.2:latest\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 2: å»ºç«‹æç¤ºæ¨¡æ¿"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æç¤ºæ¨¡æ¿å»ºç«‹å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# å»ºç«‹ä¸€å€‹ç°¡å–®çš„å•ç­”ç³»çµ±\n",
    "prompt_template = ChatPromptTemplate.from_messages([\n",
    "    (\"system\", \"ä½ æ˜¯ä¸€å€‹å‹å–„çš„åŠ©æ‰‹ï¼Œè«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”å•é¡Œã€‚\"),\n",
    "    (\"human\", \"{question}\")\n",
    "])\n",
    "\n",
    "print(\"æç¤ºæ¨¡æ¿å»ºç«‹å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 3: ä½¿ç”¨ LCEL èªæ³•ï¼ˆå‚³çµ±æ–¹å¼ï¼‰\n",
    "\n",
    "å…ˆçœ‹çœ‹æˆ‘å€‘ç¿’æ…£çš„å¯«æ³•ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LCEL å›ç­”ï¼š\n",
      "Python æ˜¯ä¸€å€‹å¼·å¤§çš„ã€æ˜“å­¸çš„Programmingèªè¨€ï¼Œå¸¸è¢«ç”¨æ–¼é–‹ç™¼ä¸åŒé ˜åŸŸçš„æ‡‰ç”¨ç¨‹å¼ã€‚å®ƒæ˜¯ä¸€ç¨®é«˜ç´šèªè¨€ï¼Œä¸»è¦ç”¨æ–¼å‰µå»ºå¤§å‹è¨ˆç®—æ©Ÿç¨‹å¼ã€ç¶²è·¯æ‡‰ç”¨ç¨‹å¼å’ŒScientific Computingç­‰æ–¹é¢ã€‚ Python çš„å„ªé»åŒ…æ‹¬ï¼š \n",
      "\n",
      "*   è¶…å¿«çš„ç¨‹åºåŸ·è¡Œé€Ÿåº¦\n",
      "*   ç°¡å–®æ˜“å­¸çš„ç¨‹åºé¢¨æ ¼\n",
      "*   å…è¨±å¤šç¬¬ä¸‰æ–¹åº«å’Œæ¡†æ¶çš„ä½¿ç”¨\n",
      "*   è·¨platform compatibilityï¼ˆå¯åœ¨ä¸åŒoperating systemä¸Šé‹è¡Œï¼‰\n"
     ]
    }
   ],
   "source": [
    "# ä½¿ç”¨ LCEL èªæ³•å»ºç«‹ Chain\n",
    "simple_chain = prompt_template | model\n",
    "\n",
    "# æ¸¬è©¦åŸ·è¡Œ\n",
    "result = simple_chain.invoke({\"question\": \"ä»€éº¼æ˜¯ Python?\"})\n",
    "print(\"LCEL å›ç­”ï¼š\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 4: æ‹†è§£æˆç¨ç«‹å‡½æ•¸\n",
    "\n",
    "ç¾åœ¨æˆ‘å€‘ä¾†ç†è§£ Chain å…§éƒ¨åšäº†ä»€éº¼äº‹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‹†è§£å‡½æ•¸å®šç¾©å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# æ‹†è§£æ­¥é©Ÿ 1: æ ¼å¼åŒ–æç¤º\n",
    "def step1_format_prompt(input_dict):\n",
    "    \"\"\"å°‡è¼¸å…¥å­—å…¸è½‰æ›æˆæ ¼å¼åŒ–çš„æç¤º\"\"\"\n",
    "    print(f\"\\n[æ­¥é©Ÿ1] è¼¸å…¥: {input_dict}\")\n",
    "    formatted = prompt_template.format_prompt(**input_dict)\n",
    "    print(f\"[æ­¥é©Ÿ1] è¼¸å‡ºé¡å‹: {type(formatted).__name__}\")\n",
    "    return formatted\n",
    "\n",
    "# æ‹†è§£æ­¥é©Ÿ 2: å‘¼å«æ¨¡å‹\n",
    "def step2_call_model(formatted_prompt):\n",
    "    \"\"\"å°‡æ ¼å¼åŒ–çš„æç¤ºå‚³çµ¦æ¨¡å‹\"\"\"\n",
    "    print(f\"\\n[æ­¥é©Ÿ2] è¼¸å…¥é¡å‹: {type(formatted_prompt).__name__}\")\n",
    "    messages = formatted_prompt.to_messages()\n",
    "    response = model.invoke(messages)\n",
    "    print(f\"[æ­¥é©Ÿ2] è¼¸å‡ºé¡å‹: {type(response).__name__}\")\n",
    "    return response\n",
    "\n",
    "# æ‹†è§£æ­¥é©Ÿ 3: å›å‚³çµæœ\n",
    "def step3_return_result(response):\n",
    "    \"\"\"å›å‚³æœ€çµ‚çµæœ\"\"\"\n",
    "    print(f\"\\n[æ­¥é©Ÿ3] è¼¸å…¥é¡å‹: {type(response).__name__}\")\n",
    "    print(f\"[æ­¥é©Ÿ3] æœ€çµ‚è¼¸å‡º: {response[:50]}...\" if len(response) > 50 else f\"[æ­¥é©Ÿ3] æœ€çµ‚è¼¸å‡º: {response}\")\n",
    "    return response\n",
    "\n",
    "print(\"æ‹†è§£å‡½æ•¸å®šç¾©å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 5: ç”¨ RunnableLambda åŒ…è£å‡½æ•¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RunnableLambda åŒ…è£å®Œæˆï¼\n",
      "æ­¥é©Ÿ1: RunnableLambda(step1_format_prompt)\n",
      "æ­¥é©Ÿ2: RunnableLambda(step2_call_model)\n",
      "æ­¥é©Ÿ3: RunnableLambda(step3_return_result)\n"
     ]
    }
   ],
   "source": [
    "# å°‡æ¯å€‹å‡½æ•¸åŒ…è£æˆ RunnableLambda\n",
    "runnable_step1 = RunnableLambda(step1_format_prompt)\n",
    "runnable_step2 = RunnableLambda(step2_call_model)\n",
    "runnable_step3 = RunnableLambda(step3_return_result)\n",
    "\n",
    "print(\"RunnableLambda åŒ…è£å®Œæˆï¼\")\n",
    "print(f\"æ­¥é©Ÿ1: {runnable_step1}\")\n",
    "print(f\"æ­¥é©Ÿ2: {runnable_step2}\")\n",
    "print(f\"æ­¥é©Ÿ3: {runnable_step3}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 6: ç”¨ RunnableSequence çµ„åˆ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "æ‰‹å‹•çµ„åˆçš„ Chain å»ºç«‹å®Œæˆï¼\n"
     ]
    }
   ],
   "source": [
    "# æ‰‹å‹•çµ„åˆ Chain\n",
    "manual_chain = RunnableSequence(\n",
    "    first=runnable_step1,\n",
    "    middle=[runnable_step2],\n",
    "    last=runnable_step3\n",
    ")\n",
    "\n",
    "print(\"æ‰‹å‹•çµ„åˆçš„ Chain å»ºç«‹å®Œæˆï¼\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 7: åŸ·è¡Œä¸¦è§€å¯Ÿè³‡æ–™æµå‹•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "é–‹å§‹åŸ·è¡Œæ‰‹å‹•çµ„åˆçš„ Chain\n",
      "============================================================\n",
      "\n",
      "[æ­¥é©Ÿ1] è¼¸å…¥: {'question': 'ä»€éº¼æ˜¯ LangChain?'}\n",
      "[æ­¥é©Ÿ1] è¼¸å‡ºé¡å‹: ChatPromptValue\n",
      "\n",
      "[æ­¥é©Ÿ2] è¼¸å…¥é¡å‹: ChatPromptValue\n",
      "[æ­¥é©Ÿ2] è¼¸å‡ºé¡å‹: str\n",
      "\n",
      "[æ­¥é©Ÿ3] è¼¸å…¥é¡å‹: str\n",
      "[æ­¥é©Ÿ3] æœ€çµ‚è¼¸å‡º: LangChain æ˜¯ä¸€å€‹é–‹æºçš„ Python åº“ï¼Œæ—¨åœ¨ç°¡åŒ–è‡ªç„¶èªè¨€è™•ç†çš„ä½œæ¥­ã€‚å®ƒæä¾›äº†ä¸€ç¨®æ–¹ä¾¿å’Œæœ‰...\n",
      "\n",
      "============================================================\n",
      "æœ€çµ‚çµæœï¼š\n",
      "============================================================\n",
      "LangChain æ˜¯ä¸€å€‹é–‹æºçš„ Python åº“ï¼Œæ—¨åœ¨ç°¡åŒ–è‡ªç„¶èªè¨€è™•ç†çš„ä½œæ¥­ã€‚å®ƒæä¾›äº†ä¸€ç¨®æ–¹ä¾¿å’Œæœ‰æ•ˆçš„æ–¹å¼ï¼Œç”¨æ–¼å»ºç«‹ã€ç®¡ç†å’Œèª¿æ•´ä¸åŒå‹åˆ¥çš„ language model å’Œå…¶ç›¸é—œå…ƒè³‡æ–™ã€‚\n",
      "\n",
      "LangChain æœƒç”¨ä¾†å¯¦ç¾å¤šç¨®ç”¨é€”ï¼Œä¾‹å¦‚ï¼š\n",
      "\n",
      "*   èªè­˜èˆ‡ç”Ÿæˆæ–‡æœ¬\n",
      "*   æ–‡æœ¬é¡åˆ¥åŒ–\n",
      "*   è‡ªå‹•å°èªè¨€æ•¸æ“šé€²è¡Œåˆ†æå’Œæ•´ç†\n",
      "*   ç­‰ç­‰\n",
      "\n",
      " LangChain æ˜¯ä¸€ç¨®é–‹æºçš„ Python åº“ï¼Œæ‰€ä»¥ä½¿ç”¨å®ƒçš„äººå¯ä»¥é€é open source çš„æ–¹å¼ä¾†ç²å¾—æ›´å¤šçš„åŠŸèƒ½å’Œ customization çš„é¸é …ã€‚\n"
     ]
    }
   ],
   "source": [
    "# åŸ·è¡Œæ‰‹å‹•çµ„åˆçš„ Chain\n",
    "print(\"=\"*60)\n",
    "print(\"é–‹å§‹åŸ·è¡Œæ‰‹å‹•çµ„åˆçš„ Chain\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "result = manual_chain.invoke({\"question\": \"ä»€éº¼æ˜¯ LangChain?\"})\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"æœ€çµ‚çµæœï¼š\")\n",
    "print(\"=\"*60)\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ­¥é©Ÿ 8: æ›´ç°¡æ½”çš„å¯«æ³•ï¼ˆä½¿ç”¨ Lambda è¡¨é”å¼ï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ç°¡æ½”ç‰ˆå›ç­”ï¼š\n",
      "LangChain æ˜¯ä¸€å€‹é–‹æºçš„ç¨‹å¼æ¡†æ¶ï¼Œå¯ä»¥å¹«åŠ©å‰µå»ºå’Œä½¿ç”¨äººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ¨¡å‹ã€‚ä»¥ä¸‹æ˜¯å®ƒçš„ä¸€äº›å„ªé»ï¼š\n",
      "\n",
      "1. **æ˜“æ–¼ä½¿ç”¨**ï¼šLangChain æä¾›äº†ç°¡å–®ä¸”æ¸…æ™°çš„ APIï¼Œè®“ä½¿ç”¨è€…å¯ä»¥è¼•é¬†åœ°æ•´åˆ AI æ¨¡å‹åˆ°è‡ªå·±çš„ç¨‹å¼ä¸­ã€‚\n",
      "2. **å¤šæ¨£æ€§**ï¼šLangChain æ”¯æŒå¤šç¨® AI_frameworksï¼Œå¦‚ Hugging Faceã€PyTorch ç­‰ï¼Œå¯ä»¥èˆ‡ä¸åŒçš„æ¨¡å‹ä¸€èµ·ä½¿ç”¨ã€‚\n",
      "3. **å¯æ“´å±•æ€§**ï¼šLangChain çš„ API å¯ä»¥è¢« customåŒ–å’Œæ“´å±•ï¼Œè®“ä½¿ç”¨è€…å¯ä»¥æ·»åŠ æ–°çš„åŠŸèƒ½æˆ–å¢å¼·ç¾æœ‰çš„åŠŸèƒ½ã€‚\n",
      "4. **å®‰å…¨æ€§**ï¼š LangChain é©åˆä½¿ç”¨ HTTPS å”è­°å‚³éæ•¸æ“šï¼Œä»¥ç¢ºä¿è³‡æ–™çš„å®‰å…¨æ€§å’Œéš±ç§æ¬Šã€‚\n",
      "5. **é–‹æº**ï¼š LangChain æ˜¯ä¸€å€‹å…è²»çš„é–‹æºç¨‹å¼æ¡†æ¶ï¼Œè®“æ‰€æœ‰äººéƒ½å¯ä»¥ä½¿ç”¨å’Œè´ˆèˆ‡ã€‚\n",
      "6. **å¿«é€Ÿé–‹ç™¼**ï¼š LangChain å¯ä»¥å¹«åŠ©æ¸›å°‘ AI ç›¸é—œé–‹ç™¼çš„æ™‚é–“ï¼Œç”±æ–¼å®ƒæä¾›äº†ä¸€å€‹ç°¡å–®å’Œæœ‰æ•ˆçš„ APIã€‚\n",
      "\n",
      "ç¸½ä¹‹ï¼ŒLangChain æ˜¯ä¸€å€‹å„ªé›…ã€æ˜“ç”¨çš„ç¨‹å¼æ¡†æ¶ï¼Œå¯ä»¥å¹«åŠ©å‰µå»ºå’Œä½¿ç”¨äººå·¥æ™ºæ…§æ¨¡å‹ã€‚\n"
     ]
    }
   ],
   "source": [
    "# ä¸éœ€è¦å…ˆå®šç¾©å‡½æ•¸ï¼Œç›´æ¥ç”¨ lambda è¡¨é”å¼\n",
    "compact_chain = RunnableSequence(\n",
    "    first=RunnableLambda(lambda x: prompt_template.format_prompt(**x)),\n",
    "    middle=[RunnableLambda(lambda x: model.invoke(x.to_messages()))],\n",
    "    last=RunnableLambda(lambda x: x)\n",
    ")\n",
    "\n",
    "# æ¸¬è©¦åŸ·è¡Œ\n",
    "result = compact_chain.invoke({\"question\": \"LangChain æœ‰ä»€éº¼å„ªé»?\"})\n",
    "print(\"ç°¡æ½”ç‰ˆå›ç­”ï¼š\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ç¸½çµï¼šä¸‰ç¨®å¯«æ³•æ¯”è¼ƒ\n",
    "\n",
    "### 1ï¸âƒ£ LCEL èªæ³•ï¼ˆæœ€ç°¡æ½”ï¼‰\n",
    "```python\n",
    "chain = prompt_template | model\n",
    "```\n",
    "- âœ… ç¨‹å¼ç¢¼æœ€çŸ­\n",
    "- âœ… é©åˆç”Ÿç”¢ç’°å¢ƒ\n",
    "- âŒ ç„¡æ³•çœ‹åˆ°å…§éƒ¨é‹ä½œ\n",
    "\n",
    "### 2ï¸âƒ£ æ‰‹å‹•æ‹†è§£ï¼ˆæœ€æ¸…æ¥šï¼‰\n",
    "```python\n",
    "def step1_format_prompt(x): ...\n",
    "def step2_call_model(x): ...\n",
    "chain = RunnableSequence(\n",
    "    first=RunnableLambda(step1_format_prompt),\n",
    "    middle=[RunnableLambda(step2_call_model)],\n",
    "    last=...\n",
    ")\n",
    "```\n",
    "- âœ… å®Œå…¨æŒæ§æ¯å€‹æ­¥é©Ÿ\n",
    "- âœ… é©åˆé™¤éŒ¯å’Œå­¸ç¿’\n",
    "- âœ… å¯ä»¥åŠ å…¥è‡ªè¨‚é‚è¼¯\n",
    "- âŒ ç¨‹å¼ç¢¼è¼ƒé•·\n",
    "\n",
    "### 3ï¸âƒ£ Lambda è¡¨é”å¼ï¼ˆæŠ˜è¡·ï¼‰\n",
    "```python\n",
    "chain = RunnableSequence(\n",
    "    first=RunnableLambda(lambda x: ...),\n",
    "    middle=[RunnableLambda(lambda x: ...)],\n",
    "    last=...\n",
    ")\n",
    "```\n",
    "- âœ… æ¯”æ‰‹å‹•æ‹†è§£ç°¡æ½”\n",
    "- âœ… ä»æœ‰å½ˆæ€§å¯èª¿æ•´\n",
    "- âŒ è¤‡é›œé‚è¼¯ä¸æ˜“é–±è®€\n",
    "\n",
    "## ğŸ¯ æ•™å­¸å»ºè­°\n",
    "1. **åˆå­¸è€…**ï¼šå¾æ‰‹å‹•æ‹†è§£é–‹å§‹ï¼Œç†è§£æ¯å€‹æ­¥é©Ÿ\n",
    "2. **é€²éšè€…**ï¼šä½¿ç”¨ Lambda è¡¨é”å¼ï¼Œå¿«é€Ÿèª¿æ•´\n",
    "3. **å°ˆæ¡ˆé–‹ç™¼**ï¼šä½¿ç”¨ LCEL èªæ³•ï¼Œä¿æŒç¨‹å¼ç¢¼ç°¡æ½”\n",
    "\n",
    "## ğŸ’¡ ä½•æ™‚éœ€è¦ RunnableLambdaï¼Ÿ\n",
    "- éœ€è¦**åŠ å…¥æ—¥èªŒè¨˜éŒ„**\n",
    "- éœ€è¦**è³‡æ–™é è™•ç†**ï¼ˆå¦‚æ ¼å¼è½‰æ›ã€é©—è­‰ï¼‰\n",
    "- éœ€è¦**æ¢ä»¶åˆ¤æ–·**ï¼ˆæ ¹æ“šçµæœæ±ºå®šä¸‹ä¸€æ­¥ï¼‰\n",
    "- éœ€è¦**å‘¼å«å¤–éƒ¨ API** æˆ–è³‡æ–™åº«\n",
    "- éœ€è¦**éŒ¯èª¤è™•ç†**å’Œä¾‹å¤–æ•æ‰"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
