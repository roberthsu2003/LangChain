{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. éˆçš„å…§éƒ¨é‹ä½œ (Chains Under the Hood) - Gemini ç‰ˆæœ¬\n",
        "\n",
        "æœ¬ç¯„ä¾‹æ·±å…¥ç†è§£ LCEL éˆçš„å…§éƒ¨å¯¦ä½œï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ RunnableSequence æ‰‹å‹•çµ„åˆéˆçš„å„å€‹æ­¥é©Ÿã€‚\n",
        "\n",
        "## å­¸ç¿’é‡é»\n",
        "- äº†è§£éˆçš„åº•å±¤é‹ä½œæ©Ÿåˆ¶\n",
        "- å­¸ç¿’ RunnableSequence çš„ä½¿ç”¨æ–¹æ³•\n",
        "- ç†è§£ RunnableLambda çš„ä½œç”¨\n",
        "- å°æ¯”æ‰‹å‹•çµ„åˆèˆ‡ LCEL èªæ³•çš„å·®ç•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å°å…¥å¿…è¦çš„å¥—ä»¶\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
        "load_dotenv()\n",
        "\n",
        "# å»ºç«‹ Gemini æ¨¡å‹\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®šç¾©æç¤ºæ¨¡æ¿\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ä½ æ˜¯ä¸€å€‹å–œåŠ‡æ¼”å“¡ï¼Œå°ˆé–€è¬›é—œæ–¼ {topic} çš„ç¬‘è©±ã€‚\"),\n",
        "        (\"human\", \"è«‹å‘Šè¨´æˆ‘ {joke_count} å€‹ç¬‘è©±ã€‚\"),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ‰‹å‹•å»ºç«‹å„å€‹å¯åŸ·è¡Œæ­¥é©Ÿ (Runnable)\n",
        "# é€™äº›æ­¥é©Ÿå°æ‡‰ LCEL éˆä¸­çš„å„å€‹å…ƒä»¶\n",
        "\n",
        "# æ­¥é©Ÿ 1: æ ¼å¼åŒ–æç¤º\n",
        "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
        "\n",
        "# æ­¥é©Ÿ 2: å‘¼å«æ¨¡å‹\n",
        "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
        "\n",
        "# æ­¥é©Ÿ 3: è§£æè¼¸å‡º\n",
        "parse_output = RunnableLambda(lambda x: x.content)\n",
        "\n",
        "print(\"å„å€‹æ­¥é©Ÿå·²å®šç¾©å®Œæˆï¼š\")\n",
        "print(\"1. format_prompt: æ ¼å¼åŒ–æç¤ºæ¨¡æ¿\")\n",
        "print(\"2. invoke_model: å‘¼å« LLM æ¨¡å‹\")\n",
        "print(\"3. parse_output: è§£ææ¨¡å‹è¼¸å‡º\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½¿ç”¨ RunnableSequence æ‰‹å‹•çµ„åˆéˆ\n",
        "# é€™ç›¸ç•¶æ–¼ LCEL çš„ prompt_template | model | StrOutputParser()\n",
        "chain = RunnableSequence(\n",
        "    first=format_prompt, \n",
        "    middle=[invoke_model], \n",
        "    last=parse_output\n",
        ")\n",
        "\n",
        "print(\"æ‰‹å‹•çµ„åˆçš„éˆå·²å»ºç«‹å®Œæˆï¼\")\n",
        "print(\"éˆçš„çµæ§‹ï¼šformat_prompt â†’ invoke_model â†’ parse_output\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŸ·è¡Œæ‰‹å‹•çµ„åˆçš„éˆ\n",
        "response = chain.invoke({\"topic\": \"å¾‹å¸«\", \"joke_count\": 3})\n",
        "\n",
        "# è¼¸å‡ºçµæœ\n",
        "print(\"=\" * 50)\n",
        "print(\"ç¬‘è©±çµæœï¼š\")\n",
        "print(\"=\" * 50)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’¡ é‡é»èªªæ˜\n",
        "\n",
        "### æ‰‹å‹•çµ„åˆ vs LCEL èªæ³•\n",
        "\n",
        "**æ‰‹å‹•çµ„åˆ (RunnableSequence)**:\n",
        "```python\n",
        "chain = RunnableSequence(\n",
        "    first=format_prompt, \n",
        "    middle=[invoke_model], \n",
        "    last=parse_output\n",
        ")\n",
        "```\n",
        "\n",
        "**LCEL èªæ³•**:\n",
        "```python\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "```\n",
        "\n",
        "### å„ªç¼ºé»æ¯”è¼ƒ\n",
        "\n",
        "**æ‰‹å‹•çµ„åˆçš„å„ªé»**:\n",
        "- æ›´æ¸…æ¥šåœ°çœ‹åˆ°æ¯å€‹æ­¥é©Ÿçš„å¯¦ä½œç´°ç¯€\n",
        "- å¯ä»¥å°æ¯å€‹æ­¥é©Ÿé€²è¡Œæ›´ç²¾ç´°çš„æ§åˆ¶\n",
        "- é©åˆå­¸ç¿’å’Œç†è§£éˆçš„å…§éƒ¨é‹ä½œ\n",
        "\n",
        "**LCEL èªæ³•çš„å„ªé»**:\n",
        "- ç¨‹å¼ç¢¼æ›´ç°¡æ½”æ˜“è®€\n",
        "- è‡ªå‹•å…·å‚™ä¸²æµã€æ‰¹æ¬¡è™•ç†ç­‰åŠŸèƒ½\n",
        "- æ˜¯ LangChain æ¨è–¦çš„ç¾ä»£åŒ–åšæ³•\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
