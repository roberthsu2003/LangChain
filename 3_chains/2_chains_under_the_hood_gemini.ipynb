{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 2. éˆçš„å…§éƒ¨é‹ä½œ (Chains Under the Hood) - Gemini ç‰ˆæœ¬\n",
        "\n",
        "æœ¬ç¯„ä¾‹æ·±å…¥ç†è§£ LCEL éˆçš„å…§éƒ¨å¯¦ä½œï¼Œå±•ç¤ºå¦‚ä½•ä½¿ç”¨ RunnableSequence æ‰‹å‹•çµ„åˆéˆçš„å„å€‹æ­¥é©Ÿï¼Œå»ºç«‹ä¸€å€‹æ–‡å­—æ‘˜è¦ç³»çµ±ã€‚\n",
        "\n",
        "## å­¸ç¿’é‡é»\n",
        "- äº†è§£éˆçš„åº•å±¤é‹ä½œæ©Ÿåˆ¶\n",
        "- å­¸ç¿’ RunnableSequence çš„ä½¿ç”¨æ–¹æ³•\n",
        "- ç†è§£ RunnableLambda çš„ä½œç”¨\n",
        "- å°æ¯”æ‰‹å‹•çµ„åˆèˆ‡ LCEL èªæ³•çš„å·®ç•°\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å°å…¥å¿…è¦çš„å¥—ä»¶\n",
        "from dotenv import load_dotenv\n",
        "from langchain.prompts import ChatPromptTemplate\n",
        "from langchain.schema.runnable import RunnableLambda, RunnableSequence\n",
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
        "load_dotenv()\n",
        "\n",
        "# å»ºç«‹ Gemini æ¨¡å‹\n",
        "model = ChatGoogleGenerativeAI(model=\"gemini-2.5-flash\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# å®šç¾©æç¤ºæ¨¡æ¿ - å»ºç«‹æ–‡å­—æ‘˜è¦ç³»çµ±\n",
        "prompt_template = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", \"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„æ–‡å­—æ‘˜è¦åŠ©æ‰‹ï¼Œæ“…é•·å°‡é•·ç¯‡æ–‡ç« æ¿ƒç¸®æˆç°¡æ½”çš„é‡é»æ‘˜è¦ã€‚\"),\n",
        "        (\"human\", \"è«‹å°‡ä»¥ä¸‹æ–‡ç« æ‘˜è¦æˆ {max_length} å­—ä»¥å…§çš„é‡é»ï¼š\\n\\n{article}\"),\n",
        "    ]\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# æ‰‹å‹•å»ºç«‹å„å€‹å¯åŸ·è¡Œæ­¥é©Ÿ (Runnable)\n",
        "# é€™äº›æ­¥é©Ÿå°æ‡‰ LCEL éˆä¸­çš„å„å€‹å…ƒä»¶\n",
        "\n",
        "# æ­¥é©Ÿ 1: æ ¼å¼åŒ–æç¤º\n",
        "format_prompt = RunnableLambda(lambda x: prompt_template.format_prompt(**x))\n",
        "\n",
        "# æ­¥é©Ÿ 2: å‘¼å«æ¨¡å‹\n",
        "invoke_model = RunnableLambda(lambda x: model.invoke(x.to_messages()))\n",
        "\n",
        "# æ­¥é©Ÿ 3: è§£æè¼¸å‡º\n",
        "parse_output = RunnableLambda(lambda x: x.content)\n",
        "\n",
        "print(\"å„å€‹æ­¥é©Ÿå·²å®šç¾©å®Œæˆï¼š\")\n",
        "print(\"1. format_prompt: æ ¼å¼åŒ–æç¤ºæ¨¡æ¿\")\n",
        "print(\"2. invoke_model: å‘¼å« LLM æ¨¡å‹\")\n",
        "print(\"3. parse_output: è§£ææ¨¡å‹è¼¸å‡º\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ä½¿ç”¨ RunnableSequence æ‰‹å‹•çµ„åˆéˆ\n",
        "# é€™ç›¸ç•¶æ–¼ LCEL çš„ prompt_template | model | StrOutputParser()\n",
        "chain = RunnableSequence(\n",
        "    first=format_prompt, \n",
        "    middle=[invoke_model], \n",
        "    last=parse_output\n",
        ")\n",
        "\n",
        "print(\"æ‰‹å‹•çµ„åˆçš„éˆå·²å»ºç«‹å®Œæˆï¼\")\n",
        "print(\"éˆçš„çµæ§‹ï¼šformat_prompt â†’ invoke_model â†’ parse_output\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# åŸ·è¡Œæ‰‹å‹•çµ„åˆçš„éˆ - æ¸¬è©¦æ–‡å­—æ‘˜è¦ç³»çµ±\n",
        "sample_article = \"\"\"\n",
        "äººå·¥æ™ºæ…§ï¼ˆAIï¼‰æ˜¯é›»è…¦ç§‘å­¸çš„ä¸€å€‹åˆ†æ”¯ï¼Œæ—¨åœ¨å‰µå»ºèƒ½å¤ åŸ·è¡Œé€šå¸¸éœ€è¦äººé¡æ™ºèƒ½çš„ä»»å‹™çš„ç³»çµ±ã€‚\n",
        "é€™äº›ä»»å‹™åŒ…æ‹¬å­¸ç¿’ã€æ¨ç†ã€å•é¡Œè§£æ±ºã€æ„ŸçŸ¥å’Œèªè¨€ç†è§£ã€‚AI æŠ€è¡“å·²ç¶“åœ¨è¨±å¤šé ˜åŸŸç”¢ç”Ÿé‡å¤§å½±éŸ¿ï¼Œ\n",
        "åŒ…æ‹¬é†«ç™‚ä¿å¥ã€é‡‘èã€äº¤é€šé‹è¼¸å’Œå¨›æ¨‚ã€‚æ©Ÿå™¨å­¸ç¿’æ˜¯ AI çš„ä¸€å€‹å­é ˜åŸŸï¼Œå®ƒä½¿è¨ˆç®—æ©Ÿèƒ½å¤ å¾æ•¸æ“šä¸­å­¸ç¿’\n",
        "è€Œç„¡éœ€æ˜ç¢ºç·¨ç¨‹ã€‚æ·±åº¦å­¸ç¿’æ˜¯æ©Ÿå™¨å­¸ç¿’çš„ä¸€å€‹åˆ†æ”¯ï¼Œä½¿ç”¨äººå·¥ç¥ç¶“ç¶²çµ¡ä¾†æ¨¡æ“¬äººè…¦çš„å·¥ä½œæ–¹å¼ã€‚\n",
        "éš¨è‘—æŠ€è¡“çš„é€²æ­¥ï¼ŒAI æœ‰æœ›åœ¨æœªä¾†å¹¾å¹´ç¹¼çºŒæ”¹è®Šæˆ‘å€‘çš„ç”Ÿæ´»å’Œå·¥ä½œæ–¹å¼ã€‚\n",
        "\"\"\"\n",
        "\n",
        "response = chain.invoke({\n",
        "    \"article\": sample_article,\n",
        "    \"max_length\": 100\n",
        "})\n",
        "\n",
        "# è¼¸å‡ºçµæœ\n",
        "print(\"=\" * 50)\n",
        "print(\"æ–‡ç« æ‘˜è¦çµæœï¼š\")\n",
        "print(\"=\" * 50)\n",
        "print(response)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## ğŸ’¡ é‡é»èªªæ˜\n",
        "\n",
        "### æ‰‹å‹•çµ„åˆ vs LCEL èªæ³•\n",
        "\n",
        "**æ‰‹å‹•çµ„åˆ (RunnableSequence)**:\n",
        "```python\n",
        "chain = RunnableSequence(\n",
        "    first=format_prompt, \n",
        "    middle=[invoke_model], \n",
        "    last=parse_output\n",
        ")\n",
        "```\n",
        "\n",
        "**LCEL èªæ³•**:\n",
        "```python\n",
        "chain = prompt_template | model | StrOutputParser()\n",
        "```\n",
        "\n",
        "### å„ªç¼ºé»æ¯”è¼ƒ\n",
        "\n",
        "**æ‰‹å‹•çµ„åˆçš„å„ªé»**:\n",
        "- æ›´æ¸…æ¥šåœ°çœ‹åˆ°æ¯å€‹æ­¥é©Ÿçš„å¯¦ä½œç´°ç¯€\n",
        "- å¯ä»¥å°æ¯å€‹æ­¥é©Ÿé€²è¡Œæ›´ç²¾ç´°çš„æ§åˆ¶\n",
        "- é©åˆå­¸ç¿’å’Œç†è§£éˆçš„å…§éƒ¨é‹ä½œ\n",
        "\n",
        "**LCEL èªæ³•çš„å„ªé»**:\n",
        "- ç¨‹å¼ç¢¼æ›´ç°¡æ½”æ˜“è®€\n",
        "- è‡ªå‹•å…·å‚™ä¸²æµã€æ‰¹æ¬¡è™•ç†ç­‰åŠŸèƒ½\n",
        "- æ˜¯ LangChain æ¨è–¦çš„ç¾ä»£åŒ–åšæ³•\n",
        "\n",
        "## ğŸ”§ å¯¦éš›æ‡‰ç”¨å ´æ™¯\n",
        "\n",
        "- **æ–°èæ‘˜è¦**: è‡ªå‹•ç”Ÿæˆæ–°èé‡é»\n",
        "- **å­¸è¡“è«–æ–‡**: æå–ç ”ç©¶é‡é»\n",
        "- **æœƒè­°è¨˜éŒ„**: æ•´ç†æœƒè­°è¦é»\n",
        "- **é•·æ–‡é–±è®€**: å¿«é€Ÿäº†è§£æ–‡ç« å…§å®¹\n"
      ]
    }
  ],
  "metadata": {
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
