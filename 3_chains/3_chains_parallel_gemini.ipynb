{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. ä¸¦è¡Œéˆ (Chains Parallel) - Gemini ç‰ˆæœ¬\n",
    "\n",
    "æœ¬ç¯„ä¾‹å±•ç¤ºå¦‚ä½•åŒæ™‚åŸ·è¡Œå¤šå€‹åˆ†æ”¯ï¼Œå»ºç«‹ä¸€å€‹ç°¡å–®çš„é¤å»³è©•è«–åˆ†æç³»çµ±ï¼ŒåŒæ™‚åˆ†æå„ªé»å’Œç¼ºé»ã€‚\n",
    "\n",
    "## å­¸ç¿’é‡é»\n",
    "- ä½¿ç”¨ RunnableParallel å¯¦ç¾ä¸¦è¡Œè™•ç†\n",
    "- å­¸ç¿’å¦‚ä½•åŒæ™‚åŸ·è¡Œå¤šå€‹åˆ†æä»»å‹™\n",
    "- ç†è§£ä¸¦è¡Œè™•ç†çš„å„ªå‹¢\n",
    "- æŒæ¡è¤‡é›œéˆçš„çµ„åˆæŠ€å·§\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å°å…¥å¿…è¦çš„å¥—ä»¶\n",
    "from dotenv import load_dotenv\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain.schema.runnable import RunnableParallel, RunnableLambda\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "\n",
    "# è¼‰å…¥ç’°å¢ƒè®Šæ•¸\n",
    "load_dotenv()\n",
    "\n",
    "# å»ºç«‹ Gemini æ¨¡å‹\n",
    "model = ChatGoogleGenerativeAI(model=\"gemini-1.5-flash-latest\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©ä¸»è¦çš„æç¤ºæ¨¡æ¿ - é¤å»³è©•è«–åˆ†æ\n",
    "prompt_template = ChatPromptTemplate.from_messages(\n",
    "    [\n",
    "        (\"system\", \"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é¤å»³è©•è«–å®¶ï¼Œæ“…é•·åˆ†æé¤å»³çš„å„ªç¼ºé»ã€‚\"),\n",
    "        (\"human\", \"è«‹åˆ†æé€™å®¶é¤å»³ï¼š{restaurant_name}\"),\n",
    "    ]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©å„ªé»åˆ†æå‡½æ•¸\n",
    "def analyze_pros(restaurant_info):\n",
    "    \"\"\"åˆ†æé¤å»³çš„å„ªé»\"\"\"\n",
    "    pros_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é¤å»³è©•è«–å®¶ï¼Œå°ˆé–€åˆ†æé¤å»³çš„å„ªé»ã€‚\"),\n",
    "            (\"human\", \"æ ¹æ“šé¤å»³è³‡è¨Šï¼š{restaurant_info}ï¼Œè«‹åˆ—å‡ºé€™å®¶é¤å»³çš„å„ªé»ã€‚\"),\n",
    "        ]\n",
    "    )\n",
    "    return pros_template.format_prompt(restaurant_info=restaurant_info)\n",
    "\n",
    "# å®šç¾©ç¼ºé»åˆ†æå‡½æ•¸\n",
    "def analyze_cons(restaurant_info):\n",
    "    \"\"\"åˆ†æé¤å»³çš„ç¼ºé»\"\"\"\n",
    "    cons_template = ChatPromptTemplate.from_messages(\n",
    "        [\n",
    "            (\"system\", \"ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„é¤å»³è©•è«–å®¶ï¼Œå°ˆé–€åˆ†æé¤å»³çš„ç¼ºé»ã€‚\"),\n",
    "            (\"human\", \"æ ¹æ“šé¤å»³è³‡è¨Šï¼š{restaurant_info}ï¼Œè«‹åˆ—å‡ºé€™å®¶é¤å»³çš„ç¼ºé»ã€‚\"),\n",
    "        ]\n",
    "    )\n",
    "    return cons_template.format_prompt(restaurant_info=restaurant_info)\n",
    "\n",
    "print(\"åˆ†æå‡½æ•¸å·²å®šç¾©ï¼š\")\n",
    "print(\"1. analyze_pros: åˆ†æé¤å»³å„ªé»\")\n",
    "print(\"2. analyze_cons: åˆ†æé¤å»³ç¼ºé»\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# å®šç¾©çµæœåˆä½µå‡½æ•¸\n",
    "def combine_pros_cons(pros, cons):\n",
    "    \"\"\"å°‡å„ªé»å’Œç¼ºé»åˆä½µæˆæœ€çµ‚è©•è«–\"\"\"\n",
    "    return f\"\"\"\n",
    "ğŸ½ï¸ é¤å»³è©•è«–åˆ†æå ±å‘Š\n",
    "{'='*50}\n",
    "\n",
    "âœ… å„ªé»åˆ†æï¼š\n",
    "{pros}\n",
    "\n",
    "âŒ ç¼ºé»åˆ†æï¼š\n",
    "{cons}\n",
    "\n",
    "ğŸ“Š ç¸½çµï¼šé€™æ˜¯ä¸€å€‹å¹³è¡¡çš„é¤å»³è©•è«–åˆ†æ\n",
    "\"\"\"\n",
    "\n",
    "print(\"çµæœåˆä½µå‡½æ•¸å·²å®šç¾©ï¼šcombine_pros_cons\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ LCEL ç°¡åŒ–åˆ†æ”¯éˆçš„å»ºç«‹\n",
    "# å„ªé»åˆ†æåˆ†æ”¯\n",
    "pros_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_pros(x)) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "# ç¼ºé»åˆ†æåˆ†æ”¯\n",
    "cons_branch_chain = (\n",
    "    RunnableLambda(lambda x: analyze_cons(x)) | model | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"ä¸¦è¡Œåˆ†æ”¯éˆå·²å»ºç«‹ï¼š\")\n",
    "print(\"1. pros_branch_chain: é¤å»³å„ªé»åˆ†æåˆ†æ”¯\")\n",
    "print(\"2. cons_branch_chain: é¤å»³ç¼ºé»åˆ†æåˆ†æ”¯\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# å»ºç«‹å®Œæ•´çš„ä¸¦è¡Œè™•ç†éˆ\n# ä¿®æ­£ï¼šRunnableParallel ç›´æ¥ä½¿ç”¨éµå€¼å°ï¼Œä¸éœ€è¦ branches åŒ…è£\nchain = (\n    prompt_template\n    | model\n    | StrOutputParser()\n    | RunnableParallel(\n        pros=pros_branch_chain,\n        cons=cons_branch_chain\n    )\n    | RunnableLambda(lambda x: combine_pros_cons(x[\"pros\"], x[\"cons\"]))\n)

print(\"ä¸¦è¡Œè™•ç†éˆå·²å»ºç«‹å®Œæˆï¼\")\nprint(\"éˆçš„æµç¨‹ï¼šé¤å»³è³‡è¨Š â†’ ä¸¦è¡Œåˆ†æ(å„ªé»+ç¼ºé») â†’ åˆä½µçµæœ\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# åŸ·è¡Œä¸¦è¡Œè™•ç†éˆ - æ¸¬è©¦é¤å»³è©•è«–åˆ†æç³»çµ±\n",
    