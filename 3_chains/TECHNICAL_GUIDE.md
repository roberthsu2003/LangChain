# æŠ€è¡“æ·±å…¥æŒ‡å—

> ğŸ“– **é–±è®€æ™‚é–“**ï¼šç´„ 25 åˆ†é˜ | [è¿”å›ä¸»é ](README.md)

æœ¬æŒ‡å—æ·±å…¥æ¢è¨ Chain å’Œ LCEL çš„æŠ€è¡“ç´°ç¯€ï¼Œå¹«åŠ©ä½ ç†è§£åº•å±¤æ©Ÿåˆ¶ä¸¦æŒæ¡æœ€ä½³å¯¦è¸ã€‚

---

## ğŸ“‘ ç›®éŒ„

- [Chain çš„ç”¨é€”å’Œç›®çš„](#-chain-çš„ç”¨é€”å’Œç›®çš„)
- [LCEL æ·±å…¥è§£æ](#-lcel-æ·±å…¥è§£æ)
- [æŠ€è¡“é¸æ“‡æŒ‡å—](#-æŠ€è¡“é¸æ“‡æŒ‡å—)
- [æœ€ä½³å¯¦è¸](#ï¸-æœ€ä½³å¯¦è¸)
- [æ•ˆèƒ½å„ªåŒ–æŠ€å·§](#-æ•ˆèƒ½å„ªåŒ–æŠ€å·§)
- [éŒ¯èª¤è™•ç†ç­–ç•¥](#-éŒ¯èª¤è™•ç†ç­–ç•¥)
- [é™¤éŒ¯æŠ€å·§](#-é™¤éŒ¯æŠ€å·§)

---

## ğŸ¤” Chain çš„ç”¨é€”å’Œç›®çš„

### æ ¸å¿ƒæ¦‚å¿µ

**Chain çš„ä¸»è¦ç›®çš„å°±æ˜¯å°‡å¤šå€‹ AI å…ƒä»¶ï¼ˆå¦‚ Prompt Templatesã€LLMsã€å…¶ä»– Chainsã€è³‡æ–™æª¢ç´¢å·¥å…·ç­‰ï¼‰æŒ‰ç…§ç‰¹å®šé †åºçµ„åˆèµ·ä¾†ï¼Œå½¢æˆä¸€å€‹é€£è²«çš„ã€è‡ªå‹•åŒ–çš„è™•ç†æµç¨‹ã€‚**

### å·¥å» ç”Ÿç”¢ç·šæ¯”å–»

æ‚¨å¯ä»¥æŠŠ Chain æƒ³åƒæˆå·¥å» è£¡çš„ä¸€æ¢ã€Œç”Ÿç”¢ç·šã€ï¼š

```
åŸæ–™ï¼ˆä½¿ç”¨è€…è¼¸å…¥ï¼‰
    â”‚
    â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Prompt Template â”‚ â† ç¬¬ä¸€ç«™ï¼šåŒ…è£æˆæ¨™æº–æ ¼å¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚      LLM        â”‚ â† ç¬¬äºŒç«™ï¼šæ ¸å¿ƒè™•ç†
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Output Parser   â”‚ â† ç¬¬ä¸‰ç«™ï¼šæ•´ç†æˆæœ€çµ‚æ ¼å¼
â””â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”˜
         â”‚
         â–¼
    æœ€çµ‚ç”¢å“
```

### Chain çš„æ ¸å¿ƒå„ªé»

#### 1. æ¨¡çµ„åŒ– (Modularity)
å°‡è¤‡é›œçš„ä»»å‹™æ‹†è§£æˆä¸€å€‹å€‹ç¨ç«‹ã€å¯é‡è¤‡ä½¿ç”¨çš„å…ƒä»¶ã€‚

```python
# æ¯å€‹å…ƒä»¶éƒ½å¯ä»¥å–®ç¨æ¸¬è©¦å’Œé‡ç”¨
prompt = ChatPromptTemplate.from_template("...")
model = OllamaLLM(model="llama3.2:latest")
parser = StrOutputParser()

# å¯ä»¥ç”¨ä¸åŒæ–¹å¼çµ„åˆ
chain1 = prompt | model | parser
chain2 = prompt | model  # ä¸ä½¿ç”¨ parser
chain3 = different_prompt | model | parser  # ä¸åŒçš„ prompt
```

#### 2. è‡ªå‹•åŒ– (Automation)
ä¸€æ—¦å®šç¾©å¥½éˆï¼Œæ•´å€‹æµç¨‹å°±å¯ä»¥è‡ªå‹•åŸ·è¡Œï¼Œä½ åªéœ€è¦æä¾›æœ€åˆçš„è¼¸å…¥ã€‚

```python
chain = prompt | model | parser

# ä¸€è¡Œèª¿ç”¨ï¼Œè‡ªå‹•å®Œæˆæ‰€æœ‰æ­¥é©Ÿ
result = chain.invoke({"topic": "äººå·¥æ™ºæ…§"})
```

#### 3. æ˜“æ–¼ç®¡ç†èˆ‡é™¤éŒ¯
ç•¶æµç¨‹å‡ºéŒ¯æ™‚ï¼Œä½ å¯ä»¥å¾ˆæ¸…æ¥šåœ°æª¢æŸ¥æ˜¯å“ªä¸€å€‹ç’°ç¯€å‡ºäº†å•é¡Œã€‚

```python
# å¯ä»¥åˆ†æ­¥é™¤éŒ¯
step1_output = prompt.invoke({"topic": "AI"})
print("Prompt è¼¸å‡º:", step1_output)

step2_output = model.invoke(step1_output)
print("Model è¼¸å‡º:", step2_output)

final_output = parser.invoke(step2_output)
print("æœ€çµ‚è¼¸å‡º:", final_output)
```

#### 4. éˆæ´»æ€§ (Flexibility)
å¯ä»¥è¼•é¬†åœ°çµ„åˆæˆ–æ›¿æ›éˆä¸­çš„å…ƒä»¶ã€‚

```python
# è¼•é¬†åˆ‡æ›æ¨¡å‹
chain_ollama = prompt | OllamaLLM(model="llama3.2") | parser
chain_gemini = prompt | ChatGoogleGenerativeAI(model="gemini-2.0") | parser

# ä¸éœ€è¦æ”¹å‹•å…¶ä»–éƒ¨åˆ†
```

---

## ğŸ” LCEL æ·±å…¥è§£æ

### ä»€éº¼æ˜¯ LCELï¼Ÿ

**LangChain Expression Language (LCEL)** æ˜¯ LangChain ç¾ä»£åŒ–ã€æ›´æ¨è–¦çš„çµ„åˆéˆçš„æ–¹å¼ã€‚

å®ƒçš„æ ¸å¿ƒèªæ³•æ˜¯ä½¿ç”¨ã€Œ**ç®¡é“ (Pipe) ç¬¦è™Ÿ `|`**ã€ï¼Œæ„ç¾©æ˜¯ã€Œ**å°‡å·¦é‚Šå…ƒä»¶çš„è¼¸å‡ºï¼Œä½œç‚ºå³é‚Šå…ƒä»¶çš„è¼¸å…¥**ã€ã€‚

### å‚³çµ±æ–¹æ³• vs. LCEL æ–¹æ³•

#### å‚³çµ±æ–¹æ³•ï¼ˆå·²éæ™‚ï¼‰

```python
from langchain.chains import LLMChain
from langchain.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# 1. å®šç¾©å„å€‹å…ƒä»¶
llm = ChatOpenAI()
prompt = PromptTemplate.from_template("è«‹ä»‹ç´¹ä¸€ä¸‹ {topic}ã€‚")

# 2. ä½¿ç”¨ LLMChain é¡ä¾†ã€ŒåŒ…è£ã€é€™äº›å…ƒä»¶
chain = LLMChain(llm=llm, prompt=prompt)

# 3. åŸ·è¡Œéˆ
response = chain.run(topic="å¤§å‹èªè¨€æ¨¡å‹")
```

**ç¼ºé»**ï¼š
- âŒ èªæ³•å†—é•·
- âŒ éœ€è¦è¨˜ä½ä¸åŒ Chain é¡çš„åƒæ•¸
- âŒ ä¸æ”¯æ´è‡ªå‹• streamingã€batchã€async
- âŒ çµ„åˆæ€§å·®

#### LCEL æ–¹æ³•ï¼ˆæ¨è–¦ï¼‰â­

```python
from langchain.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# 1. å®šç¾©å„å€‹å…ƒä»¶
prompt = ChatPromptTemplate.from_template("è«‹ä»‹ç´¹ä¸€ä¸‹ {topic}ã€‚")
model = ChatOpenAI()

# 2. ä½¿ç”¨ LCEL çš„ç®¡é“ç¬¦è™Ÿ | ä¾†ã€Œçµ„åˆã€é€™äº›å…ƒä»¶
chain = prompt | model

# 3. åŸ·è¡Œéˆ
response = chain.invoke({"topic": "å¤§å‹èªè¨€æ¨¡å‹"})
```

**å„ªé»**ï¼š
- âœ… èªæ³•ç›´è§€æ˜“è®€
- âœ… ç¨‹å¼ç¢¼å³æ–‡ä»¶
- âœ… è‡ªå‹•æ”¯æ´ streamingã€batchã€async
- âœ… çµ„åˆæ€§å¼·

### LCEL çš„æ ¸å¿ƒå„ªå‹¢

#### 1. ç›´è§€æ˜“è®€

```python
# æ¸…æ¥šåœ°è¡¨é”äº†è³‡æ–™æµå‹•çš„æ–¹å‘
chain = prompt | model | output_parser

# ç¨‹å¼ç¢¼å³æ–‡ä»¶ï¼š
# 1. prompt è™•ç†è¼¸å…¥
# 2. model ç”Ÿæˆå›æ‡‰
# 3. output_parser è§£æè¼¸å‡º
```

#### 2. å¼·å¤§çš„å…§å»ºåŠŸèƒ½

æ‰€æœ‰ç”¨ LCEL å»ºç«‹çš„éˆéƒ½**è‡ªå‹•å…·å‚™**ä»¥ä¸‹èƒ½åŠ›ï¼š

##### Streamingï¼ˆä¸²æµè¼¸å‡ºï¼‰

```python
chain = prompt | model | StrOutputParser()

# åƒ ChatGPT ä¸€æ¨£ï¼Œä¸€å€‹å­—ä¸€å€‹å­—åœ°è¼¸å‡º
for chunk in chain.stream({"topic": "AI"}):
    print(chunk, end="", flush=True)
```

##### Batchï¼ˆæ‰¹æ¬¡è™•ç†ï¼‰

```python
# ä¸€æ¬¡è™•ç†å¤šå€‹è¼¸å…¥ï¼Œä¸¦è¡Œæå‡æ•ˆç‡
inputs = [
    {"topic": "AI"},
    {"topic": "æ©Ÿå™¨å­¸ç¿’"},
    {"topic": "æ·±åº¦å­¸ç¿’"}
]

results = chain.batch(inputs)
# ä¸¦è¡Œè™•ç†ï¼Œé€Ÿåº¦å¿« 3 å€ï¼
```

##### Asyncï¼ˆéåŒæ­¥èª¿ç”¨ï¼‰

```python
# æ”¯æ´éåŒæ­¥ï¼Œé©ç”¨æ–¼é«˜æ•ˆèƒ½å¾Œç«¯
async def process():
    result = await chain.ainvoke({"topic": "AI"})
    return result

# å¯ä»¥åŒæ™‚è™•ç†å¤šå€‹è«‹æ±‚
```

#### 3. çµ„åˆæ€§æ›´å¼·

```python
# å¯ä»¥è¼•æ˜“ä¸²æ¥æ›´è¤‡é›œçš„å…ƒä»¶
from langchain.vectorstores import Chroma
from langchain.schema.runnable import RunnablePassthrough

retriever = vectorstore.as_retriever()

# æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG)
rag_chain = (
    {"context": retriever, "question": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)
```

---

## ğŸ”§ æŠ€è¡“é¸æ“‡æŒ‡å—

### Ollama vs. Gemini

| ç‰¹æ€§ | Ollama | Gemini |
|------|--------|--------|
| **éƒ¨ç½²æ–¹å¼** | ğŸ  æœ¬åœ°éƒ¨ç½² | â˜ï¸ é›²ç«¯ API |
| **éš±ç§æ€§** | â­â­â­â­â­ å®Œå…¨æœ¬åœ° | â­â­â­ æ•¸æ“šä¸Šå‚³è‡³ Google |
| **æˆæœ¬** | âœ… å…è²»ï¼ˆéœ€è¦ç¡¬é«”ï¼‰ | ğŸ’° æŒ‰ä½¿ç”¨é‡è¨ˆè²» |
| **ç¶²è·¯éœ€æ±‚** | âœ… å¯é›¢ç·šä½¿ç”¨ | âŒ éœ€è¦ç¶²è·¯é€£æ¥ |
| **æ•ˆèƒ½** | âš™ï¸ ä¾ç¡¬é«”è€Œå®š | âš¡ ç©©å®šé«˜æ•ˆ |
| **æ¨¡å‹æ›´æ–°** | ğŸ”„ æ‰‹å‹•æ›´æ–° | âœ¨ è‡ªå‹•æœ€æ–° |
| **å¤šèªè¨€æ”¯æ´** | â­â­â­ è‰¯å¥½ | â­â­â­â­â­ å„ªç§€ |
| **ç²¾åº¦** | â­â­â­â­ å„ªç§€ | â­â­â­â­â­ é ‚å°– |

### é¸æ“‡ Ollama çš„æƒ…æ³

- âœ… æœ¬åœ°éƒ¨ç½²éœ€æ±‚
- âœ… è³‡æ–™éš±ç§è¦æ±‚é«˜
- âœ… æˆæœ¬æ§åˆ¶è€ƒé‡
- âœ… é›¢ç·šç’°å¢ƒä½¿ç”¨
- âœ… éœ€è¦å®Œå…¨æ§åˆ¶æ¨¡å‹

### é¸æ“‡ Gemini çš„æƒ…æ³

- âœ… éœ€è¦æœ€æ–°çš„ AI èƒ½åŠ›
- âœ… é›²ç«¯éƒ¨ç½²ç’°å¢ƒ
- âœ… å¤šèªè¨€æ”¯æ´éœ€æ±‚
- âœ… é«˜ç²¾åº¦è¦æ±‚
- âœ… ä¸æƒ³ç¶­è­·æœ¬åœ°ç’°å¢ƒ

### æ··åˆä½¿ç”¨ç­–ç•¥

```python
# é–‹ç™¼æ™‚ç”¨ Ollamaï¼ˆå…è²»ã€å¿«é€Ÿæ¸¬è©¦ï¼‰
dev_model = OllamaLLM(model="llama3.2:latest")

# ç”Ÿç”¢æ™‚ç”¨ Geminiï¼ˆé«˜ç²¾åº¦ã€ç©©å®šï¼‰
prod_model = ChatGoogleGenerativeAI(model="gemini-2.0-flash-exp")

# åŒä¸€å€‹ chain å®šç¾©
prompt = ChatPromptTemplate.from_template("...")
parser = StrOutputParser()

# è¼•é¬†åˆ‡æ›
dev_chain = prompt | dev_model | parser
prod_chain = prompt | prod_model | parser
```

---

## âš™ï¸ æœ€ä½³å¯¦è¸

### 1. å„ªå…ˆä½¿ç”¨å‹•æ…‹æç¤ºéˆ â­

```python
# âœ… æ¨è–¦ï¼šLambda åªæº–å‚™æ•¸æ“š
def prepare_prompt(input_data):
    # æ ¹æ“šå…§å®¹å‹•æ…‹èª¿æ•´
    if "urgent" in input_data:
        priority = "high"
    else:
        priority = "normal"

    return {"content": input_data, "priority": priority}

chain = (
    RunnableLambda(prepare_prompt)
    | dynamic_prompt
    | model
    | StrOutputParser()
)

# âŒ ä¸æ¨è–¦ï¼šåœ¨ Lambda ä¸­ç›´æ¥èª¿ç”¨æ¨¡å‹ï¼ˆé–‰åŒ…æ–¹å¼ï¼‰
def process_with_model(input_data):
    result = model.invoke(...)  # é‚è¼¯éš±è—ï¼Œé›£ä»¥è¿½è¹¤
    return result
```

### 2. ä¿æŒ Chain çš„å¯è®€æ€§

```python
# âœ… å¥½ï¼šæ¸…æ™°çš„è®Šæ•¸å
summarize_prompt = ChatPromptTemplate.from_template("æ‘˜è¦ï¼š{text}")
summarize_chain = summarize_prompt | model | StrOutputParser()

# âŒ ä¸å¥½ï¼šé›£ä»¥ç†è§£
chain = p | m | s
```

### 3. é©ç•¶ä½¿ç”¨è¨»è§£

```python
# âœ… å¥½ï¼šè¤‡é›œé‚è¼¯æ·»åŠ è¨»è§£
chain = (
    prompt
    | model
    | StrOutputParser()
    | RunnableLambda(extract_keywords)  # æå–é—œéµå­—
    | format_as_list  # æ ¼å¼åŒ–ç‚ºåˆ—è¡¨
)
```

### 4. éŒ¯èª¤è™•ç†

```python
# âœ… å¥½ï¼šæ·»åŠ éŒ¯èª¤è™•ç†
def safe_process(data):
    try:
        return process(data)
    except Exception as e:
        logger.error(f"è™•ç†å¤±æ•—: {e}")
        return {"error": str(e)}

chain = prompt | model | RunnableLambda(safe_process)
```

### 5. åˆ†é›¢é…ç½®å’Œé‚è¼¯

```python
# âœ… å¥½ï¼šé…ç½®åˆ†é›¢
MODEL_CONFIG = {
    "temperature": 0.7,
    "max_tokens": 1000
}

model = OllamaLLM(**MODEL_CONFIG)
chain = prompt | model | parser
```

---

## ğŸš€ æ•ˆèƒ½å„ªåŒ–æŠ€å·§

### 1. ä½¿ç”¨ä¸¦è¡Œè™•ç†

```python
# âŒ æ…¢ï¼šé †åºè™•ç†
result1 = chain1.invoke(input1)
result2 = chain2.invoke(input2)
result3 = chain3.invoke(input3)
# ç¸½æ™‚é–“ = æ™‚é–“1 + æ™‚é–“2 + æ™‚é–“3

# âœ… å¿«ï¼šä¸¦è¡Œè™•ç†
parallel_chain = RunnableParallel(
    result1=chain1,
    result2=chain2,
    result3=chain3
)
results = parallel_chain.invoke(input)
# ç¸½æ™‚é–“ â‰ˆ max(æ™‚é–“1, æ™‚é–“2, æ™‚é–“3)
```

### 2. ä½¿ç”¨ Batch è™•ç†å¤šå€‹è¼¸å…¥

```python
# âŒ æ…¢ï¼šé€å€‹è™•ç†
results = []
for input_data in inputs:
    results.append(chain.invoke(input_data))

# âœ… å¿«ï¼šæ‰¹æ¬¡è™•ç†
results = chain.batch(inputs)
# è‡ªå‹•ä¸¦è¡Œï¼Œé€Ÿåº¦æå‡ 3-5 å€
```

### 3. ä½¿ç”¨ Streaming æ”¹å–„ç”¨æˆ¶é«”é©—

```python
# âŒ ç”¨æˆ¶é«”é©—å·®ï¼šç­‰å¾…å…¨éƒ¨å®Œæˆ
result = chain.invoke(input)
print(result)  # ç­‰å¾… 10 ç§’å¾Œä¸€æ¬¡æ€§é¡¯ç¤º

# âœ… ç”¨æˆ¶é«”é©—å¥½ï¼šå³æ™‚é¡¯ç¤º
for chunk in chain.stream(input):
    print(chunk, end="", flush=True)  # å³æ™‚é¡¯ç¤ºï¼Œåƒ ChatGPT
```

### 4. å¿«å–é‡è¤‡æŸ¥è©¢

```python
from functools import lru_cache

@lru_cache(maxsize=100)
def cached_invoke(input_str):
    return chain.invoke({"input": input_str})

# ç›¸åŒè¼¸å…¥æœƒç›´æ¥è¿”å›å¿«å–çµæœï¼Œä¸é‡è¤‡èª¿ç”¨æ¨¡å‹
```

### 5. é¸æ“‡åˆé©çš„æ¨¡å‹å¤§å°

```python
# ç°¡å–®ä»»å‹™ç”¨å°æ¨¡å‹
simple_model = OllamaLLM(model="llama3.2:1b")  # å¿«é€Ÿ

# è¤‡é›œä»»å‹™ç”¨å¤§æ¨¡å‹
complex_model = OllamaLLM(model="llama3.2:70b")  # ç²¾ç¢º
```

---

## ğŸ›¡ï¸ éŒ¯èª¤è™•ç†ç­–ç•¥

### 1. å„ªé›…çš„éŒ¯èª¤è™•ç†

```python
from langchain.schema.runnable import RunnableLambda

def safe_chain_invoke(input_data):
    try:
        result = chain.invoke(input_data)
        return {"success": True, "data": result}
    except Exception as e:
        logger.error(f"Chain åŸ·è¡Œå¤±æ•—: {e}")
        return {"success": False, "error": str(e)}

safe_chain = RunnableLambda(safe_chain_invoke)
```

### 2. é‡è©¦æ©Ÿåˆ¶

```python
import time

def invoke_with_retry(chain, input_data, max_retries=3):
    for attempt in range(max_retries):
        try:
            return chain.invoke(input_data)
        except Exception as e:
            if attempt < max_retries - 1:
                wait_time = 2 ** attempt  # æŒ‡æ•¸é€€é¿
                logger.warning(f"ç¬¬ {attempt + 1} æ¬¡å˜—è©¦å¤±æ•—ï¼Œ{wait_time}ç§’å¾Œé‡è©¦")
                time.sleep(wait_time)
            else:
                logger.error(f"æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—: {e}")
                raise
```

### 3. è¶…æ™‚æ§åˆ¶

```python
from concurrent.futures import ThreadPoolExecutor, TimeoutError

def invoke_with_timeout(chain, input_data, timeout=30):
    with ThreadPoolExecutor() as executor:
        future = executor.submit(chain.invoke, input_data)
        try:
            return future.result(timeout=timeout)
        except TimeoutError:
            logger.error("åŸ·è¡Œè¶…æ™‚")
            return None
```

### 4. è¼¸å…¥é©—è­‰

```python
def validate_and_invoke(chain, input_data):
    # é©—è­‰è¼¸å…¥
    if not input_data or not isinstance(input_data, dict):
        raise ValueError("è¼¸å…¥å¿…é ˆæ˜¯éç©ºå­—å…¸")

    if "topic" not in input_data:
        raise ValueError("ç¼ºå°‘å¿…è¦çš„ 'topic' æ¬„ä½")

    # æ¸…ç†è¼¸å…¥
    cleaned_input = {
        "topic": input_data["topic"].strip()[:500]  # é™åˆ¶é•·åº¦
    }

    return chain.invoke(cleaned_input)
```

---

## ğŸ” é™¤éŒ¯æŠ€å·§

### 1. åˆ†æ­¥é™¤éŒ¯

```python
# æª¢æŸ¥æ¯å€‹æ­¥é©Ÿçš„è¼¸å‡º
chain = prompt | model | parser

# æ¸¬è©¦ prompt
prompt_output = prompt.invoke({"topic": "AI"})
print("Prompt è¼¸å‡º:", prompt_output)

# æ¸¬è©¦ model
model_output = model.invoke(prompt_output)
print("Model è¼¸å‡º:", model_output)

# æ¸¬è©¦ parser
final_output = parser.invoke(model_output)
print("æœ€çµ‚è¼¸å‡º:", final_output)
```

### 2. æ·»åŠ æ—¥èªŒ

```python
import logging

logging.basicConfig(level=logging.DEBUG)
logger = logging.getLogger(__name__)

def logged_process(data):
    logger.debug(f"è¼¸å…¥: {data}")
    result = process(data)
    logger.debug(f"è¼¸å‡º: {result}")
    return result

chain = prompt | model | RunnableLambda(logged_process)
```

### 3. ä½¿ç”¨é™¤éŒ¯å›èª¿

```python
from langchain.callbacks import StdOutCallbackHandler

# é¡¯ç¤ºè©³ç´°çš„åŸ·è¡Œéç¨‹
chain = prompt | model | parser
result = chain.invoke(
    {"topic": "AI"},
    config={"callbacks": [StdOutCallbackHandler()]}
)
```

### 4. å–®å…ƒæ¸¬è©¦

```python
import pytest

def test_chain_output():
    test_input = {"topic": "æ¸¬è©¦"}
    result = chain.invoke(test_input)

    # é©—è­‰è¼¸å‡ºæ ¼å¼
    assert isinstance(result, str)
    assert len(result) > 0
    assert "æ¸¬è©¦" in result

def test_chain_error_handling():
    with pytest.raises(ValueError):
        chain.invoke({"invalid": "input"})
```

---

## ğŸ”— èˆ‡å…¶ä»– LangChain å…ƒä»¶çš„æ•´åˆ

### èˆ‡ Memory æ•´åˆ

```python
from langchain.memory import ConversationBufferMemory

memory = ConversationBufferMemory()

# åœ¨ chain ä¸­ä½¿ç”¨ memory
chain_with_memory = (
    RunnablePassthrough.assign(
        history=lambda x: memory.load_memory_variables({})["history"]
    )
    | prompt
    | model
    | StrOutputParser()
)
```

### èˆ‡ Retriever æ•´åˆï¼ˆRAGï¼‰

```python
from langchain.vectorstores import Chroma
from langchain.schema.runnable import RunnablePassthrough

# RAG Chain
rag_chain = (
    {
        "context": retriever | format_docs,
        "question": RunnablePassthrough()
    }
    | prompt
    | model
    | StrOutputParser()
)
```

### èˆ‡ Agents æ•´åˆ

```python
from langchain.agents import AgentExecutor, create_react_agent

# Chain å¯ä»¥ä½œç‚º Agent çš„å·¥å…·
tools = [
    Tool(
        name="Summarizer",
        func=summarize_chain.invoke,
        description="ç”¨æ–¼æ‘˜è¦æ–‡æœ¬"
    )
]

agent = create_react_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools)
```

---

## ğŸ“š å»¶ä¼¸é–±è®€

- [LangChain å®˜æ–¹æ–‡æª”](https://python.langchain.com/)
- [LCEL è©³ç´°æŒ‡å—](https://python.langchain.com/docs/expression_language/)
- [Runnable æ¥å£èªªæ˜](https://python.langchain.com/docs/expression_language/interface)
- [æ•ˆèƒ½å„ªåŒ–æŒ‡å—](https://python.langchain.com/docs/guides/productionization/)

---

## ğŸ”— ç›¸é—œæ–‡æª”

- ğŸ“– [Chain é¡å‹å®Œæ•´æŒ‡å—](CHAINS_GUIDE.md) - 10 ç¨® Chain è©³ç´°èªªæ˜
- ğŸ“š [å­¸ç¿’è·¯å¾‘å»ºè­°](LEARNING_PATHS.md) - é¸æ“‡é©åˆä½ çš„å­¸ç¿’è·¯ç·š
- ğŸ’¼ [å¯¦æˆ°æ¡ˆä¾‹](PRACTICAL_CASES.md) - çœŸå¯¦æ‡‰ç”¨å ´æ™¯
- ğŸ“‹ [ç¸½çµèˆ‡åƒè€ƒ](SUMMARY.md) - å¿«é€ŸæŸ¥è©¢è¡¨
- [è¿”å›ä¸»é ](README.md)
