{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 基礎入門\n",
    "\n",
    "本範例展示：\n",
    "1. **第1個儲存格**：建立向量資料庫\n",
    "2. **第2個儲存格**：查詢向量資料庫\n",
    "3. **第3個儲存格以後**: 整合chain的功能\n",
    "\n",
    "學習目標：理解如何將文本轉換為向量並進行相似度檢索\n",
    "\n",
    "❗️[**本範例重點程式觀念說明**](./1_rag_basics_說明.md)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第1個儲存格：建立向量資料庫\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "\n",
    "# 定義包含文字檔案的目錄和持久化目錄\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "file_path = os.path.join(current_dir, \"books\", \"智慧型手機使用手冊.txt\")\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_jina\")\n",
    "\n",
    "# 檢查 Chroma 向量存儲是否已存在\n",
    "if not os.path.exists(persistent_directory):\n",
    "    print(\"持久化目錄不存在。正在初始化向量資料庫...\")\n",
    "\n",
    "    # 確保文字檔案存在\n",
    "    if not os.path.exists(file_path):\n",
    "        raise FileNotFoundError(\n",
    "            f\"檔案 {file_path} 不存在。請檢查路徑。\"\n",
    "        )\n",
    "\n",
    "    # 從檔案讀取文字內容\n",
    "    loader = TextLoader(file_path)\n",
    "    documents = loader.load()\n",
    "\n",
    "    # 將文件分割成塊\n",
    "    # chunk_size=1000: 每個文本區塊最多 1000 個字元\n",
    "    # chunk_overlap=0: 區塊之間不重疊\n",
    "    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "    docs = text_splitter.split_documents(documents)\n",
    "\n",
    "    # 顯示分割文件的資訊\n",
    "    print(\"\\n--- 文件塊資訊 ---\")\n",
    "    print(f\"文件塊數量: {len(docs)}\")\n",
    "    print(f\"範例塊:\\n{docs[0].page_content}\\n\")\n",
    "\n",
    "    # 建立嵌入模型\n",
    "    print(\"\\n--- 正在建立嵌入 ---\")\n",
    "    print(\"使用 Jina Embeddings v2（繁體中文開源模型）\")\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n",
    "    )\n",
    "    print(\"\\n--- 完成建立嵌入 ---\")\n",
    "\n",
    "    # 建立向量存儲並自動持久化\n",
    "    print(\"\\n--- 正在建立向量存儲 ---\")\n",
    "    \n",
    "    # 使用 PersistentClient 以避免權限問題\n",
    "    client = chromadb.PersistentClient(path=persistent_directory)\n",
    "    \n",
    "    db = Chroma.from_documents(\n",
    "        docs, \n",
    "        embeddings, \n",
    "        client=client,\n",
    "        collection_name=\"smartphone_manual\"\n",
    "    )\n",
    "    print(\"\\n--- 完成建立向量存儲 ---\")\n",
    "\n",
    "else:\n",
    "    print(\"向量存儲已存在。無需初始化。\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第2個儲存格：查詢向量資料庫\n",
    "\n",
    "import os\n",
    "from langchain_chroma import Chroma\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "import chromadb\n",
    "\n",
    "# 定義持久化目錄\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_jina\")\n",
    "\n",
    "# 定義嵌入模型（使用開源繁體中文模型）\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n",
    "\n",
    "# 使用 PersistentClient 載入現有的向量存儲\n",
    "client = chromadb.PersistentClient(path=persistent_directory)\n",
    "\n",
    "db = Chroma(\n",
    "    client=client,\n",
    "    collection_name=\"smartphone_manual\",\n",
    "    embedding_function=embeddings\n",
    ")\n",
    "\n",
    "# 定義使用者的問題\n",
    "query = \"如何設定指紋辨識？\"\n",
    "\n",
    "# 根據查詢檢索相關文件\n",
    "# search_type=\"similarity\": 使用相似度搜尋（預設方法）\n",
    "# k=3: 返回最相關的 3 個文件\n",
    "# 注意：由於某些 embedding 模型的相似度分數可能不在 0-1 範圍內，\n",
    "#      因此使用簡單的 similarity 搜尋而非 similarity_score_threshold\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 3},\n",
    ")\n",
    "relevant_docs = retriever.invoke(query)\n",
    "\n",
    "# 顯示相關結果及元數據\n",
    "print(\"\\n--- 相關文件 ---\")\n",
    "for i, doc in enumerate(relevant_docs, 1):\n",
    "    print(f\"文件 {i}:\\n{doc.page_content}\\n\")\n",
    "    if doc.metadata:\n",
    "        print(f\"來源: {doc.metadata.get('source', 'Unknown')}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第4個儲存格：整合 Chain - 使用 RunnableLambda 的傳統做法\n",
    "# 這是函數式程式設計的傳統方法\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 定義使用者的問題\n",
    "question = \"如何設定指紋辨識？\"\n",
    "\n",
    "print(f\"問題: {question}\\n\")\n",
    "\n",
    "# 步驟 1: 從向量資料庫檢索相關文件\n",
    "print(\"=\" * 60)\n",
    "print(\"步驟 1: 從向量資料庫檢索相關文件\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}  # 只取最相關的 2 個文件\n",
    ")\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "print(f\"找到 {len(retrieved_docs)} 個相關文件\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"文件 {i} (前100字):\")\n",
    "    print(doc.page_content[:100] + \"...\\n\")\n",
    "\n",
    "# 步驟 2: 建立 RAG Chain（使用 RunnableLambda 傳統做法）\n",
    "print(\"=\" * 60)\n",
    "print(\"步驟 2: 建立 RAG Chain（RunnableLambda 傳統做法）\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 定義提示模板\n",
    "template = \"\"\"你是一個智慧型手機的客服助手。請根據以下參考資料回答使用者的問題。\n",
    "\n",
    "參考資料：\n",
    "{context}\n",
    "\n",
    "使用者問題：{question}\n",
    "\n",
    "請用繁體中文回答，並且：\n",
    "1. 只根據參考資料回答，不要編造內容\n",
    "2. 如果參考資料中沒有答案，請誠實說「我在資料中找不到相關資訊」\n",
    "3. 回答要清楚、具體、有條理\n",
    "\n",
    "回答：\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 建立 LLM（使用本地 Ollama）\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "# 定義文件格式化函數\n",
    "def format_docs(docs):\n",
    "    \"\"\"將檢索到的文件格式化為字串\"\"\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 定義檢索函數\n",
    "def retrieve_docs(question):\n",
    "    \"\"\"根據問題檢索相關文件\"\"\"\n",
    "    docs = retriever.invoke(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "# 定義組合函數\n",
    "def combine_context_and_question(inputs):\n",
    "    \"\"\"組合檢索到的文件和原始問題\"\"\"\n",
    "    context = inputs[\"context\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return {\"context\": context, \"question\": question}\n",
    "\n",
    "# 建立完整的 RAG Chain（使用 RunnableLambda）\n",
    "rag_chain = (\n",
    "    RunnableLambda(lambda x: {\"context\": retrieve_docs(x), \"question\": x})\n",
    "    | RunnableLambda(combine_context_and_question)\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ RAG Chain 建立完成（RunnableLambda 傳統做法）\\n\")\n",
    "\n",
    "# 步驟 3: 執行 RAG Chain 並取得答案\n",
    "print(\"=\" * 60)\n",
    "print(\"步驟 3: 執行 RAG Chain\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"\\n【AI 回答】\\n{answer}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RAG Chain 流程總結（RunnableLambda 做法）\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. RunnableLambda(lambda x: {...}) → 檢索文件並組合輸入\")\n",
    "print(\"2. RunnableLambda(combine_context_and_question) → 格式化輸入\")\n",
    "print(\"3. prompt → 建立提示詞\")\n",
    "print(\"4. llm → 生成回答\")\n",
    "print(\"5. StrOutputParser → 解析輸出\")\n",
    "print(\"\\n💡 這是使用 RunnableLambda 的傳統函數式程式設計方法！\")\n",
    "print(\"\\n🔍 與第3個儲存格的差異：\")\n",
    "print(\"   - 第3個儲存格：使用 dict 和 RunnablePassthrough（現代化做法）\")\n",
    "print(\"   - 第4個儲存格：使用 RunnableLambda（傳統函數式做法）\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第5個儲存格：使用說明文件中的 RunnableLambda 方法\n",
    "# 根據 1_rag_basics_說明.md 中的教學實作\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnableLambda\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 定義使用者的問題\n",
    "question = \"如何設定指紋辨識？\"\n",
    "\n",
    "print(f\"問題: {question}\\n\")\n",
    "\n",
    "# 步驟 1: 從向量資料庫檢索相關文件\n",
    "print(\"=\" * 60)\n",
    "print(\"步驟 1: 從向量資料庫檢索相關文件\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}  # 只取最相關的 2 個文件\n",
    ")\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "print(f\"找到 {len(retrieved_docs)} 個相關文件\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"文件 {i} (前100字):\")\n",
    "    print(doc.page_content[:100] + \"...\\n\")\n",
    "\n",
    "# 步驟 2: 建立 RAG Chain（使用說明文件中的 RunnableLambda 方法）\n",
    "print(\"=\" * 60)\n",
    "print(\"步驟 2: 建立 RAG Chain（說明文件中的 RunnableLambda 方法）\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 定義提示模板\n",
    "template = \"\"\"你是一個智慧型手機的客服助手。請根據以下參考資料回答使用者的問題。\n",
    "\n",
    "參考資料：\n",
    "{context}\n",
    "\n",
    "使用者問題：{question}\n",
    "\n",
    "請用繁體中文回答，並且：\n",
    "1. 只根據參考資料回答，不要編造內容\n",
    "2. 如果參考資料中沒有答案，請誠實說「我在資料中找不到相關資訊」\n",
    "3. 回答要清楚、具體、有條理\n",
    "\n",
    "回答：\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 建立 LLM（使用本地 Ollama）\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "# 定義文件格式化函數（說明文件中的方法）\n",
    "def format_docs(docs):\n",
    "    \"\"\"將檢索到的文件格式化為字串\"\"\"\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 定義檢索函數（說明文件中的方法）\n",
    "def retrieve_docs(question):\n",
    "    \"\"\"根據問題檢索相關文件\"\"\"\n",
    "    docs = retriever.invoke(question)\n",
    "    return format_docs(docs)\n",
    "\n",
    "# 定義組合函數（說明文件中的方法）\n",
    "def combine_context_and_question(inputs):\n",
    "    \"\"\"組合檢索到的文件和原始問題\"\"\"\n",
    "    context = inputs[\"context\"]\n",
    "    question = inputs[\"question\"]\n",
    "    return {\"context\": context, \"question\": question}\n",
    "\n",
    "# 建立完整的 RAG Chain（使用說明文件中的 RunnableLambda 方法）\n",
    "rag_chain = (\n",
    "    RunnableLambda(lambda x: {\"context\": retrieve_docs(x), \"question\": x})\n",
    "    | RunnableLambda(combine_context_and_question)\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ RAG Chain 建立完成（說明文件中的 RunnableLambda 方法）\\n\")\n",
    "\n",
    "# 步驟 3: 執行 RAG Chain 並取得答案\n",
    "print(\"=\" * 60)\n",
    "print(\"步驟 3: 執行 RAG Chain\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"\\n【AI 回答】\\n{answer}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RAG Chain 流程總結（說明文件中的 RunnableLambda 做法）\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. RunnableLambda(lambda x: {...}) → 檢索文件並組合輸入\")\n",
    "print(\"2. RunnableLambda(combine_context_and_question) → 格式化輸入\")\n",
    "print(\"3. prompt → 建立提示詞\")\n",
    "print(\"4. llm → 生成回答\")\n",
    "print(\"5. StrOutputParser → 解析輸出\")\n",
    "print(\"\\n💡 這是說明文件中教學的 RunnableLambda 傳統函數式程式設計方法！\")\n",
    "print(\"\\n📚 學習重點（來自說明文件）：\")\n",
    "print(\"   - RunnableLambda 將普通函數包裝成 LangChain 的 Runnable 物件\")\n",
    "print(\"   - 每個步驟都是獨立的函數，容易除錯和測試\")\n",
    "print(\"   - 使用 | 操作符串聯各個處理步驟\")\n",
    "print(\"   - 這是函數式程式設計的傳統方法\")\n",
    "print(\"   - 符合說明文件中『從簡單的範例開始，逐步增加複雜度』的教學理念\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第3個儲存格：整合 Chain - 建立簡單的 RAG 問答鏈\n",
    "# Chain鍊內使用dict,和RunnablePassthrough,retriever\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 定義使用者的問題\n",
    "question = \"如何設定指紋辨識？\"\n",
    "\n",
    "print(f\"問題: {question}\\n\")\n",
    "\n",
    "# 步驟 1: 從向量資料庫檢索相關文件\n",
    "print(\"=\" * 60)\n",
    "print(\"步驟 1: 從向量資料庫檢索相關文件\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "retriever = db.as_retriever(\n",
    "    search_type=\"similarity\",\n",
    "    search_kwargs={\"k\": 2}  # 只取最相關的 2 個文件\n",
    ")\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "print(f\"找到 {len(retrieved_docs)} 個相關文件\\n\")\n",
    "for i, doc in enumerate(retrieved_docs, 1):\n",
    "    print(f\"文件 {i} (前100字):\")\n",
    "    print(doc.page_content[:100] + \"...\\n\")\n",
    "\n",
    "# 步驟 2: 建立 RAG Chain\n",
    "print(\"=\" * 60)\n",
    "print(\"步驟 2: 建立 RAG Chain\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "# 定義提示模板\n",
    "template = \"\"\"你是一個智慧型手機的客服助手。請根據以下參考資料回答使用者的問題。\n",
    "\n",
    "參考資料：\n",
    "{context}\n",
    "\n",
    "使用者問題：{question}\n",
    "\n",
    "請用繁體中文回答，並且：\n",
    "1. 只根據參考資料回答，不要編造內容\n",
    "2. 如果參考資料中沒有答案，請誠實說「我在資料中找不到相關資訊」\n",
    "3. 回答要清楚、具體、有條理\n",
    "\n",
    "回答：\"\"\"\n",
    "\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 建立 LLM（使用本地 Ollama）\n",
    "llm = ChatOllama(model=\"llama3.2\", temperature=0)\n",
    "\n",
    "# 定義文件格式化函數\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "# 建立完整的 RAG Chain\n",
    "# RunnablePassthrough() 讓 question 直接傳遞下去\n",
    "# retriever | format_docs 將檢索結果格式化為 context\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | llm\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "print(\"✅ RAG Chain 建立完成\\n\")\n",
    "\n",
    "# 步驟 3: 執行 RAG Chain 並取得答案\n",
    "print(\"=\" * 60)\n",
    "print(\"步驟 3: 執行 RAG Chain\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "answer = rag_chain.invoke(question)\n",
    "\n",
    "print(f\"\\n【AI 回答】\\n{answer}\\n\")\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"RAG Chain 流程總結\")\n",
    "print(\"=\" * 60)\n",
    "print(\"1. 使用者提問 → 向量檢索找相關文件\")\n",
    "print(\"2. 將文件和問題組合成提示詞\")\n",
    "print(\"3. 送給 LLM 生成答案\")\n",
    "print(\"4. 解析並返回答案\")\n",
    "print(\"\\n💡 這就是從 Chain 3 學到的技巧應用在 RAG 上！\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
