{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分割策略深入探討\n",
    "\n",
    "本範例展示：\n",
    "1. **第1個儲存格**：5種文本分割策略並建立向量資料庫\n",
    "2. **第2個儲存格**：比較不同分割策略的查詢結果\n",
    "\n",
    "學習目標：理解文本分割對檢索效果的影響，選擇最適合的分割策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第1個儲存格：5種文本分割策略並建立向量資料庫\n\nimport os\nfrom langchain.text_splitter import (\n    CharacterTextSplitter,\n    RecursiveCharacterTextSplitter,\n    SentenceTransformersTokenTextSplitter,\n    TextSplitter,\n    TokenTextSplitter,\n)\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# 定義包含文字檔案的目錄\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\nfile_path = os.path.join(current_dir, \"books\", \"信用卡權益說明.txt\")\ndb_dir = os.path.join(current_dir, \"db\")\n\n# 檢查文字檔案是否存在\nif not os.path.exists(file_path):\n    raise FileNotFoundError(\n        f\"檔案 {file_path} 不存在。請檢查路徑。\"\n    )\n\n# 從檔案讀取文字內容\nloader = TextLoader(file_path)\ndocuments = loader.load()\n\n# 定義嵌入模型\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n)\n\n# 建立並持久化向量存儲的函數\ndef create_vector_store(docs, store_name):\n    persistent_directory = os.path.join(db_dir, store_name)\n    if not os.path.exists(persistent_directory):\n        print(f\"\\n--- 正在建立向量存儲 {store_name} ---\")\n        db = Chroma.from_documents(\n            docs, embeddings, persist_directory=persistent_directory\n        )\n        print(f\"--- 完成建立向量存儲 {store_name} ---\")\n    else:\n        print(\n            f\"向量存儲 {store_name} 已存在。無需初始化。\")\n\n# 1. 基於字符的分割\n# 根據指定的字符數將文本分割成塊\n# 適用於無論內容結構如何都需要一致塊大小的情況\nprint(\"\\n--- 使用基於字符的分割 ---\")\nchar_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\nchar_docs = char_splitter.split_documents(documents)\ncreate_vector_store(char_docs, \"chroma_db_char_nb\")\n\n# 2. 基於句子的分割\n# 根據句子將文本分割成塊，確保塊在句子邊界處結束\n# 適用於在塊內保持語義連貫性\nprint(\"\\n--- 使用基於句子的分割 ---\")\nsent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\nsent_docs = sent_splitter.split_documents(documents)\ncreate_vector_store(sent_docs, \"chroma_db_sent_nb\")\n\n# 3. 基於標記的分割\n# 使用標記器（如 GPT-2）根據標記（單詞或子詞）將文本分割成塊\n# 適用於具有嚴格標記限制的轉換器模型\nprint(\"\\n--- 使用基於標記的分割 ---\")\ntoken_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\ntoken_docs = token_splitter.split_documents(documents)\ncreate_vector_store(token_docs, \"chroma_db_token_nb\")\n\n# 4. 遞迴基於字符的分割\n# 嘗試在字符限制內的自然邊界（句子、段落）處分割文本\n# 在保持連貫性和遵守字符限制之間取得平衡\nprint(\"\\n--- 使用遞迴基於字符的分割 ---\")\nrec_char_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=100)\nrec_char_docs = rec_char_splitter.split_documents(documents)\ncreate_vector_store(rec_char_docs, \"chroma_db_rec_char_nb\")\n\n# 5. 自訂分割\n# 允許根據特定需求建立自訂分割邏輯\n# 適用於標準分割器無法處理的具有獨特結構的文件\nprint(\"\\n--- 使用自訂分割 ---\")\n\nclass CustomTextSplitter(TextSplitter):\n    def split_text(self, text):\n        # 自訂分割文本的邏輯\n        return text.split(\"\\n\\n\")  # 範例：按段落分割\n\ncustom_splitter = CustomTextSplitter()\ncustom_docs = custom_splitter.split_documents(documents)\ncreate_vector_store(custom_docs, \"chroma_db_custom_nb\")\n\nprint(\"\\n=== 所有向量存儲已建立完成 ===\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第3個儲存格：整合 Chain 功能 - 比較不同分割策略的 RAG Chain\n\nimport os\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\n\n# 定義路徑\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\ndb_dir = os.path.join(current_dir, \"db\")\n\n# 定義嵌入模型\nembeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n\n# 初始化 LLM\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# 定義 Prompt 模板\nprompt = ChatPromptTemplate.from_template(\"\"\"你是一位專業的客服助理，請根據提供的資料回答使用者問題。\n\n參考資料：\n{context}\n\n問題：{question}\n\n請提供清楚、準確的回答：\"\"\")\n\n# 格式化文件的函數\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# 使用不同分割策略的 RAG Chain 並比較結果\ndef compare_rag_chains(store_name, query, strategy_name):\n    persistent_directory = os.path.join(db_dir, store_name)\n    if os.path.exists(persistent_directory):\n        print(f\"\\n{'='*70}\")\n        print(f\"分割策略：{strategy_name}\")\n        print(f\"{'='*70}\")\n        \n        # 載入向量資料庫\n        db = Chroma(\n            persist_directory=persistent_directory,\n            embedding_function=embeddings\n        )\n        \n        # 建立檢索器\n        retriever = db.as_retriever(\n            search_type=\"similarity\",\n            search_kwargs={\"k\": 2}\n        )\n        \n        # 步驟 1：檢索相關文件\n        print(f\"\\n📋 步驟 1：檢索相關文件\")\n        retrieved_docs = retriever.invoke(query)\n        print(f\"找到 {len(retrieved_docs)} 個相關文件\")\n        for i, doc in enumerate(retrieved_docs, 1):\n            print(f\"\\n文件 {i} 內容預覽：\")\n            print(f\"{doc.page_content[:150]}...\")\n        \n        # 步驟 2：建立 RAG Chain\n        print(f\"\\n⛓️  步驟 2：建立 RAG Chain\")\n        rag_chain = (\n            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n            | prompt\n            | llm\n            | StrOutputParser()\n        )\n        \n        # 步驟 3：執行 Chain 並取得答案\n        print(f\"\\n🤖 步驟 3：生成回答\")\n        answer = rag_chain.invoke(query)\n        print(f\"\\n【{strategy_name} 的回答】\")\n        print(answer)\n        print(f\"\\n{'-'*70}\")\n        \n        return answer\n    else:\n        print(f\"向量存儲 {store_name} 不存在。\")\n        return None\n\n# 定義使用者的問題\nquery = \"信用卡的現金回饋比例是多少？年費如何計算？\"\n\nprint(\"\\n\" + \"=\"*70)\nprint(f\"查詢問題：{query}\")\nprint(\"=\"*70)\n\n# 比較不同分割策略的 RAG Chain 效果\nresults = {}\nresults[\"CharacterTextSplitter\"] = compare_rag_chains(\n    \"chroma_db_char_nb\", query, \"CharacterTextSplitter (字符分割)\"\n)\nresults[\"SentenceTransformersTokenTextSplitter\"] = compare_rag_chains(\n    \"chroma_db_sent_nb\", query, \"SentenceTransformersTokenTextSplitter (句子分割)\"\n)\nresults[\"RecursiveCharacterTextSplitter\"] = compare_rag_chains(\n    \"chroma_db_rec_char_nb\", query, \"RecursiveCharacterTextSplitter (遞迴字符分割)\"\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"📊 分割策略對 RAG Chain 的影響總結\")\nprint(\"=\"*70)\nprint(\"\"\"\n🔍 觀察重點：\n1. 不同分割策略檢索到的文件內容是否完整\n2. 分割邊界是否切斷了重要資訊\n3. 哪種策略的答案最準確、最完整\n\n💡 最佳實踐建議：\n- CharacterTextSplitter：適用於結構簡單、格式統一的文件\n- SentenceTransformers：適合需要保持語義完整性的文件\n- RecursiveCharacterTextSplitter：推薦！在完整性和靈活性之間取得最佳平衡\n\n📌 關鍵學習：\n文本分割策略直接影響 RAG 系統的回答品質。\n選擇合適的分割策略能夠：\n✅ 避免重要資訊被切斷\n✅ 提高檢索的準確性\n✅ 改善最終答案的完整度\n\"\"\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}