{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分割策略深入探討\n",
    "\n",
    "本範例展示：\n",
    "1. **第1個儲存格**：5種文本分割策略並建立向量資料庫\n",
    "2. **第2個儲存格**：比較不同分割策略的查詢結果\n",
    "\n",
    "學習目標：理解文本分割對檢索效果的影響，選擇最適合的分割策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/kb_wcfhj3710_xnyqdgx_v940000gn/T/ipykernel_3931/474284515.py:31: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-zh and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 使用基於字符的分割 ---\n",
      "\n",
      "--- 正在建立向量存儲 chroma_db_char_nb ---\n",
      "--- 完成建立向量存儲 chroma_db_char_nb ---\n",
      "\n",
      "--- 使用基於句子的分割 ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06df91e9c4b476fb1bea4214b154bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d05e8df8484b67b3087c4489679fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de30a086d884660abb591c9f11f4fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97da04636b8a4ff8a1b6ad37b6482364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65941fb5f4f94890996ad500d4158901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09923cb04fe0447f950f3d9045846ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e252ba7bf3b4284aeaa8c33a67a0bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7edb17b01e54e2f92ff26364892e585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff4f8d06363477ba7b8070a8579d0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500ab2e95e0e45c6a31ab0b36f98fd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda0c32ce76d4913927f1ff21ba4a4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- 正在建立向量存儲 chroma_db_sent_nb ---\n",
      "--- 完成建立向量存儲 chroma_db_sent_nb ---\n",
      "\n",
      "--- 使用基於標記的分割 ---\n",
      "\n",
      "--- 正在建立向量存儲 chroma_db_token_nb ---\n",
      "--- 完成建立向量存儲 chroma_db_token_nb ---\n",
      "\n",
      "--- 使用遞迴基於字符的分割 ---\n",
      "\n",
      "--- 正在建立向量存儲 chroma_db_rec_char_nb ---\n",
      "--- 完成建立向量存儲 chroma_db_rec_char_nb ---\n",
      "\n",
      "--- 使用自訂分割 ---\n",
      "\n",
      "--- 正在建立向量存儲 chroma_db_custom_nb ---\n",
      "--- 完成建立向量存儲 chroma_db_custom_nb ---\n",
      "\n",
      "=== 所有向量存儲已建立完成 ===\n"
     ]
    }
   ],
   "source": [
    "# 第1個儲存格：5種文本分割策略並建立向量資料庫\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    "    TextSplitter,\n",
    "    TokenTextSplitter,\n",
    ")\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 定義包含文字檔案的目錄\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "file_path = os.path.join(current_dir, \"books\", \"信用卡權益說明.txt\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# 檢查文字檔案是否存在\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"檔案 {file_path} 不存在。請檢查路徑。\"\n",
    "    )\n",
    "\n",
    "# 從檔案讀取文字內容\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# 定義嵌入模型\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n",
    ")\n",
    "\n",
    "# 建立並持久化向量存儲的函數\n",
    "def create_vector_store(docs, store_name):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(f\"\\n--- 正在建立向量存儲 {store_name} ---\")\n",
    "        db = Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory\n",
    "        )\n",
    "        print(f\"--- 完成建立向量存儲 {store_name} ---\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"向量存儲 {store_name} 已存在。無需初始化。\")\n",
    "\n",
    "# 1. 基於字符的分割\n",
    "# 根據指定的字符數將文本分割成塊\n",
    "# 適用於無論內容結構如何都需要一致塊大小的情況\n",
    "print(\"\\n--- 使用基於字符的分割 ---\")\n",
    "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "char_docs = char_splitter.split_documents(documents)\n",
    "create_vector_store(char_docs, \"chroma_db_char_nb\")\n",
    "\n",
    "# 2. 基於句子的分割\n",
    "# 根據句子將文本分割成塊，確保塊在句子邊界處結束\n",
    "# 適用於在塊內保持語義連貫性\n",
    "print(\"\\n--- 使用基於句子的分割 ---\")\n",
    "sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
    "sent_docs = sent_splitter.split_documents(documents)\n",
    "create_vector_store(sent_docs, \"chroma_db_sent_nb\")\n",
    "\n",
    "# 3. 基於標記的分割\n",
    "# 使用標記器（如 GPT-2）根據標記（單詞或子詞）將文本分割成塊\n",
    "# 適用於具有嚴格標記限制的轉換器模型\n",
    "print(\"\\n--- 使用基於標記的分割 ---\")\n",
    "token_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\n",
    "token_docs = token_splitter.split_documents(documents)\n",
    "create_vector_store(token_docs, \"chroma_db_token_nb\")\n",
    "\n",
    "# 4. 遞迴基於字符的分割\n",
    "# 嘗試在字符限制內的自然邊界（句子、段落）處分割文本\n",
    "# 在保持連貫性和遵守字符限制之間取得平衡\n",
    "print(\"\\n--- 使用遞迴基於字符的分割 ---\")\n",
    "rec_char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100)\n",
    "rec_char_docs = rec_char_splitter.split_documents(documents)\n",
    "create_vector_store(rec_char_docs, \"chroma_db_rec_char_nb\")\n",
    "\n",
    "# 5. 自訂分割\n",
    "# 允許根據特定需求建立自訂分割邏輯\n",
    "# 適用於標準分割器無法處理的具有獨特結構的文件\n",
    "print(\"\\n--- 使用自訂分割 ---\")\n",
    "\n",
    "class CustomTextSplitter(TextSplitter):\n",
    "    def split_text(self, text):\n",
    "        # 自訂分割文本的邏輯\n",
    "        return text.split(\"\\n\\n\")  # 範例：按段落分割\n",
    "\n",
    "custom_splitter = CustomTextSplitter()\n",
    "custom_docs = custom_splitter.split_documents(documents)\n",
    "create_vector_store(custom_docs, \"chroma_db_custom_nb\")\n",
    "\n",
    "print(\"\\n=== 所有向量存儲已建立完成 ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-zh and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "查詢問題：信用卡的現金回饋比例是多少？\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "分割策略：CharacterTextSplitter (字符分割)\n",
      "======================================================================\n",
      "\n",
      "找到 2 個相關文件\n",
      "\n",
      "--- 文件 1 ---\n",
      "內容長度: 962 字元\n",
      "內容預覽：\n",
      "點數保護：\n",
      "- 卡片遺失補發，點數保留\n",
      "- 盜刷消費的點數會扣回\n",
      "- 退貨消費的點數會扣回\n",
      "\n",
      "點數使用策略：\n",
      "建議 1：集中消費\n",
      "- 主力使用一張卡累積點數\n",
      "- 避免點數分散\n",
      "\n",
      "建議 2：留意到期日\n",
      "- 優先使用即將到期點數\n",
      "- 設定到期提醒\n",
      "\n",
      "建議 3：高價值兌換\n",
      "- 兌換划算商品（CP值高）\n",
      "- 大型家電通常較划算\n",
      "- 避免兌換低價值商品\n",
      "\n",
      "建議 4：合併點數\n",
      "- 將附卡點數轉入正卡\n",
      "- 達到...\n",
      "\n",
      "--- 文件 2 ---\n",
      "內容長度: 939 字元\n",
      "內容預覽：\n",
      "7.3 消費爭議處理\n",
      "----------------------------------------\n",
      "常見消費爭議：\n",
      "\n",
      "爭議類型 1：未收到商品\n",
      "處理方式：\n",
      "1. 先聯絡商家確認出貨\n",
      "2. 查詢物流進度\n",
      "3. 若商家無法處理，向銀行申訴\n",
      "4. 提供訂單紀錄、對話紀錄\n",
      "5. 銀行協助向商家求償\n",
      "\n",
      "爭議類型 2：商品瑕疵\n",
      "處理方式：\n",
      "1. 依消保法 7天鑑賞期退貨\n",
      "2. 商家拒絕退貨再向銀行申訴\n",
      "3...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "分割策略：SentenceTransformersTokenTextSplitter (句子分割)\n",
      "======================================================================\n",
      "\n",
      "找到 2 個相關文件\n",
      "\n",
      "--- 文件 1 ---\n",
      "內容長度: 1550 字元\n",
      "內容預覽：\n",
      "人 士 ： [UNK] [UNK] + [UNK] [UNK] [UNK] [UNK] 力 [UNK] 明 （ [UNK] 一 ） ： - [UNK] [UNK] [UNK] [UNK] [UNK] 明 （ [UNK] 3 [UNK] 月 ） - [UNK] [UNK] [UNK] [UNK] - [UNK] [UNK] 清 [UNK] - 不 [UNK] [UNK] [UNK] 明 - 定 [U...\n",
      "\n",
      "--- 文件 2 ---\n",
      "內容長度: 1472 字元\n",
      "內容預覽：\n",
      "[UNK] - budget ： [UNK] 場 [UNK] [UNK] [UNK] [UNK] = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = [UNK] [UNK] 章 ： [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] = = = = = = =...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "分割策略：RecursiveCharacterTextSplitter (遞迴字符分割)\n",
      "======================================================================\n",
      "\n",
      "找到 2 個相關文件\n",
      "\n",
      "--- 文件 1 ---\n",
      "內容長度: 962 字元\n",
      "內容預覽：\n",
      "點數保護：\n",
      "- 卡片遺失補發，點數保留\n",
      "- 盜刷消費的點數會扣回\n",
      "- 退貨消費的點數會扣回\n",
      "\n",
      "點數使用策略：\n",
      "建議 1：集中消費\n",
      "- 主力使用一張卡累積點數\n",
      "- 避免點數分散\n",
      "\n",
      "建議 2：留意到期日\n",
      "- 優先使用即將到期點數\n",
      "- 設定到期提醒\n",
      "\n",
      "建議 3：高價值兌換\n",
      "- 兌換划算商品（CP值高）\n",
      "- 大型家電通常較划算\n",
      "- 避免兌換低價值商品\n",
      "\n",
      "建議 4：合併點數\n",
      "- 將附卡點數轉入正卡\n",
      "- 達到...\n",
      "\n",
      "--- 文件 2 ---\n",
      "內容長度: 939 字元\n",
      "內容預覽：\n",
      "7.3 消費爭議處理\n",
      "----------------------------------------\n",
      "常見消費爭議：\n",
      "\n",
      "爭議類型 1：未收到商品\n",
      "處理方式：\n",
      "1. 先聯絡商家確認出貨\n",
      "2. 查詢物流進度\n",
      "3. 若商家無法處理，向銀行申訴\n",
      "4. 提供訂單紀錄、對話紀錄\n",
      "5. 銀行協助向商家求償\n",
      "\n",
      "爭議類型 2：商品瑕疵\n",
      "處理方式：\n",
      "1. 依消保法 7天鑑賞期退貨\n",
      "2. 商家拒絕退貨再向銀行申訴\n",
      "3...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "📊 觀察重點\n",
      "======================================================================\n",
      "\n",
      "🔍 比較不同分割策略檢索到的文件：\n",
      "1. 文件內容是否完整？\n",
      "2. 分割邊界是否合理？\n",
      "3. 哪種策略最能保留語義完整性？\n",
      "\n",
      "💡 提示：\n",
      "- CharacterTextSplitter：固定字元數分割，可能切斷句子\n",
      "- SentenceTransformers：按句子邊界分割，保持語義完整\n",
      "- RecursiveCharacterTextSplitter：智慧分割，平衡完整性和大小\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/kb_wcfhj3710_xnyqdgx_v940000gn/T/ipykernel_3931/3411143644.py:30: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# 第2個儲存格：比較不同分割策略的查詢結果\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 定義路徑\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# 定義嵌入模型\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n",
    "\n",
    "# 定義使用者的問題\n",
    "query = \"信用卡的現金回饋比例是多少？\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"查詢問題：{query}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 比較不同分割策略的檢索結果\n",
    "def compare_splitting_strategies(store_name, strategy_name):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if os.path.exists(persistent_directory):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"分割策略：{strategy_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # 載入向量資料庫\n",
    "        db = Chroma(\n",
    "            persist_directory=persistent_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        # 建立檢索器\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 2}\n",
    "        )\n",
    "        \n",
    "        # 檢索相關文件\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        print(f\"\\n找到 {len(retrieved_docs)} 個相關文件\")\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            print(f\"\\n--- 文件 {i} ---\")\n",
    "            print(f\"內容長度: {len(doc.page_content)} 字元\")\n",
    "            print(f\"內容預覽：\")\n",
    "            print(f\"{doc.page_content[:200]}...\")\n",
    "        \n",
    "        print(f\"\\n{'-'*70}\")\n",
    "    else:\n",
    "        print(f\"向量存儲 {store_name} 不存在。\")\n",
    "\n",
    "# 比較不同分割策略\n",
    "compare_splitting_strategies(\"chroma_db_char_nb\", \"CharacterTextSplitter (字符分割)\")\n",
    "compare_splitting_strategies(\"chroma_db_sent_nb\", \"SentenceTransformersTokenTextSplitter (句子分割)\")\n",
    "compare_splitting_strategies(\"chroma_db_rec_char_nb\", \"RecursiveCharacterTextSplitter (遞迴字符分割)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 觀察重點\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "🔍 比較不同分割策略檢索到的文件：\n",
    "1. 文件內容是否完整？\n",
    "2. 分割邊界是否合理？\n",
    "3. 哪種策略最能保留語義完整性？\n",
    "\n",
    "💡 提示：\n",
    "- CharacterTextSplitter：固定字元數分割，可能切斷句子\n",
    "- SentenceTransformers：按句子邊界分割，保持語義完整\n",
    "- RecursiveCharacterTextSplitter：智慧分割，平衡完整性和大小\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第3個儲存格：整合 Chain 功能 - 比較不同分割策略的 RAG Chain\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# 定義路徑\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# 定義嵌入模型\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n",
    "\n",
    "# 初始化 LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# 定義 Prompt 模板\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"你是一位專業的客服助理，請根據提供的資料回答使用者問題。\n",
    "\n",
    "參考資料：\n",
    "{context}\n",
    "\n",
    "問題：{question}\n",
    "\n",
    "請提供清楚、準確的回答：\"\"\")\n",
    "\n",
    "# 格式化文件的函數\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# 使用不同分割策略的 RAG Chain 並比較結果\n",
    "def compare_rag_chains(store_name, query, strategy_name):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if os.path.exists(persistent_directory):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"分割策略：{strategy_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # 載入向量資料庫\n",
    "        db = Chroma(\n",
    "            persist_directory=persistent_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        # 建立檢索器\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 2}\n",
    "        )\n",
    "        \n",
    "        # 步驟 1：檢索相關文件\n",
    "        print(f\"\\n📋 步驟 1：檢索相關文件\")\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        print(f\"找到 {len(retrieved_docs)} 個相關文件\")\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            print(f\"\\n文件 {i} 內容預覽：\")\n",
    "            print(f\"{doc.page_content[:150]}...\")\n",
    "        \n",
    "        # 步驟 2：建立 RAG Chain\n",
    "        print(f\"\\n⛓️  步驟 2：建立 RAG Chain\")\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        # 步驟 3：執行 Chain 並取得答案\n",
    "        print(f\"\\n🤖 步驟 3：生成回答\")\n",
    "        answer = rag_chain.invoke(query)\n",
    "        print(f\"\\n【{strategy_name} 的回答】\")\n",
    "        print(answer)\n",
    "        print(f\"\\n{'-'*70}\")\n",
    "        \n",
    "        return answer\n",
    "    else:\n",
    "        print(f\"向量存儲 {store_name} 不存在。\")\n",
    "        return None\n",
    "\n",
    "# 定義使用者的問題\n",
    "query = \"信用卡的現金回饋比例是多少？年費如何計算？\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"查詢問題：{query}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# 比較不同分割策略的 RAG Chain 效果\n",
    "results = {}\n",
    "results[\"CharacterTextSplitter\"] = compare_rag_chains(\n",
    "    \"chroma_db_char_nb\", query, \"CharacterTextSplitter (字符分割)\"\n",
    ")\n",
    "results[\"SentenceTransformersTokenTextSplitter\"] = compare_rag_chains(\n",
    "    \"chroma_db_sent_nb\", query, \"SentenceTransformersTokenTextSplitter (句子分割)\"\n",
    ")\n",
    "results[\"RecursiveCharacterTextSplitter\"] = compare_rag_chains(\n",
    "    \"chroma_db_rec_char_nb\", query, \"RecursiveCharacterTextSplitter (遞迴字符分割)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"📊 分割策略對 RAG Chain 的影響總結\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "🔍 觀察重點：\n",
    "1. 不同分割策略檢索到的文件內容是否完整\n",
    "2. 分割邊界是否切斷了重要資訊\n",
    "3. 哪種策略的答案最準確、最完整\n",
    "\n",
    "💡 最佳實踐建議：\n",
    "- CharacterTextSplitter：適用於結構簡單、格式統一的文件\n",
    "- SentenceTransformers：適合需要保持語義完整性的文件\n",
    "- RecursiveCharacterTextSplitter：推薦！在完整性和靈活性之間取得最佳平衡\n",
    "\n",
    "📌 關鍵學習：\n",
    "文本分割策略直接影響 RAG 系統的回答品質。\n",
    "選擇合適的分割策略能夠：\n",
    "✅ 避免重要資訊被切斷\n",
    "✅ 提高檢索的準確性\n",
    "✅ 改善最終答案的完整度\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
