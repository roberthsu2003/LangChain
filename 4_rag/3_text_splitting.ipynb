{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬åˆ†å‰²ç­–ç•¥æ·±å…¥æ¢è¨\n",
    "\n",
    "æœ¬ç¯„ä¾‹å±•ç¤ºï¼š\n",
    "1. **ç¬¬1å€‹å„²å­˜æ ¼**ï¼š5ç¨®æ–‡æœ¬åˆ†å‰²ç­–ç•¥ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«\n",
    "2. **ç¬¬2å€‹å„²å­˜æ ¼**ï¼šæ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥çš„æŸ¥è©¢çµæœ\n",
    "\n",
    "å­¸ç¿’ç›®æ¨™ï¼šç†è§£æ–‡æœ¬åˆ†å‰²å°æª¢ç´¢æ•ˆæœçš„å½±éŸ¿ï¼Œé¸æ“‡æœ€é©åˆçš„åˆ†å‰²ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç¬¬1å€‹å„²å­˜æ ¼ï¼š5ç¨®æ–‡æœ¬åˆ†å‰²ç­–ç•¥ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«\n\nimport os\nfrom langchain.text_splitter import (\n    CharacterTextSplitter,\n    RecursiveCharacterTextSplitter,\n    SentenceTransformersTokenTextSplitter,\n    TextSplitter,\n    TokenTextSplitter,\n)\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# å®šç¾©åŒ…å«æ–‡å­—æª”æ¡ˆçš„ç›®éŒ„\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\nfile_path = os.path.join(current_dir, \"books\", \"ä¿¡ç”¨å¡æ¬Šç›Šèªªæ˜.txt\")\ndb_dir = os.path.join(current_dir, \"db\")\n\n# æª¢æŸ¥æ–‡å­—æª”æ¡ˆæ˜¯å¦å­˜åœ¨\nif not os.path.exists(file_path):\n    raise FileNotFoundError(\n        f\"æª”æ¡ˆ {file_path} ä¸å­˜åœ¨ã€‚è«‹æª¢æŸ¥è·¯å¾‘ã€‚\"\n    )\n\n# å¾æª”æ¡ˆè®€å–æ–‡å­—å…§å®¹\nloader = TextLoader(file_path)\ndocuments = loader.load()\n\n# å®šç¾©åµŒå…¥æ¨¡å‹\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n)\n\n# å»ºç«‹ä¸¦æŒä¹…åŒ–å‘é‡å­˜å„²çš„å‡½æ•¸\ndef create_vector_store(docs, store_name):\n    persistent_directory = os.path.join(db_dir, store_name)\n    if not os.path.exists(persistent_directory):\n        print(f\"\\n--- æ­£åœ¨å»ºç«‹å‘é‡å­˜å„² {store_name} ---\")\n        db = Chroma.from_documents(\n            docs, embeddings, persist_directory=persistent_directory\n        )\n        print(f\"--- å®Œæˆå»ºç«‹å‘é‡å­˜å„² {store_name} ---\")\n    else:\n        print(\n            f\"å‘é‡å­˜å„² {store_name} å·²å­˜åœ¨ã€‚ç„¡éœ€åˆå§‹åŒ–ã€‚\")\n\n# 1. åŸºæ–¼å­—ç¬¦çš„åˆ†å‰²\n# æ ¹æ“šæŒ‡å®šçš„å­—ç¬¦æ•¸å°‡æ–‡æœ¬åˆ†å‰²æˆå¡Š\n# é©ç”¨æ–¼ç„¡è«–å…§å®¹çµæ§‹å¦‚ä½•éƒ½éœ€è¦ä¸€è‡´å¡Šå¤§å°çš„æƒ…æ³\nprint(\"\\n--- ä½¿ç”¨åŸºæ–¼å­—ç¬¦çš„åˆ†å‰² ---\")\nchar_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\nchar_docs = char_splitter.split_documents(documents)\ncreate_vector_store(char_docs, \"chroma_db_char_nb\")\n\n# 2. åŸºæ–¼å¥å­çš„åˆ†å‰²\n# æ ¹æ“šå¥å­å°‡æ–‡æœ¬åˆ†å‰²æˆå¡Šï¼Œç¢ºä¿å¡Šåœ¨å¥å­é‚Šç•Œè™•çµæŸ\n# é©ç”¨æ–¼åœ¨å¡Šå…§ä¿æŒèªç¾©é€£è²«æ€§\nprint(\"\\n--- ä½¿ç”¨åŸºæ–¼å¥å­çš„åˆ†å‰² ---\")\nsent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\nsent_docs = sent_splitter.split_documents(documents)\ncreate_vector_store(sent_docs, \"chroma_db_sent_nb\")\n\n# 3. åŸºæ–¼æ¨™è¨˜çš„åˆ†å‰²\n# ä½¿ç”¨æ¨™è¨˜å™¨ï¼ˆå¦‚ GPT-2ï¼‰æ ¹æ“šæ¨™è¨˜ï¼ˆå–®è©æˆ–å­è©ï¼‰å°‡æ–‡æœ¬åˆ†å‰²æˆå¡Š\n# é©ç”¨æ–¼å…·æœ‰åš´æ ¼æ¨™è¨˜é™åˆ¶çš„è½‰æ›å™¨æ¨¡å‹\nprint(\"\\n--- ä½¿ç”¨åŸºæ–¼æ¨™è¨˜çš„åˆ†å‰² ---\")\ntoken_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\ntoken_docs = token_splitter.split_documents(documents)\ncreate_vector_store(token_docs, \"chroma_db_token_nb\")\n\n# 4. éè¿´åŸºæ–¼å­—ç¬¦çš„åˆ†å‰²\n# å˜—è©¦åœ¨å­—ç¬¦é™åˆ¶å…§çš„è‡ªç„¶é‚Šç•Œï¼ˆå¥å­ã€æ®µè½ï¼‰è™•åˆ†å‰²æ–‡æœ¬\n# åœ¨ä¿æŒé€£è²«æ€§å’Œéµå®ˆå­—ç¬¦é™åˆ¶ä¹‹é–“å–å¾—å¹³è¡¡\nprint(\"\\n--- ä½¿ç”¨éè¿´åŸºæ–¼å­—ç¬¦çš„åˆ†å‰² ---\")\nrec_char_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=100)\nrec_char_docs = rec_char_splitter.split_documents(documents)\ncreate_vector_store(rec_char_docs, \"chroma_db_rec_char_nb\")\n\n# 5. è‡ªè¨‚åˆ†å‰²\n# å…è¨±æ ¹æ“šç‰¹å®šéœ€æ±‚å»ºç«‹è‡ªè¨‚åˆ†å‰²é‚è¼¯\n# é©ç”¨æ–¼æ¨™æº–åˆ†å‰²å™¨ç„¡æ³•è™•ç†çš„å…·æœ‰ç¨ç‰¹çµæ§‹çš„æ–‡ä»¶\nprint(\"\\n--- ä½¿ç”¨è‡ªè¨‚åˆ†å‰² ---\")\n\nclass CustomTextSplitter(TextSplitter):\n    def split_text(self, text):\n        # è‡ªè¨‚åˆ†å‰²æ–‡æœ¬çš„é‚è¼¯\n        return text.split(\"\\n\\n\")  # ç¯„ä¾‹ï¼šæŒ‰æ®µè½åˆ†å‰²\n\ncustom_splitter = CustomTextSplitter()\ncustom_docs = custom_splitter.split_documents(documents)\ncreate_vector_store(custom_docs, \"chroma_db_custom_nb\")\n\nprint(\"\\n=== æ‰€æœ‰å‘é‡å­˜å„²å·²å»ºç«‹å®Œæˆ ===\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ç¬¬3å€‹å„²å­˜æ ¼ï¼šæ•´åˆ Chain åŠŸèƒ½ - æ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥çš„ RAG Chain\n\nimport os\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_openai import ChatOpenAI\n\n# å®šç¾©è·¯å¾‘\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\ndb_dir = os.path.join(current_dir, \"db\")\n\n# å®šç¾©åµŒå…¥æ¨¡å‹\nembeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n\n# åˆå§‹åŒ– LLM\nllm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n\n# å®šç¾© Prompt æ¨¡æ¿\nprompt = ChatPromptTemplate.from_template(\"\"\"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å®¢æœåŠ©ç†ï¼Œè«‹æ ¹æ“šæä¾›çš„è³‡æ–™å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚\n\nåƒè€ƒè³‡æ–™ï¼š\n{context}\n\nå•é¡Œï¼š{question}\n\nè«‹æä¾›æ¸…æ¥šã€æº–ç¢ºçš„å›ç­”ï¼š\"\"\")\n\n# æ ¼å¼åŒ–æ–‡ä»¶çš„å‡½æ•¸\ndef format_docs(docs):\n    return \"\\n\\n\".join(doc.page_content for doc in docs)\n\n# ä½¿ç”¨ä¸åŒåˆ†å‰²ç­–ç•¥çš„ RAG Chain ä¸¦æ¯”è¼ƒçµæœ\ndef compare_rag_chains(store_name, query, strategy_name):\n    persistent_directory = os.path.join(db_dir, store_name)\n    if os.path.exists(persistent_directory):\n        print(f\"\\n{'='*70}\")\n        print(f\"åˆ†å‰²ç­–ç•¥ï¼š{strategy_name}\")\n        print(f\"{'='*70}\")\n        \n        # è¼‰å…¥å‘é‡è³‡æ–™åº«\n        db = Chroma(\n            persist_directory=persistent_directory,\n            embedding_function=embeddings\n        )\n        \n        # å»ºç«‹æª¢ç´¢å™¨\n        retriever = db.as_retriever(\n            search_type=\"similarity\",\n            search_kwargs={\"k\": 2}\n        )\n        \n        # æ­¥é©Ÿ 1ï¼šæª¢ç´¢ç›¸é—œæ–‡ä»¶\n        print(f\"\\nğŸ“‹ æ­¥é©Ÿ 1ï¼šæª¢ç´¢ç›¸é—œæ–‡ä»¶\")\n        retrieved_docs = retriever.invoke(query)\n        print(f\"æ‰¾åˆ° {len(retrieved_docs)} å€‹ç›¸é—œæ–‡ä»¶\")\n        for i, doc in enumerate(retrieved_docs, 1):\n            print(f\"\\næ–‡ä»¶ {i} å…§å®¹é è¦½ï¼š\")\n            print(f\"{doc.page_content[:150]}...\")\n        \n        # æ­¥é©Ÿ 2ï¼šå»ºç«‹ RAG Chain\n        print(f\"\\nâ›“ï¸  æ­¥é©Ÿ 2ï¼šå»ºç«‹ RAG Chain\")\n        rag_chain = (\n            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n            | prompt\n            | llm\n            | StrOutputParser()\n        )\n        \n        # æ­¥é©Ÿ 3ï¼šåŸ·è¡Œ Chain ä¸¦å–å¾—ç­”æ¡ˆ\n        print(f\"\\nğŸ¤– æ­¥é©Ÿ 3ï¼šç”Ÿæˆå›ç­”\")\n        answer = rag_chain.invoke(query)\n        print(f\"\\nã€{strategy_name} çš„å›ç­”ã€‘\")\n        print(answer)\n        print(f\"\\n{'-'*70}\")\n        \n        return answer\n    else:\n        print(f\"å‘é‡å­˜å„² {store_name} ä¸å­˜åœ¨ã€‚\")\n        return None\n\n# å®šç¾©ä½¿ç”¨è€…çš„å•é¡Œ\nquery = \"ä¿¡ç”¨å¡çš„ç¾é‡‘å›é¥‹æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿå¹´è²»å¦‚ä½•è¨ˆç®—ï¼Ÿ\"\n\nprint(\"\\n\" + \"=\"*70)\nprint(f\"æŸ¥è©¢å•é¡Œï¼š{query}\")\nprint(\"=\"*70)\n\n# æ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥çš„ RAG Chain æ•ˆæœ\nresults = {}\nresults[\"CharacterTextSplitter\"] = compare_rag_chains(\n    \"chroma_db_char_nb\", query, \"CharacterTextSplitter (å­—ç¬¦åˆ†å‰²)\"\n)\nresults[\"SentenceTransformersTokenTextSplitter\"] = compare_rag_chains(\n    \"chroma_db_sent_nb\", query, \"SentenceTransformersTokenTextSplitter (å¥å­åˆ†å‰²)\"\n)\nresults[\"RecursiveCharacterTextSplitter\"] = compare_rag_chains(\n    \"chroma_db_rec_char_nb\", query, \"RecursiveCharacterTextSplitter (éè¿´å­—ç¬¦åˆ†å‰²)\"\n)\n\nprint(\"\\n\" + \"=\"*70)\nprint(\"ğŸ“Š åˆ†å‰²ç­–ç•¥å° RAG Chain çš„å½±éŸ¿ç¸½çµ\")\nprint(\"=\"*70)\nprint(\"\"\"\nğŸ” è§€å¯Ÿé‡é»ï¼š\n1. ä¸åŒåˆ†å‰²ç­–ç•¥æª¢ç´¢åˆ°çš„æ–‡ä»¶å…§å®¹æ˜¯å¦å®Œæ•´\n2. åˆ†å‰²é‚Šç•Œæ˜¯å¦åˆ‡æ–·äº†é‡è¦è³‡è¨Š\n3. å“ªç¨®ç­–ç•¥çš„ç­”æ¡ˆæœ€æº–ç¢ºã€æœ€å®Œæ•´\n\nğŸ’¡ æœ€ä½³å¯¦è¸å»ºè­°ï¼š\n- CharacterTextSplitterï¼šé©ç”¨æ–¼çµæ§‹ç°¡å–®ã€æ ¼å¼çµ±ä¸€çš„æ–‡ä»¶\n- SentenceTransformersï¼šé©åˆéœ€è¦ä¿æŒèªç¾©å®Œæ•´æ€§çš„æ–‡ä»¶\n- RecursiveCharacterTextSplitterï¼šæ¨è–¦ï¼åœ¨å®Œæ•´æ€§å’Œéˆæ´»æ€§ä¹‹é–“å–å¾—æœ€ä½³å¹³è¡¡\n\nğŸ“Œ é—œéµå­¸ç¿’ï¼š\næ–‡æœ¬åˆ†å‰²ç­–ç•¥ç›´æ¥å½±éŸ¿ RAG ç³»çµ±çš„å›ç­”å“è³ªã€‚\né¸æ“‡åˆé©çš„åˆ†å‰²ç­–ç•¥èƒ½å¤ ï¼š\nâœ… é¿å…é‡è¦è³‡è¨Šè¢«åˆ‡æ–·\nâœ… æé«˜æª¢ç´¢çš„æº–ç¢ºæ€§\nâœ… æ”¹å–„æœ€çµ‚ç­”æ¡ˆçš„å®Œæ•´åº¦\n\"\"\")\nprint(\"=\"*70)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}