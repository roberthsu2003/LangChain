{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 文本分割策略深入探討\n",
    "\n",
    "本範例展示：\n",
    "1. **第1個儲存格**：5種文本分割策略並建立向量資料庫\n",
    "2. **第2個儲存格**：比較不同分割策略的查詢結果\n",
    "\n",
    "學習目標：理解文本分割對檢索效果的影響，選擇最適合的分割策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第1個儲存格：5種文本分割策略並建立向量資料庫\n\nimport os\nfrom langchain.text_splitter import (\n    CharacterTextSplitter,\n    RecursiveCharacterTextSplitter,\n    SentenceTransformersTokenTextSplitter,\n    TextSplitter,\n    TokenTextSplitter,\n)\nfrom langchain_community.document_loaders import TextLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# 定義包含文字檔案的目錄\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\nfile_path = os.path.join(current_dir, \"books\", \"紅樓夢.txt\")\ndb_dir = os.path.join(current_dir, \"db\")\n\n# 檢查文字檔案是否存在\nif not os.path.exists(file_path):\n    raise FileNotFoundError(\n        f\"檔案 {file_path} 不存在。請檢查路徑。\"\n    )\n\n# 從檔案讀取文字內容\nloader = TextLoader(file_path)\ndocuments = loader.load()\n\n# 定義嵌入模型\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n)\n\n# 建立並持久化向量存儲的函數\ndef create_vector_store(docs, store_name):\n    persistent_directory = os.path.join(db_dir, store_name)\n    if not os.path.exists(persistent_directory):\n        print(f\"\\n--- 正在建立向量存儲 {store_name} ---\")\n        db = Chroma.from_documents(\n            docs, embeddings, persist_directory=persistent_directory\n        )\n        print(f\"--- 完成建立向量存儲 {store_name} ---\")\n    else:\n        print(\n            f\"向量存儲 {store_name} 已存在。無需初始化。\")\n\n# 1. 基於字符的分割\n# 根據指定的字符數將文本分割成塊\n# 適用於無論內容結構如何都需要一致塊大小的情況\nprint(\"\\n--- 使用基於字符的分割 ---\")\nchar_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\nchar_docs = char_splitter.split_documents(documents)\ncreate_vector_store(char_docs, \"chroma_db_char_nb\")\n\n# 2. 基於句子的分割\n# 根據句子將文本分割成塊，確保塊在句子邊界處結束\n# 適用於在塊內保持語義連貫性\nprint(\"\\n--- 使用基於句子的分割 ---\")\nsent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\nsent_docs = sent_splitter.split_documents(documents)\ncreate_vector_store(sent_docs, \"chroma_db_sent_nb\")\n\n# 3. 基於標記的分割\n# 使用標記器（如 GPT-2）根據標記（單詞或子詞）將文本分割成塊\n# 適用於具有嚴格標記限制的轉換器模型\nprint(\"\\n--- 使用基於標記的分割 ---\")\ntoken_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\ntoken_docs = token_splitter.split_documents(documents)\ncreate_vector_store(token_docs, \"chroma_db_token_nb\")\n\n# 4. 遞迴基於字符的分割\n# 嘗試在字符限制內的自然邊界（句子、段落）處分割文本\n# 在保持連貫性和遵守字符限制之間取得平衡\nprint(\"\\n--- 使用遞迴基於字符的分割 ---\")\nrec_char_splitter = RecursiveCharacterTextSplitter(\n    chunk_size=1000, chunk_overlap=100)\nrec_char_docs = rec_char_splitter.split_documents(documents)\ncreate_vector_store(rec_char_docs, \"chroma_db_rec_char_nb\")\n\n# 5. 自訂分割\n# 允許根據特定需求建立自訂分割邏輯\n# 適用於標準分割器無法處理的具有獨特結構的文件\nprint(\"\\n--- 使用自訂分割 ---\")\n\nclass CustomTextSplitter(TextSplitter):\n    def split_text(self, text):\n        # 自訂分割文本的邏輯\n        return text.split(\"\\n\\n\")  # 範例：按段落分割\n\ncustom_splitter = CustomTextSplitter()\ncustom_docs = custom_splitter.split_documents(documents)\ncreate_vector_store(custom_docs, \"chroma_db_custom_nb\")\n\nprint(\"\\n=== 所有向量存儲已建立完成 ===\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第2個儲存格：比較不同分割策略的查詢結果\n\nimport os\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# 定義路徑\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\ndb_dir = os.path.join(current_dir, \"db\")\n\n# 定義嵌入模型\nembeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n\n# 查詢向量存儲的函數\ndef query_vector_store(store_name, query):\n    persistent_directory = os.path.join(db_dir, store_name)\n    if os.path.exists(persistent_directory):\n        print(f\"\\n--- 正在查詢向量存儲 {store_name} ---\")\n        db = Chroma(\n            persist_directory=persistent_directory, embedding_function=embeddings\n        )\n        retriever = db.as_retriever(\n            search_type=\"similarity_score_threshold\",\n            search_kwargs={\"k\": 1, \"score_threshold\": 0.1},\n        )\n        relevant_docs = retriever.invoke(query)\n        # 顯示相關結果及元數據\n        print(f\"\\n--- {store_name} 的相關文件 ---\")\n        for i, doc in enumerate(relevant_docs, 1):\n            print(f\"文件 {i}:\\n{doc.page_content}\\n\")\n            if doc.metadata:\n                print(f\"來源: {doc.metadata.get('source', 'Unknown')}\\n\")\n    else:\n        print(f\"向量存儲 {store_name} 不存在。\")\n\n# 定義使用者的問題\nquery = \"賈寶玉和林黛玉是什麼關係?\"\n\n# 查詢每個向量存儲並比較結果\nprint(\"\\n\" + \"=\"*60)\nprint(f\"查詢問題：{query}\")\nprint(\"=\"*60)\n\nquery_vector_store(\"chroma_db_char_nb\", query)\nquery_vector_store(\"chroma_db_sent_nb\", query)\nquery_vector_store(\"chroma_db_token_nb\", query)\nquery_vector_store(\"chroma_db_rec_char_nb\", query)\nquery_vector_store(\"chroma_db_custom_nb\", query)\n\nprint(\"\\n\" + \"=\"*60)\nprint(\"結論：不同的分割策略會影響檢索結果的品質和相關性\")\nprint(\"建議：根據文本特性和應用需求選擇合適的分割策略\")\nprint(\"=\"*60)"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}