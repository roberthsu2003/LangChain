{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# æ–‡æœ¬åˆ†å‰²ç­–ç•¥æ·±å…¥æ¢è¨\n",
    "\n",
    "æœ¬ç¯„ä¾‹å±•ç¤ºï¼š\n",
    "1. **ç¬¬1å€‹å„²å­˜æ ¼**ï¼š5ç¨®æ–‡æœ¬åˆ†å‰²ç­–ç•¥ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«\n",
    "2. **ç¬¬2å€‹å„²å­˜æ ¼**ï¼šæ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥çš„æŸ¥è©¢çµæœ\n",
    "\n",
    "å­¸ç¿’ç›®æ¨™ï¼šç†è§£æ–‡æœ¬åˆ†å‰²å°æª¢ç´¢æ•ˆæœçš„å½±éŸ¿ï¼Œé¸æ“‡æœ€é©åˆçš„åˆ†å‰²ç­–ç•¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/kb_wcfhj3710_xnyqdgx_v940000gn/T/ipykernel_3931/474284515.py:31: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = HuggingFaceEmbeddings(\n",
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-zh and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- ä½¿ç”¨åŸºæ–¼å­—ç¬¦çš„åˆ†å‰² ---\n",
      "\n",
      "--- æ­£åœ¨å»ºç«‹å‘é‡å­˜å„² chroma_db_char_nb ---\n",
      "--- å®Œæˆå»ºç«‹å‘é‡å­˜å„² chroma_db_char_nb ---\n",
      "\n",
      "--- ä½¿ç”¨åŸºæ–¼å¥å­çš„åˆ†å‰² ---\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d06df91e9c4b476fb1bea4214b154bb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "modules.json:   0%|          | 0.00/349 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06d05e8df8484b67b3087c4489679fcf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config_sentence_transformers.json:   0%|          | 0.00/116 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6de30a086d884660abb591c9f11f4fa8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "97da04636b8a4ff8a1b6ad37b6482364",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "sentence_bert_config.json:   0%|          | 0.00/53.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "65941fb5f4f94890996ad500d4158901",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "09923cb04fe0447f950f3d9045846ac5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/438M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e252ba7bf3b4284aeaa8c33a67a0bd0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer_config.json:   0%|          | 0.00/363 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7edb17b01e54e2f92ff26364892e585",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "vocab.txt: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ff4f8d06363477ba7b8070a8579d0b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "tokenizer.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "500ab2e95e0e45c6a31ab0b36f98fd33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "special_tokens_map.json:   0%|          | 0.00/239 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dda0c32ce76d4913927f1ff21ba4a4b0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/190 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- æ­£åœ¨å»ºç«‹å‘é‡å­˜å„² chroma_db_sent_nb ---\n",
      "--- å®Œæˆå»ºç«‹å‘é‡å­˜å„² chroma_db_sent_nb ---\n",
      "\n",
      "--- ä½¿ç”¨åŸºæ–¼æ¨™è¨˜çš„åˆ†å‰² ---\n",
      "\n",
      "--- æ­£åœ¨å»ºç«‹å‘é‡å­˜å„² chroma_db_token_nb ---\n",
      "--- å®Œæˆå»ºç«‹å‘é‡å­˜å„² chroma_db_token_nb ---\n",
      "\n",
      "--- ä½¿ç”¨éè¿´åŸºæ–¼å­—ç¬¦çš„åˆ†å‰² ---\n",
      "\n",
      "--- æ­£åœ¨å»ºç«‹å‘é‡å­˜å„² chroma_db_rec_char_nb ---\n",
      "--- å®Œæˆå»ºç«‹å‘é‡å­˜å„² chroma_db_rec_char_nb ---\n",
      "\n",
      "--- ä½¿ç”¨è‡ªè¨‚åˆ†å‰² ---\n",
      "\n",
      "--- æ­£åœ¨å»ºç«‹å‘é‡å­˜å„² chroma_db_custom_nb ---\n",
      "--- å®Œæˆå»ºç«‹å‘é‡å­˜å„² chroma_db_custom_nb ---\n",
      "\n",
      "=== æ‰€æœ‰å‘é‡å­˜å„²å·²å»ºç«‹å®Œæˆ ===\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬1å€‹å„²å­˜æ ¼ï¼š5ç¨®æ–‡æœ¬åˆ†å‰²ç­–ç•¥ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«\n",
    "\n",
    "import os\n",
    "from langchain.text_splitter import (\n",
    "    CharacterTextSplitter,\n",
    "    RecursiveCharacterTextSplitter,\n",
    "    SentenceTransformersTokenTextSplitter,\n",
    "    TextSplitter,\n",
    "    TokenTextSplitter,\n",
    ")\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# å®šç¾©åŒ…å«æ–‡å­—æª”æ¡ˆçš„ç›®éŒ„\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "file_path = os.path.join(current_dir, \"books\", \"ä¿¡ç”¨å¡æ¬Šç›Šèªªæ˜.txt\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# æª¢æŸ¥æ–‡å­—æª”æ¡ˆæ˜¯å¦å­˜åœ¨\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"æª”æ¡ˆ {file_path} ä¸å­˜åœ¨ã€‚è«‹æª¢æŸ¥è·¯å¾‘ã€‚\"\n",
    "    )\n",
    "\n",
    "# å¾æª”æ¡ˆè®€å–æ–‡å­—å…§å®¹\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# å®šç¾©åµŒå…¥æ¨¡å‹\n",
    "embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n",
    ")\n",
    "\n",
    "# å»ºç«‹ä¸¦æŒä¹…åŒ–å‘é‡å­˜å„²çš„å‡½æ•¸\n",
    "def create_vector_store(docs, store_name):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(f\"\\n--- æ­£åœ¨å»ºç«‹å‘é‡å­˜å„² {store_name} ---\")\n",
    "        db = Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory\n",
    "        )\n",
    "        print(f\"--- å®Œæˆå»ºç«‹å‘é‡å­˜å„² {store_name} ---\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"å‘é‡å­˜å„² {store_name} å·²å­˜åœ¨ã€‚ç„¡éœ€åˆå§‹åŒ–ã€‚\")\n",
    "\n",
    "# 1. åŸºæ–¼å­—ç¬¦çš„åˆ†å‰²\n",
    "# æ ¹æ“šæŒ‡å®šçš„å­—ç¬¦æ•¸å°‡æ–‡æœ¬åˆ†å‰²æˆå¡Š\n",
    "# é©ç”¨æ–¼ç„¡è«–å…§å®¹çµæ§‹å¦‚ä½•éƒ½éœ€è¦ä¸€è‡´å¡Šå¤§å°çš„æƒ…æ³\n",
    "print(\"\\n--- ä½¿ç”¨åŸºæ–¼å­—ç¬¦çš„åˆ†å‰² ---\")\n",
    "char_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "char_docs = char_splitter.split_documents(documents)\n",
    "create_vector_store(char_docs, \"chroma_db_char_nb\")\n",
    "\n",
    "# 2. åŸºæ–¼å¥å­çš„åˆ†å‰²\n",
    "# æ ¹æ“šå¥å­å°‡æ–‡æœ¬åˆ†å‰²æˆå¡Šï¼Œç¢ºä¿å¡Šåœ¨å¥å­é‚Šç•Œè™•çµæŸ\n",
    "# é©ç”¨æ–¼åœ¨å¡Šå…§ä¿æŒèªç¾©é€£è²«æ€§\n",
    "print(\"\\n--- ä½¿ç”¨åŸºæ–¼å¥å­çš„åˆ†å‰² ---\")\n",
    "sent_splitter = SentenceTransformersTokenTextSplitter(chunk_size=1000)\n",
    "sent_docs = sent_splitter.split_documents(documents)\n",
    "create_vector_store(sent_docs, \"chroma_db_sent_nb\")\n",
    "\n",
    "# 3. åŸºæ–¼æ¨™è¨˜çš„åˆ†å‰²\n",
    "# ä½¿ç”¨æ¨™è¨˜å™¨ï¼ˆå¦‚ GPT-2ï¼‰æ ¹æ“šæ¨™è¨˜ï¼ˆå–®è©æˆ–å­è©ï¼‰å°‡æ–‡æœ¬åˆ†å‰²æˆå¡Š\n",
    "# é©ç”¨æ–¼å…·æœ‰åš´æ ¼æ¨™è¨˜é™åˆ¶çš„è½‰æ›å™¨æ¨¡å‹\n",
    "print(\"\\n--- ä½¿ç”¨åŸºæ–¼æ¨™è¨˜çš„åˆ†å‰² ---\")\n",
    "token_splitter = TokenTextSplitter(chunk_overlap=0, chunk_size=512)\n",
    "token_docs = token_splitter.split_documents(documents)\n",
    "create_vector_store(token_docs, \"chroma_db_token_nb\")\n",
    "\n",
    "# 4. éè¿´åŸºæ–¼å­—ç¬¦çš„åˆ†å‰²\n",
    "# å˜—è©¦åœ¨å­—ç¬¦é™åˆ¶å…§çš„è‡ªç„¶é‚Šç•Œï¼ˆå¥å­ã€æ®µè½ï¼‰è™•åˆ†å‰²æ–‡æœ¬\n",
    "# åœ¨ä¿æŒé€£è²«æ€§å’Œéµå®ˆå­—ç¬¦é™åˆ¶ä¹‹é–“å–å¾—å¹³è¡¡\n",
    "print(\"\\n--- ä½¿ç”¨éè¿´åŸºæ–¼å­—ç¬¦çš„åˆ†å‰² ---\")\n",
    "rec_char_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=1000, chunk_overlap=100)\n",
    "rec_char_docs = rec_char_splitter.split_documents(documents)\n",
    "create_vector_store(rec_char_docs, \"chroma_db_rec_char_nb\")\n",
    "\n",
    "# 5. è‡ªè¨‚åˆ†å‰²\n",
    "# å…è¨±æ ¹æ“šç‰¹å®šéœ€æ±‚å»ºç«‹è‡ªè¨‚åˆ†å‰²é‚è¼¯\n",
    "# é©ç”¨æ–¼æ¨™æº–åˆ†å‰²å™¨ç„¡æ³•è™•ç†çš„å…·æœ‰ç¨ç‰¹çµæ§‹çš„æ–‡ä»¶\n",
    "print(\"\\n--- ä½¿ç”¨è‡ªè¨‚åˆ†å‰² ---\")\n",
    "\n",
    "class CustomTextSplitter(TextSplitter):\n",
    "    def split_text(self, text):\n",
    "        # è‡ªè¨‚åˆ†å‰²æ–‡æœ¬çš„é‚è¼¯\n",
    "        return text.split(\"\\n\\n\")  # ç¯„ä¾‹ï¼šæŒ‰æ®µè½åˆ†å‰²\n",
    "\n",
    "custom_splitter = CustomTextSplitter()\n",
    "custom_docs = custom_splitter.split_documents(documents)\n",
    "create_vector_store(custom_docs, \"chroma_db_custom_nb\")\n",
    "\n",
    "print(\"\\n=== æ‰€æœ‰å‘é‡å­˜å„²å·²å»ºç«‹å®Œæˆ ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-base-zh and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.10.intermediate.dense.bias', 'encoder.layer.10.intermediate.dense.weight', 'encoder.layer.10.output.LayerNorm.bias', 'encoder.layer.10.output.LayerNorm.weight', 'encoder.layer.10.output.dense.bias', 'encoder.layer.10.output.dense.weight', 'encoder.layer.11.intermediate.dense.bias', 'encoder.layer.11.intermediate.dense.weight', 'encoder.layer.11.output.LayerNorm.bias', 'encoder.layer.11.output.LayerNorm.weight', 'encoder.layer.11.output.dense.bias', 'encoder.layer.11.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight', 'encoder.layer.4.intermediate.dense.bias', 'encoder.layer.4.intermediate.dense.weight', 'encoder.layer.4.output.LayerNorm.bias', 'encoder.layer.4.output.LayerNorm.weight', 'encoder.layer.4.output.dense.bias', 'encoder.layer.4.output.dense.weight', 'encoder.layer.5.intermediate.dense.bias', 'encoder.layer.5.intermediate.dense.weight', 'encoder.layer.5.output.LayerNorm.bias', 'encoder.layer.5.output.LayerNorm.weight', 'encoder.layer.5.output.dense.bias', 'encoder.layer.5.output.dense.weight', 'encoder.layer.6.intermediate.dense.bias', 'encoder.layer.6.intermediate.dense.weight', 'encoder.layer.6.output.LayerNorm.bias', 'encoder.layer.6.output.LayerNorm.weight', 'encoder.layer.6.output.dense.bias', 'encoder.layer.6.output.dense.weight', 'encoder.layer.7.intermediate.dense.bias', 'encoder.layer.7.intermediate.dense.weight', 'encoder.layer.7.output.LayerNorm.bias', 'encoder.layer.7.output.LayerNorm.weight', 'encoder.layer.7.output.dense.bias', 'encoder.layer.7.output.dense.weight', 'encoder.layer.8.intermediate.dense.bias', 'encoder.layer.8.intermediate.dense.weight', 'encoder.layer.8.output.LayerNorm.bias', 'encoder.layer.8.output.LayerNorm.weight', 'encoder.layer.8.output.dense.bias', 'encoder.layer.8.output.dense.weight', 'encoder.layer.9.intermediate.dense.bias', 'encoder.layer.9.intermediate.dense.weight', 'encoder.layer.9.output.LayerNorm.bias', 'encoder.layer.9.output.LayerNorm.weight', 'encoder.layer.9.output.dense.bias', 'encoder.layer.9.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "======================================================================\n",
      "æŸ¥è©¢å•é¡Œï¼šä¿¡ç”¨å¡çš„ç¾é‡‘å›é¥‹æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿ\n",
      "======================================================================\n",
      "\n",
      "======================================================================\n",
      "åˆ†å‰²ç­–ç•¥ï¼šCharacterTextSplitter (å­—ç¬¦åˆ†å‰²)\n",
      "======================================================================\n",
      "\n",
      "æ‰¾åˆ° 2 å€‹ç›¸é—œæ–‡ä»¶\n",
      "\n",
      "--- æ–‡ä»¶ 1 ---\n",
      "å…§å®¹é•·åº¦: 962 å­—å…ƒ\n",
      "å…§å®¹é è¦½ï¼š\n",
      "é»æ•¸ä¿è­·ï¼š\n",
      "- å¡ç‰‡éºå¤±è£œç™¼ï¼Œé»æ•¸ä¿ç•™\n",
      "- ç›œåˆ·æ¶ˆè²»çš„é»æ•¸æœƒæ‰£å›\n",
      "- é€€è²¨æ¶ˆè²»çš„é»æ•¸æœƒæ‰£å›\n",
      "\n",
      "é»æ•¸ä½¿ç”¨ç­–ç•¥ï¼š\n",
      "å»ºè­° 1ï¼šé›†ä¸­æ¶ˆè²»\n",
      "- ä¸»åŠ›ä½¿ç”¨ä¸€å¼µå¡ç´¯ç©é»æ•¸\n",
      "- é¿å…é»æ•¸åˆ†æ•£\n",
      "\n",
      "å»ºè­° 2ï¼šç•™æ„åˆ°æœŸæ—¥\n",
      "- å„ªå…ˆä½¿ç”¨å³å°‡åˆ°æœŸé»æ•¸\n",
      "- è¨­å®šåˆ°æœŸæé†’\n",
      "\n",
      "å»ºè­° 3ï¼šé«˜åƒ¹å€¼å…Œæ›\n",
      "- å…Œæ›åˆ’ç®—å•†å“ï¼ˆCPå€¼é«˜ï¼‰\n",
      "- å¤§å‹å®¶é›»é€šå¸¸è¼ƒåˆ’ç®—\n",
      "- é¿å…å…Œæ›ä½åƒ¹å€¼å•†å“\n",
      "\n",
      "å»ºè­° 4ï¼šåˆä½µé»æ•¸\n",
      "- å°‡é™„å¡é»æ•¸è½‰å…¥æ­£å¡\n",
      "- é”åˆ°...\n",
      "\n",
      "--- æ–‡ä»¶ 2 ---\n",
      "å…§å®¹é•·åº¦: 939 å­—å…ƒ\n",
      "å…§å®¹é è¦½ï¼š\n",
      "7.3 æ¶ˆè²»çˆ­è­°è™•ç†\n",
      "----------------------------------------\n",
      "å¸¸è¦‹æ¶ˆè²»çˆ­è­°ï¼š\n",
      "\n",
      "çˆ­è­°é¡å‹ 1ï¼šæœªæ”¶åˆ°å•†å“\n",
      "è™•ç†æ–¹å¼ï¼š\n",
      "1. å…ˆè¯çµ¡å•†å®¶ç¢ºèªå‡ºè²¨\n",
      "2. æŸ¥è©¢ç‰©æµé€²åº¦\n",
      "3. è‹¥å•†å®¶ç„¡æ³•è™•ç†ï¼Œå‘éŠ€è¡Œç”³è¨´\n",
      "4. æä¾›è¨‚å–®ç´€éŒ„ã€å°è©±ç´€éŒ„\n",
      "5. éŠ€è¡Œå”åŠ©å‘å•†å®¶æ±‚å„Ÿ\n",
      "\n",
      "çˆ­è­°é¡å‹ 2ï¼šå•†å“ç‘•ç–µ\n",
      "è™•ç†æ–¹å¼ï¼š\n",
      "1. ä¾æ¶ˆä¿æ³• 7å¤©é‘‘è³æœŸé€€è²¨\n",
      "2. å•†å®¶æ‹’çµ•é€€è²¨å†å‘éŠ€è¡Œç”³è¨´\n",
      "3...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "åˆ†å‰²ç­–ç•¥ï¼šSentenceTransformersTokenTextSplitter (å¥å­åˆ†å‰²)\n",
      "======================================================================\n",
      "\n",
      "æ‰¾åˆ° 2 å€‹ç›¸é—œæ–‡ä»¶\n",
      "\n",
      "--- æ–‡ä»¶ 1 ---\n",
      "å…§å®¹é•·åº¦: 1550 å­—å…ƒ\n",
      "å…§å®¹é è¦½ï¼š\n",
      "äºº å£« ï¼š [UNK] [UNK] + [UNK] [UNK] [UNK] [UNK] åŠ› [UNK] æ˜ ï¼ˆ [UNK] ä¸€ ï¼‰ ï¼š - [UNK] [UNK] [UNK] [UNK] [UNK] æ˜ ï¼ˆ [UNK] 3 [UNK] æœˆ ï¼‰ - [UNK] [UNK] [UNK] [UNK] - [UNK] [UNK] æ¸… [UNK] - ä¸ [UNK] [UNK] [UNK] æ˜ - å®š [U...\n",
      "\n",
      "--- æ–‡ä»¶ 2 ---\n",
      "å…§å®¹é•·åº¦: 1472 å­—å…ƒ\n",
      "å…§å®¹é è¦½ï¼š\n",
      "[UNK] - budget ï¼š [UNK] å ´ [UNK] [UNK] [UNK] [UNK] = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = = [UNK] [UNK] ç«  ï¼š [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] [UNK] = = = = = = =...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "åˆ†å‰²ç­–ç•¥ï¼šRecursiveCharacterTextSplitter (éè¿´å­—ç¬¦åˆ†å‰²)\n",
      "======================================================================\n",
      "\n",
      "æ‰¾åˆ° 2 å€‹ç›¸é—œæ–‡ä»¶\n",
      "\n",
      "--- æ–‡ä»¶ 1 ---\n",
      "å…§å®¹é•·åº¦: 962 å­—å…ƒ\n",
      "å…§å®¹é è¦½ï¼š\n",
      "é»æ•¸ä¿è­·ï¼š\n",
      "- å¡ç‰‡éºå¤±è£œç™¼ï¼Œé»æ•¸ä¿ç•™\n",
      "- ç›œåˆ·æ¶ˆè²»çš„é»æ•¸æœƒæ‰£å›\n",
      "- é€€è²¨æ¶ˆè²»çš„é»æ•¸æœƒæ‰£å›\n",
      "\n",
      "é»æ•¸ä½¿ç”¨ç­–ç•¥ï¼š\n",
      "å»ºè­° 1ï¼šé›†ä¸­æ¶ˆè²»\n",
      "- ä¸»åŠ›ä½¿ç”¨ä¸€å¼µå¡ç´¯ç©é»æ•¸\n",
      "- é¿å…é»æ•¸åˆ†æ•£\n",
      "\n",
      "å»ºè­° 2ï¼šç•™æ„åˆ°æœŸæ—¥\n",
      "- å„ªå…ˆä½¿ç”¨å³å°‡åˆ°æœŸé»æ•¸\n",
      "- è¨­å®šåˆ°æœŸæé†’\n",
      "\n",
      "å»ºè­° 3ï¼šé«˜åƒ¹å€¼å…Œæ›\n",
      "- å…Œæ›åˆ’ç®—å•†å“ï¼ˆCPå€¼é«˜ï¼‰\n",
      "- å¤§å‹å®¶é›»é€šå¸¸è¼ƒåˆ’ç®—\n",
      "- é¿å…å…Œæ›ä½åƒ¹å€¼å•†å“\n",
      "\n",
      "å»ºè­° 4ï¼šåˆä½µé»æ•¸\n",
      "- å°‡é™„å¡é»æ•¸è½‰å…¥æ­£å¡\n",
      "- é”åˆ°...\n",
      "\n",
      "--- æ–‡ä»¶ 2 ---\n",
      "å…§å®¹é•·åº¦: 939 å­—å…ƒ\n",
      "å…§å®¹é è¦½ï¼š\n",
      "7.3 æ¶ˆè²»çˆ­è­°è™•ç†\n",
      "----------------------------------------\n",
      "å¸¸è¦‹æ¶ˆè²»çˆ­è­°ï¼š\n",
      "\n",
      "çˆ­è­°é¡å‹ 1ï¼šæœªæ”¶åˆ°å•†å“\n",
      "è™•ç†æ–¹å¼ï¼š\n",
      "1. å…ˆè¯çµ¡å•†å®¶ç¢ºèªå‡ºè²¨\n",
      "2. æŸ¥è©¢ç‰©æµé€²åº¦\n",
      "3. è‹¥å•†å®¶ç„¡æ³•è™•ç†ï¼Œå‘éŠ€è¡Œç”³è¨´\n",
      "4. æä¾›è¨‚å–®ç´€éŒ„ã€å°è©±ç´€éŒ„\n",
      "5. éŠ€è¡Œå”åŠ©å‘å•†å®¶æ±‚å„Ÿ\n",
      "\n",
      "çˆ­è­°é¡å‹ 2ï¼šå•†å“ç‘•ç–µ\n",
      "è™•ç†æ–¹å¼ï¼š\n",
      "1. ä¾æ¶ˆä¿æ³• 7å¤©é‘‘è³æœŸé€€è²¨\n",
      "2. å•†å®¶æ‹’çµ•é€€è²¨å†å‘éŠ€è¡Œç”³è¨´\n",
      "3...\n",
      "\n",
      "----------------------------------------------------------------------\n",
      "\n",
      "======================================================================\n",
      "ğŸ“Š è§€å¯Ÿé‡é»\n",
      "======================================================================\n",
      "\n",
      "ğŸ” æ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥æª¢ç´¢åˆ°çš„æ–‡ä»¶ï¼š\n",
      "1. æ–‡ä»¶å…§å®¹æ˜¯å¦å®Œæ•´ï¼Ÿ\n",
      "2. åˆ†å‰²é‚Šç•Œæ˜¯å¦åˆç†ï¼Ÿ\n",
      "3. å“ªç¨®ç­–ç•¥æœ€èƒ½ä¿ç•™èªç¾©å®Œæ•´æ€§ï¼Ÿ\n",
      "\n",
      "ğŸ’¡ æç¤ºï¼š\n",
      "- CharacterTextSplitterï¼šå›ºå®šå­—å…ƒæ•¸åˆ†å‰²ï¼Œå¯èƒ½åˆ‡æ–·å¥å­\n",
      "- SentenceTransformersï¼šæŒ‰å¥å­é‚Šç•Œåˆ†å‰²ï¼Œä¿æŒèªç¾©å®Œæ•´\n",
      "- RecursiveCharacterTextSplitterï¼šæ™ºæ…§åˆ†å‰²ï¼Œå¹³è¡¡å®Œæ•´æ€§å’Œå¤§å°\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/7g/kb_wcfhj3710_xnyqdgx_v940000gn/T/ipykernel_3931/3411143644.py:30: LangChainDeprecationWarning: The class `Chroma` was deprecated in LangChain 0.2.9 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-chroma package and should be used instead. To use it run `pip install -U :class:`~langchain-chroma` and import as `from :class:`~langchain_chroma import Chroma``.\n",
      "  db = Chroma(\n"
     ]
    }
   ],
   "source": [
    "# ç¬¬2å€‹å„²å­˜æ ¼ï¼šæ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥çš„æŸ¥è©¢çµæœ\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# å®šç¾©è·¯å¾‘\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# å®šç¾©åµŒå…¥æ¨¡å‹\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n",
    "\n",
    "# å®šç¾©ä½¿ç”¨è€…çš„å•é¡Œ\n",
    "query = \"ä¿¡ç”¨å¡çš„ç¾é‡‘å›é¥‹æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿ\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"æŸ¥è©¢å•é¡Œï¼š{query}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# æ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥çš„æª¢ç´¢çµæœ\n",
    "def compare_splitting_strategies(store_name, strategy_name):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if os.path.exists(persistent_directory):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"åˆ†å‰²ç­–ç•¥ï¼š{strategy_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # è¼‰å…¥å‘é‡è³‡æ–™åº«\n",
    "        db = Chroma(\n",
    "            persist_directory=persistent_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        # å»ºç«‹æª¢ç´¢å™¨\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 2}\n",
    "        )\n",
    "        \n",
    "        # æª¢ç´¢ç›¸é—œæ–‡ä»¶\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        print(f\"\\næ‰¾åˆ° {len(retrieved_docs)} å€‹ç›¸é—œæ–‡ä»¶\")\n",
    "        \n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            print(f\"\\n--- æ–‡ä»¶ {i} ---\")\n",
    "            print(f\"å…§å®¹é•·åº¦: {len(doc.page_content)} å­—å…ƒ\")\n",
    "            print(f\"å…§å®¹é è¦½ï¼š\")\n",
    "            print(f\"{doc.page_content[:200]}...\")\n",
    "        \n",
    "        print(f\"\\n{'-'*70}\")\n",
    "    else:\n",
    "        print(f\"å‘é‡å­˜å„² {store_name} ä¸å­˜åœ¨ã€‚\")\n",
    "\n",
    "# æ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥\n",
    "compare_splitting_strategies(\"chroma_db_char_nb\", \"CharacterTextSplitter (å­—ç¬¦åˆ†å‰²)\")\n",
    "compare_splitting_strategies(\"chroma_db_sent_nb\", \"SentenceTransformersTokenTextSplitter (å¥å­åˆ†å‰²)\")\n",
    "compare_splitting_strategies(\"chroma_db_rec_char_nb\", \"RecursiveCharacterTextSplitter (éè¿´å­—ç¬¦åˆ†å‰²)\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š è§€å¯Ÿé‡é»\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "ğŸ” æ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥æª¢ç´¢åˆ°çš„æ–‡ä»¶ï¼š\n",
    "1. æ–‡ä»¶å…§å®¹æ˜¯å¦å®Œæ•´ï¼Ÿ\n",
    "2. åˆ†å‰²é‚Šç•Œæ˜¯å¦åˆç†ï¼Ÿ\n",
    "3. å“ªç¨®ç­–ç•¥æœ€èƒ½ä¿ç•™èªç¾©å®Œæ•´æ€§ï¼Ÿ\n",
    "\n",
    "ğŸ’¡ æç¤ºï¼š\n",
    "- CharacterTextSplitterï¼šå›ºå®šå­—å…ƒæ•¸åˆ†å‰²ï¼Œå¯èƒ½åˆ‡æ–·å¥å­\n",
    "- SentenceTransformersï¼šæŒ‰å¥å­é‚Šç•Œåˆ†å‰²ï¼Œä¿æŒèªç¾©å®Œæ•´\n",
    "- RecursiveCharacterTextSplitterï¼šæ™ºæ…§åˆ†å‰²ï¼Œå¹³è¡¡å®Œæ•´æ€§å’Œå¤§å°\n",
    "\"\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç¬¬3å€‹å„²å­˜æ ¼ï¼šæ•´åˆ Chain åŠŸèƒ½ - æ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥çš„ RAG Chain\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "# å®šç¾©è·¯å¾‘\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# å®šç¾©åµŒå…¥æ¨¡å‹\n",
    "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n",
    "\n",
    "# åˆå§‹åŒ– LLM\n",
    "llm = ChatOpenAI(model=\"gpt-4o-mini\", temperature=0)\n",
    "\n",
    "# å®šç¾© Prompt æ¨¡æ¿\n",
    "prompt = ChatPromptTemplate.from_template(\"\"\"ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„å®¢æœåŠ©ç†ï¼Œè«‹æ ¹æ“šæä¾›çš„è³‡æ–™å›ç­”ä½¿ç”¨è€…å•é¡Œã€‚\n",
    "\n",
    "åƒè€ƒè³‡æ–™ï¼š\n",
    "{context}\n",
    "\n",
    "å•é¡Œï¼š{question}\n",
    "\n",
    "è«‹æä¾›æ¸…æ¥šã€æº–ç¢ºçš„å›ç­”ï¼š\"\"\")\n",
    "\n",
    "# æ ¼å¼åŒ–æ–‡ä»¶çš„å‡½æ•¸\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# ä½¿ç”¨ä¸åŒåˆ†å‰²ç­–ç•¥çš„ RAG Chain ä¸¦æ¯”è¼ƒçµæœ\n",
    "def compare_rag_chains(store_name, query, strategy_name):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if os.path.exists(persistent_directory):\n",
    "        print(f\"\\n{'='*70}\")\n",
    "        print(f\"åˆ†å‰²ç­–ç•¥ï¼š{strategy_name}\")\n",
    "        print(f\"{'='*70}\")\n",
    "        \n",
    "        # è¼‰å…¥å‘é‡è³‡æ–™åº«\n",
    "        db = Chroma(\n",
    "            persist_directory=persistent_directory,\n",
    "            embedding_function=embeddings\n",
    "        )\n",
    "        \n",
    "        # å»ºç«‹æª¢ç´¢å™¨\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=\"similarity\",\n",
    "            search_kwargs={\"k\": 2}\n",
    "        )\n",
    "        \n",
    "        # æ­¥é©Ÿ 1ï¼šæª¢ç´¢ç›¸é—œæ–‡ä»¶\n",
    "        print(f\"\\nğŸ“‹ æ­¥é©Ÿ 1ï¼šæª¢ç´¢ç›¸é—œæ–‡ä»¶\")\n",
    "        retrieved_docs = retriever.invoke(query)\n",
    "        print(f\"æ‰¾åˆ° {len(retrieved_docs)} å€‹ç›¸é—œæ–‡ä»¶\")\n",
    "        for i, doc in enumerate(retrieved_docs, 1):\n",
    "            print(f\"\\næ–‡ä»¶ {i} å…§å®¹é è¦½ï¼š\")\n",
    "            print(f\"{doc.page_content[:150]}...\")\n",
    "        \n",
    "        # æ­¥é©Ÿ 2ï¼šå»ºç«‹ RAG Chain\n",
    "        print(f\"\\nâ›“ï¸  æ­¥é©Ÿ 2ï¼šå»ºç«‹ RAG Chain\")\n",
    "        rag_chain = (\n",
    "            {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "            | prompt\n",
    "            | llm\n",
    "            | StrOutputParser()\n",
    "        )\n",
    "        \n",
    "        # æ­¥é©Ÿ 3ï¼šåŸ·è¡Œ Chain ä¸¦å–å¾—ç­”æ¡ˆ\n",
    "        print(f\"\\nğŸ¤– æ­¥é©Ÿ 3ï¼šç”Ÿæˆå›ç­”\")\n",
    "        answer = rag_chain.invoke(query)\n",
    "        print(f\"\\nã€{strategy_name} çš„å›ç­”ã€‘\")\n",
    "        print(answer)\n",
    "        print(f\"\\n{'-'*70}\")\n",
    "        \n",
    "        return answer\n",
    "    else:\n",
    "        print(f\"å‘é‡å­˜å„² {store_name} ä¸å­˜åœ¨ã€‚\")\n",
    "        return None\n",
    "\n",
    "# å®šç¾©ä½¿ç”¨è€…çš„å•é¡Œ\n",
    "query = \"ä¿¡ç”¨å¡çš„ç¾é‡‘å›é¥‹æ¯”ä¾‹æ˜¯å¤šå°‘ï¼Ÿå¹´è²»å¦‚ä½•è¨ˆç®—ï¼Ÿ\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(f\"æŸ¥è©¢å•é¡Œï¼š{query}\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "# æ¯”è¼ƒä¸åŒåˆ†å‰²ç­–ç•¥çš„ RAG Chain æ•ˆæœ\n",
    "results = {}\n",
    "results[\"CharacterTextSplitter\"] = compare_rag_chains(\n",
    "    \"chroma_db_char_nb\", query, \"CharacterTextSplitter (å­—ç¬¦åˆ†å‰²)\"\n",
    ")\n",
    "results[\"SentenceTransformersTokenTextSplitter\"] = compare_rag_chains(\n",
    "    \"chroma_db_sent_nb\", query, \"SentenceTransformersTokenTextSplitter (å¥å­åˆ†å‰²)\"\n",
    ")\n",
    "results[\"RecursiveCharacterTextSplitter\"] = compare_rag_chains(\n",
    "    \"chroma_db_rec_char_nb\", query, \"RecursiveCharacterTextSplitter (éè¿´å­—ç¬¦åˆ†å‰²)\"\n",
    ")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ğŸ“Š åˆ†å‰²ç­–ç•¥å° RAG Chain çš„å½±éŸ¿ç¸½çµ\")\n",
    "print(\"=\"*70)\n",
    "print(\"\"\"\n",
    "ğŸ” è§€å¯Ÿé‡é»ï¼š\n",
    "1. ä¸åŒåˆ†å‰²ç­–ç•¥æª¢ç´¢åˆ°çš„æ–‡ä»¶å…§å®¹æ˜¯å¦å®Œæ•´\n",
    "2. åˆ†å‰²é‚Šç•Œæ˜¯å¦åˆ‡æ–·äº†é‡è¦è³‡è¨Š\n",
    "3. å“ªç¨®ç­–ç•¥çš„ç­”æ¡ˆæœ€æº–ç¢ºã€æœ€å®Œæ•´\n",
    "\n",
    "ğŸ’¡ æœ€ä½³å¯¦è¸å»ºè­°ï¼š\n",
    "- CharacterTextSplitterï¼šé©ç”¨æ–¼çµæ§‹ç°¡å–®ã€æ ¼å¼çµ±ä¸€çš„æ–‡ä»¶\n",
    "- SentenceTransformersï¼šé©åˆéœ€è¦ä¿æŒèªç¾©å®Œæ•´æ€§çš„æ–‡ä»¶\n",
    "- RecursiveCharacterTextSplitterï¼šæ¨è–¦ï¼åœ¨å®Œæ•´æ€§å’Œéˆæ´»æ€§ä¹‹é–“å–å¾—æœ€ä½³å¹³è¡¡\n",
    "\n",
    "ğŸ“Œ é—œéµå­¸ç¿’ï¼š\n",
    "æ–‡æœ¬åˆ†å‰²ç­–ç•¥ç›´æ¥å½±éŸ¿ RAG ç³»çµ±çš„å›ç­”å“è³ªã€‚\n",
    "é¸æ“‡åˆé©çš„åˆ†å‰²ç­–ç•¥èƒ½å¤ ï¼š\n",
    "âœ… é¿å…é‡è¦è³‡è¨Šè¢«åˆ‡æ–·\n",
    "âœ… æé«˜æª¢ç´¢çš„æº–ç¢ºæ€§\n",
    "âœ… æ”¹å–„æœ€çµ‚ç­”æ¡ˆçš„å®Œæ•´åº¦\n",
    "\"\"\")\n",
    "print(\"=\"*70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
