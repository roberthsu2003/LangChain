{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Embedding 模型比較\n",
    "\n",
    "本範例展示：\n",
    "1. **第1個儲存格**：使用不同 Embedding 模型建立向量資料庫\n",
    "2. **第2個儲存格**：比較不同模型的檢索效果\n",
    "\n",
    "學習目標：理解不同 Embedding 模型對檢索結果的影響，選擇最適合的模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第1個儲存格：使用不同 Embedding 模型建立向量資料庫\n",
    "\n",
    "import os\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 定義包含文字檔案的目錄和持久化目錄\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "file_path = os.path.join(current_dir, \"books\", \"三國演義.txt\")\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# 檢查文字檔案是否存在\n",
    "if not os.path.exists(file_path):\n",
    "    raise FileNotFoundError(\n",
    "        f\"檔案 {file_path} 不存在。請檢查路徑。\"\n",
    "    )\n",
    "\n",
    "# 從檔案讀取文字內容\n",
    "loader = TextLoader(file_path)\n",
    "documents = loader.load()\n",
    "\n",
    "# 將文件分割成塊\n",
    "text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "docs = text_splitter.split_documents(documents)\n",
    "\n",
    "# 顯示分割文件的資訊\n",
    "print(\"\\n--- 文件塊資訊 ---\")\n",
    "print(f\"文件塊數量: {len(docs)}\")\n",
    "print(f\"範例塊:\\n{docs[0].page_content}\\n\")\n",
    "\n",
    "\n",
    "# 建立並持久化向量存儲的函數\n",
    "def create_vector_store(docs, embeddings, store_name):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if not os.path.exists(persistent_directory):\n",
    "        print(f\"\\n--- 正在建立向量存儲 {store_name} ---\")\n",
    "        Chroma.from_documents(\n",
    "            docs, embeddings, persist_directory=persistent_directory)\n",
    "        print(f\"--- 完成建立向量存儲 {store_name} ---\")\n",
    "    else:\n",
    "        print(\n",
    "            f\"向量存儲 {store_name} 已存在。無需初始化。\")\n",
    "\n",
    "\n",
    "# 1. Jina Embeddings v2 (繁體中文專用)\n",
    "# 專門為中英雙語優化的模型\n",
    "# 維度：768，最大序列長度：8192 tokens\n",
    "# 適用於：繁體中文內容、長文檔處理\n",
    "print(\"\\n--- 使用 Jina Embeddings v2（繁體中文專用）---\")\n",
    "jina_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n",
    ")\n",
    "create_vector_store(docs, jina_embeddings, \"chroma_db_jina_nb\")\n",
    "\n",
    "# 2. Multilingual E5 Large (多語言模型)\n",
    "# 微軟開發的多語言嵌入模型\n",
    "# 維度：1024，最大序列長度：512 tokens\n",
    "# 適用於：多語言內容、高精度需求\n",
    "print(\"\\n--- 使用 Multilingual E5 Large（多語言模型）---\")\n",
    "e5_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-large\"\n",
    ")\n",
    "create_vector_store(docs, e5_embeddings, \"chroma_db_e5_nb\")\n",
    "\n",
    "# 3. All-MiniLM-L6-v2 (輕量級英文模型)\n",
    "# Sentence Transformers 的輕量級模型\n",
    "# 維度：384，最大序列長度：256 tokens\n",
    "# 適用於：速度優先、資源受限環境\n",
    "print(\"\\n--- 使用 All-MiniLM-L6-v2（輕量級模型）---\")\n",
    "minilm_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "create_vector_store(docs, minilm_embeddings, \"chroma_db_minilm_nb\")\n",
    "\n",
    "# 4. All-MPNet-Base-v2 (平衡型英文模型)\n",
    "# Sentence Transformers 的平衡型模型\n",
    "# 維度：768，最大序列長度：384 tokens\n",
    "# 適用於：英文內容、平衡效能與速度\n",
    "print(\"\\n--- 使用 All-MPNet-Base-v2（平衡型模型）---\")\n",
    "mpnet_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "create_vector_store(docs, mpnet_embeddings, \"chroma_db_mpnet_nb\")\n",
    "\n",
    "print(\"\\n=== 所有向量存儲已建立完成 ===\")\n",
    "print(\"\\n模型特性比較：\")\n",
    "print(\"- Jina v2: 768維，8192 tokens，繁體中文專用\")\n",
    "print(\"- E5 Large: 1024維，512 tokens，多語言高精度\")\n",
    "print(\"- MiniLM: 384維，256 tokens，輕量快速\")\n",
    "print(\"- MPNet: 768維，384 tokens，英文平衡型\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第2個儲存格：比較不同模型的檢索效果\n",
    "\n",
    "import os\n",
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "\n",
    "# 定義路徑\n",
    "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "db_dir = os.path.join(current_dir, \"db\")\n",
    "\n",
    "# 查詢向量存儲的函數\n",
    "def query_vector_store(store_name, query, embedding_function):\n",
    "    persistent_directory = os.path.join(db_dir, store_name)\n",
    "    if os.path.exists(persistent_directory):\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(f\"正在查詢向量存儲：{store_name}\")\n",
    "        print(f\"{'='*60}\")\n",
    "        db = Chroma(\n",
    "            persist_directory=persistent_directory,\n",
    "            embedding_function=embedding_function,\n",
    "        )\n",
    "        retriever = db.as_retriever(\n",
    "            search_type=\"similarity_score_threshold\",\n",
    "            search_kwargs={\"k\": 3, \"score_threshold\": 0.1},\n",
    "        )\n",
    "        relevant_docs = retriever.invoke(query)\n",
    "        \n",
    "        # 顯示檢索結果數量\n",
    "        print(f\"\\n找到 {len(relevant_docs)} 個相關文件\\n\")\n",
    "        \n",
    "        # 顯示相關結果及元數據\n",
    "        for i, doc in enumerate(relevant_docs, 1):\n",
    "            print(f\"--- 文件 {i} ---\")\n",
    "            print(f\"{doc.page_content[:200]}...\")  # 只顯示前 200 字元\n",
    "            if doc.metadata:\n",
    "                print(f\"來源: {doc.metadata.get('source', 'Unknown')}\")\n",
    "            print()\n",
    "    else:\n",
    "        print(f\"向量存儲 {store_name} 不存在。\")\n",
    "\n",
    "# 定義不同的嵌入模型\n",
    "jina_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n",
    ")\n",
    "e5_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"intfloat/multilingual-e5-large\"\n",
    ")\n",
    "minilm_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-MiniLM-L6-v2\"\n",
    ")\n",
    "mpnet_embeddings = HuggingFaceEmbeddings(\n",
    "    model_name=\"sentence-transformers/all-mpnet-base-v2\"\n",
    ")\n",
    "\n",
    "# 定義使用者的問題\n",
    "query = \"劉備是誰?\"\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(f\"查詢問題：{query}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# 查詢每個向量存儲並比較結果\n",
    "query_vector_store(\"chroma_db_jina_nb\", query, jina_embeddings)\n",
    "query_vector_store(\"chroma_db_e5_nb\", query, e5_embeddings)\n",
    "query_vector_store(\"chroma_db_minilm_nb\", query, minilm_embeddings)\n",
    "query_vector_store(\"chroma_db_mpnet_nb\", query, mpnet_embeddings)\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"結論：\")\n",
    "print(\"=\"*60)\n",
    "print(\"1. Jina v2 和 E5 Large 對繁體中文的理解最佳\")\n",
    "print(\"2. MiniLM 和 MPNet 主要針對英文優化，中文效果較差\")\n",
    "print(\"3. 建議繁體中文應用使用 Jina v2 或 E5 Large\")\n",
    "print(\"4. 如需處理長文檔，Jina v2 (8192 tokens) 優於 E5 (512 tokens)\")\n",
    "print(\"=\"*60)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
