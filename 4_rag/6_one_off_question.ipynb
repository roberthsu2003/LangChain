{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 單次問答系統\n",
    "\n",
    "本範例展示：\n",
    "1. **第1個儲存格**：載入向量資料庫並進行相似度檢索\n",
    "2. **第2個儲存格**：整合 LLM 建立完整的 RAG 問答 Chain\n",
    "\n",
    "學習目標：理解如何將向量檢索與 LLM 結合，建立完整的問答系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第1個儲存格：載入向量資料庫並進行相似度檢索\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# 從 .env 載入環境變數\nload_dotenv()\n\n# 定義持久化目錄\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\npersistent_directory = os.path.join(\n    current_dir, \"db\", \"chroma_db_with_metadata_chinese_nb\")\n\n# 定義嵌入模型\nembeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n\n# 使用嵌入函數載入現有的向量存儲\ndb = Chroma(persist_directory=persistent_directory,\n            embedding_function=embeddings)\n\n# 定義使用者的問題\nquery = \"洗衣機如何清潔保養？\"\n\n# 根據查詢檢索相關文件\n# search_type=\"similarity\": 使用相似度搜尋\n# k=1: 只返回最相關的 1 個文件\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 1},
)
relevant_docs = retriever.invoke(query)\n\n# 顯示相關結果及元數據\nprint(\"\\n--- 相關文件 ---\")\nfor i, doc in enumerate(relevant_docs, 1):\n    print(f\"文件 {i}:\\n{doc.page_content}\\n\")\n    if doc.metadata:\n        print(f\"來源: {doc.metadata.get('source', 'Unknown')}\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第2個儲存格：整合 LLM 建立完整的 RAG 問答 Chain\n\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_ollama import ChatOllama\n\n# 合併查詢和相關文件內容\n# 這是 RAG 的核心：將檢索到的文件作為上下文提供給 LLM\ncombined_input = (\n    \"以下是一些可能有助於回答問題的文件：\"
    + query
    + \"\\n\\n相關文件：\\n\"
    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])
    + \"\\n\\n請僅根據提供的文件提供答案。如果在文件中找不到答案，請回覆「我不確定」。\"
)

print(\"\\n--- 合併後的輸入 ---\")
print(combined_input)

# 建立 ChatOllama 模型（使用本地 LLM ）\nmodel = ChatOllama(model=\"llama3.2\")\n
# 定義模型的訊息\n# SystemMessage: 設定 AI 的角色和行為\n# HumanMessage: 使用者的問題加上檢索到的上下文\nmessages = [
    SystemMessage(content=\"你是一個有幫助的助手。\"),
    HumanMessage(content=combined_input),
]

# 使用合併的輸入調用模型\nresult = model.invoke(messages)

# 顯示生成的回應\nprint(\"\\n--- 生成的回應 ---\")
print(\"僅內容：\")
print(result.content)

print(\"\\n--- RAG 流程說明 ---\")
print(\"1. 使用者提問\")
print(\"2. 向量資料庫檢索相關文件\")
print(\"3. 將文件作為上下文提供給 LLM\")
print(\"4. LLM 基於上下文生成答案\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第3個儲存格：使用 LCEL 建立 RAG Chain\n\nfrom langchain_core.prompts import ChatPromptTemplate\nfrom langchain_core.runnables import RunnablePassthrough\nfrom langchain_core.output_parsers import StrOutputParser\nfrom langchain_ollama import ChatOllama\n\n# 建立 ChatOllama 模型\n# 注意：模型已在第2個儲存格中建立，此處為保持儲存格獨立性而重新建立\nmodel = ChatOllama(model=\"llama3.2\")\n
# 建立提示模板\n# 這個模板指導 LLM 如何使用上下文來回答問題\ntemplate = '''\n僅根據以下上下文來回答問題：\n{context}\n\n問題：{question}\n'''
prompt = ChatPromptTemplate.from_template(template)

# 格式化文件函數\n# 將檢索到的文件列表轉換為單一字串\ndef format_docs(docs):\n    return "\n\n".join(doc.page_content for doc in docs)

# 建立 RAG Chain\n# 這是一個使用 LCEL (LangChain Expression Language) 的標準 RAG 實作\nrag_chain = (
    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}
    | prompt
    | model
    | StrOutputParser()
)

# 執行 RAG Chain\nprint(\"\\n--- 使用 LCEL RAG Chain 執行 ---\")
# 我們傳遞原始問題，Chain 會自動處理檢索、格式化和模型調用\nresponse = rag_chain.invoke(query)

print(f\"\\n問題: {query}\")
print(f\"\\nAI 回應:\\n{response}\")

print(\"\\n--- LCEL RAG Chain 流程說明 ---\")
print(\"1. RunnablePassthrough() 將使用者問題傳遞下去\")
print(\"2. `retriever | format_docs` 檢索文件並格式化為字串\")
print(\"3. `prompt` 將上下文和問題填入模板\")
print(\"4. `model` 使用提示生成回應\")
print(\"5. `StrOutputParser()` 提取回應內容\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
