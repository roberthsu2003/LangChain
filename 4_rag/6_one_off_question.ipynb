{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RAG 單次問答系統\n",
    "\n",
    "本範例展示：\n",
    "1. **第1個儲存格**：載入向量資料庫並進行相似度檢索\n",
    "2. **第2個儲存格**：整合 LLM 建立完整的 RAG 問答 Chain\n",
    "\n",
    "學習目標：理解如何將向量檢索與 LLM 結合，建立完整的問答系統"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第1個儲存格：載入向量資料庫並進行相似度檢索\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# 從 .env 載入環境變數\nload_dotenv()\n\n# 定義持久化目錄\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\npersistent_directory = os.path.join(\n    current_dir, \"db\", \"chroma_db_with_metadata_chinese_nb\")\n\n# 定義嵌入模型\nembeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n\n# 使用嵌入函數載入現有的向量存儲\ndb = Chroma(persist_directory=persistent_directory,\n            embedding_function=embeddings)\n\n# 定義使用者的問題\nquery = \"洗衣機如何清潔保養？\"\n\n# 根據查詢檢索相關文件\n# search_type=\"similarity\": 使用相似度搜尋\n# k=1: 只返回最相關的 1 個文件\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 1},\n)\nrelevant_docs = retriever.invoke(query)\n\n# 顯示相關結果及元數據\nprint(\"\\n--- 相關文件 ---\")\nfor i, doc in enumerate(relevant_docs, 1):\n    print(f\"文件 {i}:\\n{doc.page_content}\\n\")\n    if doc.metadata:\n        print(f\"來源: {doc.metadata.get('source', 'Unknown')}\\n\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第2個儲存格：整合 LLM 建立完整的 RAG 問答 Chain\n",
    "\n",
    "from langchain_core.messages import HumanMessage, SystemMessage\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 合併查詢和相關文件內容\n",
    "# 這是 RAG 的核心：將檢索到的文件作為上下文提供給 LLM\n",
    "combined_input = (\n",
    "    \"以下是一些可能有助於回答問題的文件：\"\n",
    "    + query\n",
    "    + \"\\n\\n相關文件：\\n\"\n",
    "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
    "    + \"\\n\\n請僅根據提供的文件提供答案。如果在文件中找不到答案，請回覆「我不確定」。\"\n",
    ")\n",
    "\n",
    "print(\"\\n--- 合併後的輸入 ---\")\n",
    "print(combined_input)\n",
    "\n",
    "# 建立 ChatOllama 模型（使用本地 LLM）\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "# 定義模型的訊息\n",
    "# SystemMessage: 設定 AI 的角色和行為\n",
    "# HumanMessage: 使用者的問題加上檢索到的上下文\n",
    "messages = [\n",
    "    SystemMessage(content=\"你是一個有幫助的助手。\"),\n",
    "    HumanMessage(content=combined_input),\n",
    "]\n",
    "\n",
    "# 使用合併的輸入調用模型\n",
    "result = model.invoke(messages)\n",
    "\n",
    "# 顯示生成的回應\n",
    "print(\"\\n--- 生成的回應 ---\")\n",
    "print(\"僅內容：\")\n",
    "print(result.content)\n",
    "\n",
    "print(\"\\n--- RAG 流程說明 ---\")\n",
    "print(\"1. 使用者提問\")\n",
    "print(\"2. 向量資料庫檢索相關文件\")\n",
    "print(\"3. 將文件作為上下文提供給 LLM\")\n",
    "print(\"4. LLM 基於上下文生成答案\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}