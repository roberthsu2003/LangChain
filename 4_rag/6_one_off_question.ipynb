{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# RAG 單次問答系統\n",
        "\n",
        "本範例展示：\n",
        "1. **第1個儲存格**：載入向量資料庫並進行相似度檢索\n",
        "2. **第2個儲存格**：整合 LLM 建立完整的 RAG 問答 Chain\n",
        "\n",
        "學習目標：理解如何將向量檢索與 LLM 結合，建立完整的問答系統"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第1個儲存格：載入向量資料庫並進行相似度檢索\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# 從 .env 載入環境變數\n",
        "load_dotenv()\n",
        "\n",
        "# 定義持久化目錄\n",
        "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "persistent_directory = os.path.join(\n",
        "    current_dir, \"db\", \"chroma_db_with_metadata_chinese_nb\")\n",
        "\n",
        "# 定義嵌入模型\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n",
        "\n",
        "# 使用嵌入函數載入現有的向量存儲\n",
        "db = Chroma(persist_directory=persistent_directory,\n",
        "            embedding_function=embeddings)\n",
        "\n",
        "# 定義使用者的問題\n",
        "query = \"洗衣機如何清潔保養？\"\n",
        "\n",
        "# 根據查詢檢索相關文件\n",
        "# search_type=\"similarity\": 使用相似度搜尋\n",
        "# k=1: 只返回最相關的 1 個文件\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 1},\n",
        ")\n",
        "relevant_docs = retriever.invoke(query)\n",
        "\n",
        "# 顯示相關結果及元數據\n",
        "print(\"\\n--- 相關文件 ---\")\n",
        "for i, doc in enumerate(relevant_docs, 1):\n",
        "    print(f\"文件 {i}:\\n{doc.page_content}\\n\")\n",
        "    if doc.metadata:\n",
        "        print(f\"來源: {doc.metadata.get('source', 'Unknown')}\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第2個儲存格：整合 LLM 建立完整的 RAG 問答 Chain\n",
        "\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# 合併查詢和相關文件內容\n",
        "# 這是 RAG 的核心：將檢索到的文件作為上下文提供給 LLM\n",
        "combined_input = (\n",
        "    \"以下是一些可能有助於回答問題：\"\n",
        "    + query\n",
        "    + \"\\n\\n相關文件：\\n\"\n",
        "    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n",
        "    + \"\\n\\n請僅根據提供的文件提供答案。如果在文件中找不到答案，請回覆「我不確定」。\"\n",
        ")\n",
        "\n",
        "print(\"\\n--- 合併後的輸入 ---\")\n",
        "print(combined_input)\n",
        "\n",
        "# 建立 ChatOllama 模型（使用本地 LLM）\n",
        "model = ChatOllama(model=\"llama3.2\")\n",
        "\n",
        "# 定義模型的訊息\n",
        "# SystemMessage: 設定 AI 的角色和行為\n",
        "# HumanMessage: 使用者的問題加上檢索到的上下文\n",
        "messages = [\n",
        "    SystemMessage(content=\"你是一個有幫助的助手。\"),\n",
        "    HumanMessage(content=combined_input),\n",
        "]\n",
        "\n",
        "# 使用合併的輸入調用模型\n",
        "result = model.invoke(messages)\n",
        "\n",
        "# 顯示生成的回應\n",
        "print(\"\\n--- 生成的回應 ---\")\n",
        "print(\"僅內容：\")\n",
        "print(result.content)\n",
        "\n",
        "print(\"\\n--- RAG 流程說明 ---\")\n",
        "print(\"1. 使用者提問\")\n",
        "print(\"2. 向量資料庫檢索相關文件\")\n",
        "print(\"3. 將文件作為上下文提供給 LLM\")\n",
        "print(\"4. LLM 基於上下文生成答案\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第3個儲存格：使用 LCEL 建立 RAG Chain\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "from langchain_core.output_parsers import StrOutputParser\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# 建立 ChatOllama 模型\n",
        "# 注意：模型已在第2個儲存格中建立，此處為保持儲存格獨立性而重新建立\n",
        "model = ChatOllama(model=\"llama3.2\")\n",
        "\n",
        "# 建立提示模板\n",
        "# 這個模板指導 LLM 如何使用上下文來回答問題\n",
        "template = '''\n",
        "僅根據以下上下文來回答問題：\n",
        "{context}\n",
        "\n",
        "問題：{question}\n",
        "'''\n",
        "prompt = ChatPromptTemplate.from_template(template)\n",
        "\n",
        "# 格式化文件函數\n",
        "# 將檢索到的文件列表轉換為單一字串\n",
        "def format_docs(docs):\n",
        "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
        "\n",
        "# 建立 RAG Chain\n",
        "# 這是一個使用 LCEL (LangChain Expression Language) 的標準 RAG 實作\n",
        "rag_chain = (\n",
        "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
        "    | prompt\n",
        "    | model\n",
        "    | StrOutputParser()\n",
        ")\n",
        "\n",
        "# 執行 RAG Chain\n",
        "print(\"\\n--- 使用 LCEL RAG Chain 執行 ---\")\n",
        "# 我們傳遞原始問題，Chain 會自動處理檢索、格式化和模型調用\n",
        "response = rag_chain.invoke(query)\n",
        "\n",
        "print(f\"\\n問題: {query}\")\n",
        "print(f\"\\nAI 回應:\\n{response}\")\n",
        "\n",
        "print(\"\\n--- LCEL RAG Chain 流程說明 ---\")\n",
        "print(\"1. RunnablePassthrough() 將使用者問題傳遞下去\")\n",
        "print(\"2. `retriever | format_docs` 檢索文件並格式化為字串\")\n",
        "print(\"3. `prompt` 將上下文和問題填入模板\")\n",
        "print(\"4. `model` 使用提示生成回應\")\n",
        "print(\"5. `StrOutputParser()` 提取回應內容\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}