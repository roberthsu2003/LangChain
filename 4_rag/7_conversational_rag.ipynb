{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# 對話式 RAG 系統\n",
        "\n",
        "本範例展示：\n",
        "1. **第1個儲存格**：載入向量資料庫並建立檢索器\n",
        "2. **第2個儲存格**：建立對話式 RAG Chain（含歷史記憶）\n",
        "\n",
        "學習目標：理解如何建立具有上下文記憶的多輪對話 RAG 系統"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第1個儲存格：載入向量資料庫並建立檢索器\n",
        "\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain_community.vectorstores import Chroma\n",
        "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
        "\n",
        "# 從 .env 載入環境變數\n",
        "load_dotenv()\n",
        "\n",
        "# 定義持久化目錄\n",
        "current_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
        "persistent_directory = os.path.join(current_dir, \"db\", \"chroma_db_with_metadata_chinese_nb\")\n",
        "\n",
        "# 定義嵌入模型\n",
        "embeddings = HuggingFaceEmbeddings(model_name=\"jinaai/jina-embeddings-v2-base-zh\")\n",
        "\n",
        "# 使用嵌入函數載入現有的向量存儲\n",
        "db = Chroma(persist_directory=persistent_directory, embedding_function=embeddings)\n",
        "\n",
        "# 建立用於查詢向量存儲的檢索器\n",
        "# search_type=\"similarity\": 使用相似度搜尋\n",
        "# k=3: 返回最相關的 3 個文件\n",
        "retriever = db.as_retriever(\n",
        "    search_type=\"similarity\",\n",
        "    search_kwargs={\"k\": 3},\n",
        ")\n",
        "\n",
        "print(\"向量資料庫和檢索器已準備就緒\")\n",
        "print(f\"資料庫位置: {persistent_directory}\")\n",
        "print(f\"檢索器設定: 相似度搜尋，返回 3 個文件\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第2個儲存格：建立對話式 RAG Chain（含歷史記憶）\n",
        "\n",
        "from langchain.chains import create_history_aware_retriever, create_retrieval_chain\n",
        "from langchain.chains.combine_documents import create_stuff_documents_chain\n",
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "from langchain_ollama import ChatOllama\n",
        "\n",
        "# 建立 ChatOllama 模型\n",
        "llm = ChatOllama(model=\"llama3.2\")\n",
        "\n",
        "# 情境化問題提示\n",
        "# 此提示幫助 AI 理解應根據聊天歷史重新表述問題，使其成為獨立的問題\n",
        "contextualize_q_system_prompt = (\n",
        "    \"給定聊天歷史和最新的使用者問題，\"\n",
        "    \"該問題可能引用聊天歷史中的上下文，\"\n",
        "    \"請制定一個獨立的問題，可以在沒有聊天歷史的情況下理解。\"\n",
        "    \"不要回答問題，只需在需要時重新表述，否則按原樣返回。\"\n",
        ")\n",
        "\n",
        "# 建立用於情境化問題的提示模板\n",
        "contextualize_q_prompt = ChatPromptTemplate.from_messages(\n",
        "    [ \n",
        "        (\"system\", contextualize_q_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 建立具有歷史意識的檢索器\n",
        "# 這使用 LLM 幫助根據聊天歷史重新表述問題\n",
        "history_aware_retriever = create_history_aware_retriever(\n",
        "    llm, retriever, contextualize_q_prompt\n",
        ")\n",
        "\n",
        "# 回答問題提示\n",
        "# 此提示幫助 AI 理解應根據檢索到的上下文提供簡潔的答案\n",
        "qa_system_prompt = (\n",
        "    \"你是一個問答任務的助手。使用\"\n",
        "    \"以下檢索到的上下文片段來回答\"\n",
        "    \"問題。如果你不知道答案，就說你\"\n",
        "    \"不知道。最多使用三個句子並保持答案\"\n",
        "    \"簡潔。\"\n",
        "    \"\\n\\n\"\n",
        "    \"{context}\"\n",
        ")\n",
        "\n",
        "# 建立用於回答問題的提示模板\n",
        "qa_prompt = ChatPromptTemplate.from_messages(\n",
        "    [ \n",
        "        (\"system\", qa_system_prompt),\n",
        "        MessagesPlaceholder(\"chat_history\"),\n",
        "        (\"human\", \"{input}\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "# 建立用於問答的文件組合鏈\n",
        "question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)\n",
        "\n",
        "# 建立結合具有歷史意識的檢索器和問答鏈的檢索鏈\n",
        "rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)\n",
        "\n",
        "print(\"對話式 RAG Chain 已建立完成\")\n",
        "print(\"\\n--- RAG Chain 架構 ---\")\n",
        "print(\"1. 歷史意識檢索器：根據聊天歷史重新表述問題\")\n",
        "print(\"2. 向量檢索：找出相關文件\")\n",
        "print(\"3. 問答鏈：基於文件和歷史生成答案\")\n",
        "\n",
        "# 模擬持續聊天的函數\n",
        "def continual_chat():\n",
        "    print(\"\\n開始與 AI 聊天！輸入 'exit' 結束對話。\")\n",
        "    chat_history = []  # 在此收集聊天歷史（訊息序列）\n",
        "    while True:\n",
        "        query = input(\"你：\")\n",
        "        if query.lower() == \"exit\":\n",
        "            break\n",
        "        # 透過檢索鏈處理使用者的查詢\n",
        "        result = rag_chain.invoke({\"input\": query, \"chat_history\": chat_history})\n",
        "        # 顯示 AI 的回應\n",
        "        print(f\"AI：{result['answer']}\")\n",
        "        # 更新聊天歷史\n",
        "        chat_history.append(HumanMessage(content=query))\n",
        "        chat_history.append(SystemMessage(content=result[\"answer\"]))\n",
        "\n",
        "# 啟動持續聊天\n",
        "continual_chat()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# 第3個儲存格：逐步執行對話式 RAG Chain\n",
        "\n",
        "# 由於第二個儲存格的 continual_chat() 函數會導致程式卡在輸入迴圈，\n",
        "# 我們在此建立一個新的儲存格，以手動、逐步的方式來執行對話，\n",
        "# 這樣可以更清楚地展示每一步的輸入和輸出。\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage\n",
        "\n",
        "# 初始化聊天歷史\n",
        "chat_history = []\n",
        "\n",
        "# --- 第 1 次對話 ---\n",
        "print(\"\\n--- 第一次對話 ---\")\n",
        "question1 = \"洗衣機的建議安裝地點在哪裡？\"\n",
        "print(f\"使用者問題: {question1}\")\n",
        "\n",
        "# 執行 RAG Chain\n",
        "# 我們傳遞問題和當前的聊天歷史（目前是空的）\n",
        "result1 = rag_chain.invoke({\"input\": question1, \"chat_history\": chat_history})\n",
        "\n",
        "print(f\"\\nAI 回應: {result1['answer']}\")\n",
        "\n",
        "# 將第一次對話的問與答加入聊天歷史\n",
        "chat_history.extend([HumanMessage(content=question1), AIMessage(content=result1[\"answer\"])])\n",
        "\n",
        "print(\"\\n--- 檢視第一次對話的上下文 ---\")\n",
        "# 顯示檢索到的文件，以了解 AI 回應的依據\n",
        "for i, doc in enumerate(result1['context'], 1):\n",
        "    print(f\"文件 {i}: {doc.page_content[:100]}...\")\n",
        "\n",
        "# --- 第 2 次對話 ---\n",
        "print(\"\\n\\n--- 第二次對話（有歷史紀錄） ---\")\n",
        "question2 = \"那裡有提到電源要注意什麼嗎？\"\n",
        "print(f\"使用者問題: {question2}\")\n",
        "\n",
        "# 再次執行 RAG Chain\n",
        "# 這次，我們傳遞了包含第一次對話的聊天歷史\n",
        "result2 = rag_chain.invoke({\"input\": question2, \"chat_history\": chat_history})\n",
        "\n",
        "print(f\"\\nAI 回應: {result2['answer']}\")\n",
        "\n",
        "# 更新聊天歷史\n",
        "chat_history.extend([HumanMessage(content=question2), AIMessage(content=result2[\"answer\"])])\n",
        "\n",
        "print(\"\\n--- 檢視第二次對話的上下文 ---\")\n",
        "# 觀察 `history_aware_retriever` 如何根據歷史重寫問題並找到相關文件\n",
        "for i, doc in enumerate(result2['context'], 1):\n",
        "    print(f\"文件 {i}: {doc.page_content[:100]}...\")\n",
        "\n",
        "print(\"\\n\\n--- 最終聊天歷史 ---\")\n",
        "print(chat_history)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}