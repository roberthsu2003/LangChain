{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 網頁爬取與 RAG 整合\n",
    "\n",
    "本範例展示：\n",
    "1. **第1個儲存格**：使用 WebBaseLoader 爬取網頁並建立向量資料庫\n",
    "2. **第2個儲存格**：查詢網頁內容並展示 RAG 應用\n",
    "\n",
    "學習目標：理解如何將網頁內容整合到 RAG 系統中，實現動態知識檢索"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第1個儲存格：使用 WebBaseLoader 爬取網頁並建立向量資料庫\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain.text_splitter import CharacterTextSplitter\nfrom langchain_community.document_loaders import WebBaseLoader\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\n\n# 從 .env 載入環境變數（如果有的話）\nload_dotenv()\n\n# 定義持久化目錄\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\ndb_dir = os.path.join(current_dir, \"db\")\npersistent_directory = os.path.join(db_dir, \"chroma_db_web_nb\")\n\nprint(f\"持久化目錄: {persistent_directory}\")\n\n# 檢查 Chroma 向量存儲是否已存在\nif not os.path.exists(persistent_directory):\n    print(\"持久化目錄不存在。正在初始化向量存儲...\")\n\n    # 步驟 1：使用 WebBaseLoader 爬取網頁內容\n    # WebBaseLoader 載入網頁並提取其內容\n    # 使用繁體中文網頁範例 - 維基百科台灣相關頁面\n    urls = [\n        \"https://zh.wikipedia.org/zh-tw/台灣\",\n        \"https://zh.wikipedia.org/zh-tw/臺北101\"\n    ]\n\n    print(\"\\n--- 正在爬取網頁內容 ---\")\n    print(f\"目標網址: {urls}\")\n    \n    # 建立網頁內容載入器\n    loader = WebBaseLoader(urls)\n    documents = loader.load()\n    \n    print(f\"\\n成功載入 {len(documents)} 個網頁\")\n    for i, doc in enumerate(documents, 1):\n        print(f\"  網頁 {i}: {doc.metadata.get('source', 'Unknown')}\")\n        print(f\"  內容長度: {len(doc.page_content)} 字元\")\n\n    # 步驟 2：將爬取的內容分割成塊\n    # CharacterTextSplitter 將文本分割成較小的塊\n    # chunk_size=1000: 每個文本區塊最多 1000 個字元\n    # chunk_overlap=200: 區塊之間重疊 200 個字元\n    print(\"\\n--- 正在分割文本 ---\")\n    text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=200)\n    docs = text_splitter.split_documents(documents)\n\n    # 顯示分割文件的資訊\n    print(f\"文件塊數量: {len(docs)}\")\n    print(f\"\\n範例塊 (前 200 字元):\")\n    print(f\"{docs[0].page_content[:200]}...\\n\")\n\n    # 步驟 3：為文件塊建立嵌入\n    # HuggingFaceEmbeddings 將文本轉換為捕捉語義意義的數值向量\n    print(\"--- 正在建立嵌入 ---\")\n    embeddings = HuggingFaceEmbeddings(\n        model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n    )\n    print(\"完成建立嵌入\")\n\n    # 步驟 4：使用嵌入建立並持久化向量存儲\n    # Chroma 儲存嵌入以進行高效搜尋\n    print(\"\\n--- 正在建立向量存儲 ---\")\n    db = Chroma.from_documents(\n        docs, embeddings, persist_directory=persistent_directory\n    )\n    print(f\"完成建立向量存儲於 {persistent_directory}\")\n\nelse:\n    print(\"向量存儲已存在。無需初始化。\")\n    print(\"如需重新爬取網頁，請刪除資料庫目錄後重新執行。\")\n\nprint(\"\\n✅ 第1個儲存格執行完成\")\nprint(\"\\n💡 提示：\")\nprint(\"- WebBaseLoader 可以爬取任何公開的網頁\")\nprint(\"- 建議選擇內容豐富、結構清晰的繁體中文網頁\")\nprint(\"- 注意遵守網站的 robots.txt 和使用條款\")\nprint(\"- 對於需要登入或動態載入的網頁，可考慮使用 Firecrawl 或 Playwright\")\nprint(\"\\n📌 範例網頁說明：\")\nprint(\"- 維基百科台灣頁面：介紹台灣的地理、歷史、文化等\")\nprint(\"- 臺北101頁面：介紹台灣著名地標建築\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# 第2個儲存格：查詢網頁內容並展示 RAG 應用\n\nimport os\nfrom dotenv import load_dotenv\nfrom langchain_community.vectorstores import Chroma\nfrom langchain_community.embeddings import HuggingFaceEmbeddings\nfrom langchain_core.messages import HumanMessage, SystemMessage\nfrom langchain_ollama import ChatOllama\n\n# 從 .env 載入環境變數\nload_dotenv()\n\n# 定義持久化目錄\ncurrent_dir = os.path.dirname(os.path.abspath(\"__file__\"))\ndb_dir = os.path.join(current_dir, \"db\")\npersistent_directory = os.path.join(db_dir, \"chroma_db_web_nb\")\n\n# 定義嵌入模型\nembeddings = HuggingFaceEmbeddings(\n    model_name=\"jinaai/jina-embeddings-v2-base-zh\"\n)\n\n# 使用嵌入函數載入現有的向量存儲\ndb = Chroma(\n    persist_directory=persistent_directory,\n    embedding_function=embeddings\n)\n\n# 步驟 1：定義使用者的問題（改為繁體中文問題）\nquery = \"台灣的地理位置在哪裡？\"\n\nprint(f\"\\n{'='*70}\")\nprint(f\"使用者問題: {query}\")\nprint(f\"{'='*70}\")\n\n# 步驟 2：根據查詢檢索相關文件\n# 建立用於查詢向量存儲的檢索器\n# search_type=\"similarity\": 使用相似度搜尋\n# k=3: 返回最相關的 3 個文件\nprint(\"\\n--- 正在檢索相關文件 ---\")\nretriever = db.as_retriever(\n    search_type=\"similarity\",\n    search_kwargs={\"k\": 3},\n)\nrelevant_docs = retriever.invoke(query)\n\n# 顯示檢索到的文件\nprint(f\"\\n找到 {len(relevant_docs)} 個相關文件:\\n\")\nfor i, doc in enumerate(relevant_docs, 1):\n    print(f\"--- 文件 {i} ---\")\n    print(f\"來源: {doc.metadata.get('source', 'Unknown')}\")\n    print(f\"內容預覽: {doc.page_content[:200]}...\\n\")\n\n# 步驟 3：整合檢索到的文件與 LLM\n# 這是 RAG 的核心：將檢索到的文件作為上下文提供給 LLM\nprint(\"--- 正在準備 RAG 提示 ---\")\ncombined_input = (\n    \"以下是一些可能有助於回答問題的網頁內容：\"\n    + query\n    + \"\\n\\n相關內容：\\n\"\n    + \"\\n\\n\".join([doc.page_content for doc in relevant_docs])\n    + \"\\n\\n請僅根據提供的網頁內容提供答案。如果在內容中找不到答案，請回覆『我不確定』。\"\n)\n\nprint(f\"\\n合併後的提示長度: {len(combined_input)} 字元\")\n\n# 步驟 4：使用 LLM 生成答案\n# 建立 ChatOllama 模型（使用本地 LLM）\nprint(\"\\n--- 正在生成答案 ---\")\nmodel = ChatOllama(model=\"llama3.2\")\n\n# 定義模型的訊息\n# SystemMessage: 設定 AI 的角色和行為\n# HumanMessage: 使用者的問題加上檢索到的上下文\nmessages = [\n    SystemMessage(content=\"你是一個有幫助的助手，專門回答關於台灣的問題。請用繁體中文回答。\"),\n    HumanMessage(content=combined_input),\n]\n\n# 使用合併的輸入調用模型\nresult = model.invoke(messages)\n\n# 顯示生成的回應\nprint(f\"\\n{'='*70}\")\nprint(\"AI 生成的回應:\")\nprint(f\"{'='*70}\")\nprint(result.content)\nprint(f\"{'='*70}\")\n\nprint(\"\\n--- RAG 流程總結 ---\")\nprint(\"1️⃣  使用 WebBaseLoader 爬取網頁內容\")\nprint(\"2️⃣  將內容分割並建立向量資料庫\")\nprint(\"3️⃣  使用者提問\")\nprint(\"4️⃣  向量檢索相關內容\")\nprint(\"5️⃣  將檢索結果作為上下文提供給 LLM\")\nprint(\"6️⃣  LLM 基於網頁內容生成答案\")\n\nprint(\"\\n💡 應用場景：\")\nprint(\"- 知識庫問答系統（如：台灣旅遊資訊）\")\nprint(\"- 新聞/部落格內容搜尋\")\nprint(\"- 產品說明查詢\")\nprint(\"- 教育資源整合\")\nprint(\"\\n📌 範例查詢建議：\")\nprint(\"- 台灣的地理位置在哪裡？\")\nprint(\"- 臺北101有多高？\")\nprint(\"- 台灣有哪些特色文化？\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 第3個儲存格：使用 LCEL 建立 RAG Chain\n",
    "\n",
    "from langchain_core.prompts import ChatPromptTemplate\n",
    "from langchain_core.runnables import RunnablePassthrough\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "from langchain_ollama import ChatOllama\n",
    "\n",
    "# 建立 ChatOllama 模型\n",
    "# 注意：模型已在第2個儲存格中建立，此處為保持儲存格獨立性而重新建立\n",
    "model = ChatOllama(model=\"llama3.2\")\n",
    "\n",
    "# 建立提示模板\n",
    "# 這個模板指導 LLM 如何使用從網頁爬取的上下文來回答問題\n",
    "template = '''\n",
    "僅根據以下從網頁爬取的上下文來回答問題：\n",
    "{context}\n",
    "\n",
    "問題：{question}\n",
    "'''\n",
    "prompt = ChatPromptTemplate.from_template(template)\n",
    "\n",
    "# 格式化文件函數\n",
    "# 將檢索到的文件列表轉換為單一字串\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# 建立 RAG Chain\n",
    "# 這是一個使用 LCEL (LangChain Expression Language) 的標準 RAG 實作\n",
    "# retriever 已在第 2 個儲存格中定義\n",
    "rag_chain = (\n",
    "    {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "    | prompt\n",
    "    | model\n",
    "    | StrOutputParser()\n",
    ")\n",
    "\n",
    "# 執行 RAG Chain\n",
    "print(\"\\n--- 使用 LCEL RAG Chain 執行 ---\")\n",
    "# 我們傳遞原始問題，Chain 會自動處理檢索、格式化和模型調用\n",
    "# query 已在第 2 個儲存格中定義\n",
    "response = rag_chain.invoke(query)\n",
    "\n",
    "print(f\"\\n問題: {query}\")\n",
    "print(f\"\\nAI 回應:\\n{response}\")\n",
    "\n",
    "print(\"\\n--- LCEL RAG Chain 流程說明 ---\")\n",
    "print(\"1. RunnablePassthrough() 將使用者問題傳遞下去\")\n",
    "print(\"2. `retriever | format_docs` 檢索文件並格式化為字串\")\n",
    "print(\"3. `prompt` 將上下文和問題填入模板\")\n",
    "print(\"4. `model` 使用提示生成回應\")\n",
    "print(\"5. `StrOutputParser()` 提取回應內容\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}