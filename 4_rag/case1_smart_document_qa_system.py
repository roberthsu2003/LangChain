"""
æ¡ˆä¾‹ 1: æ™ºæ…§æ–‡æª”å•ç­”ç³»çµ±
åŠŸèƒ½ï¼šåŸºæ–¼å¤šå€‹ä½¿ç”¨æ‰‹å†Šå»ºç«‹å•ç­”ç³»çµ±ï¼Œè‡ªå‹•æª¢ç´¢ç›¸é—œæ–‡æª”å›ç­”ä½¿ç”¨è€…å•é¡Œ
ä½¿ç”¨æŠ€è¡“ï¼šå‘é‡è³‡æ–™åº« (Chroma) + RAG Chain + Metadata éæ¿¾

ğŸ¤– AI è¼”åŠ©æç¤ºï¼š
ä½ å¯ä»¥ä½¿ç”¨ AI å”åŠ©å®Œæˆä»¥ä¸‹ä»»å‹™ï¼š

1. å»ºç«‹åŸºç¤æ¶æ§‹
   Prompt: "å¹«æˆ‘ä½¿ç”¨ langchain å’Œ gradio å»ºç«‹ä¸€å€‹æ™ºæ…§æ–‡æª”å•ç­”ç³»çµ±ï¼Œéœ€è¦åŒ…å«å‘é‡è³‡æ–™åº«åˆå§‹åŒ–ã€æ–‡æª”è¼‰å…¥ã€ä»¥åŠ Gradio ä»‹é¢"

2. æ–‡æª”è™•ç†èˆ‡åˆ†å‰²
   Prompt: "ç‚ºæ–‡æª”å•ç­”ç³»çµ±è¨­è¨ˆæ–‡æª”è™•ç†æµç¨‹ï¼ŒåŒ…å« TextLoader è¼‰å…¥å¤šå€‹æª”æ¡ˆã€CharacterTextSplitter åˆ†å‰²æ–‡æœ¬ã€ä»¥åŠæ·»åŠ  metadata"

3. å‘é‡è³‡æ–™åº«å»ºç«‹
   Prompt: "ä½¿ç”¨ Chroma å»ºç«‹å‘é‡è³‡æ–™åº«ï¼Œä¸¦ä½¿ç”¨ jina-embeddings-v2-base-zh ä½œç‚º embedding æ¨¡å‹ï¼Œæ”¯æ´æŒä¹…åŒ–å­˜å„²"

4. RAG Chain æ•´åˆ
   Prompt: "å»ºç«‹å®Œæ•´çš„ RAG Chainï¼ŒåŒ…å« retrieverã€prompt templateã€modelã€ä»¥åŠ output parserï¼Œä¸¦æ ¼å¼åŒ–è¼¸å‡ºçµæœ"

5. Gradio ä»‹é¢è¨­è¨ˆ
   Prompt: "è¨­è¨ˆå‹å–„çš„ Gradio ä»‹é¢ï¼ŒåŒ…å«æ–‡æª”é¸æ“‡å™¨ã€å•é¡Œè¼¸å…¥æ¡†ã€ç›¸ä¼¼åº¦èª¿æ•´æ»‘æ¡¿ï¼Œä»¥åŠé¡¯ç¤ºä¾†æºæ–‡æª”çš„åŠŸèƒ½"
"""

import gradio as gr
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnablePassthrough
from langchain_ollama.llms import OllamaLLM
import os
from pathlib import Path

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# å»ºç«‹æ¨¡å‹
model = OllamaLLM(model="llama3.2:latest")

# æ–‡æª”ç›®éŒ„
BOOKS_DIR = "books"

# å¯ç”¨çš„æ–‡æª”åˆ—è¡¨
AVAILABLE_DOCS = {
    "å…¨éƒ¨æ–‡æª”": [],
    "æ™ºæ…§å‹æ‰‹æ©Ÿä½¿ç”¨æ‰‹å†Š": "æ™ºæ…§å‹æ‰‹æ©Ÿä½¿ç”¨æ‰‹å†Š.txt",
    "æ´—è¡£æ©Ÿä½¿ç”¨èªªæ˜": "æ´—è¡£æ©Ÿä½¿ç”¨èªªæ˜.txt",
    "å†·æ°£æ©Ÿå®‰è£ç¶­è­·æ‰‹å†Š": "å†·æ°£æ©Ÿå®‰è£ç¶­è­·æ‰‹å†Š.txt",
    "è·¯ç”±å™¨è¨­å®šæ‰‹å†Š": "è·¯ç”±å™¨è¨­å®šæ‰‹å†Š.txt",
    "é›»å‹•æ©Ÿè»Šä½¿ç”¨æ‰‹å†Š": "é›»å‹•æ©Ÿè»Šä½¿ç”¨æ‰‹å†Š.txt",
    "ä¿¡ç”¨å¡æ¬Šç›Šèªªæ˜": "ä¿¡ç”¨å¡æ¬Šç›Šèªªæ˜.txt",
    "å¥ä¿å°±é†«æŒ‡å—": "å¥ä¿å°±é†«æŒ‡å—.txt",
    "ç§Ÿå±‹å¥‘ç´„ç¯„æœ¬èˆ‡èªªæ˜": "ç§Ÿå±‹å¥‘ç´„ç¯„æœ¬èˆ‡èªªæ˜.txt"
}

# ğŸ’¡ AI æç¤ºï¼šå„ªåŒ–æ–‡æª”è¼‰å…¥æµç¨‹
# Prompt: "ç‚ºæ–‡æª”è¼‰å…¥å™¨æ·»åŠ éŒ¯èª¤è™•ç†æ©Ÿåˆ¶ï¼Œç•¶æª”æ¡ˆä¸å­˜åœ¨æˆ–è®€å–å¤±æ•—æ™‚æä¾›å‹å–„çš„éŒ¯èª¤è¨Šæ¯"
def load_documents():
    """è¼‰å…¥æ‰€æœ‰æ–‡æª”ä¸¦å»ºç«‹å‘é‡è³‡æ–™åº«"""
    all_docs = []
    
    for doc_name, filename in AVAILABLE_DOCS.items():
        if doc_name == "å…¨éƒ¨æ–‡æª”":
            continue
            
        file_path = os.path.join(BOOKS_DIR, filename)
        if not os.path.exists(file_path):
            print(f"âš ï¸ æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
            continue
            
        try:
            loader = TextLoader(file_path, encoding='utf-8')
            documents = loader.load()
            
            # æ·»åŠ  metadata
            for doc in documents:
                doc.metadata["source_name"] = doc_name
                doc.metadata["filename"] = filename
                
            all_docs.extend(documents)
            print(f"âœ… æˆåŠŸè¼‰å…¥: {doc_name}")
        except Exception as e:
            print(f"âŒ è¼‰å…¥å¤±æ•— {doc_name}: {e}")
    
    return all_docs

# ğŸ’¡ AI æç¤ºï¼šèª¿æ•´åˆ†å‰²ç­–ç•¥
# Prompt: "æ¯”è¼ƒä¸åŒçš„ chunk_size (500, 1000, 1500) å’Œ chunk_overlap (50, 100, 200) å°æª¢ç´¢æ•ˆæœçš„å½±éŸ¿"
def create_vector_store():
    """å»ºç«‹æˆ–è¼‰å…¥å‘é‡è³‡æ–™åº«"""
    db_path = os.path.abspath("./db")
    
    # ä½¿ç”¨ HuggingFace çš„ä¸­æ–‡ embedding æ¨¡å‹
    embeddings = HuggingFaceEmbeddings(
        model_name='jinaai/jina-embeddings-v2-base-zh'
    )
    
    # æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å·²å­˜åœ¨
    if Path(db_path).exists():
        print("ğŸ“‚ è¼‰å…¥ç¾æœ‰å‘é‡è³‡æ–™åº«...")
        db = Chroma(persist_directory=db_path, embedding_function=embeddings)
        return db
    
    print("ğŸ”¨ å»ºç«‹æ–°çš„å‘é‡è³‡æ–™åº«...")
    
    # è¼‰å…¥æ‰€æœ‰æ–‡æª”
    documents = load_documents()
    
    if not documents:
        raise ValueError("æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æ–‡æª”")
    
    # åˆ†å‰²æ–‡æª”
    text_splitter = CharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        separator="\n"
    )
    docs = text_splitter.split_documents(documents)
    
    print(f"ğŸ“„ å…±åˆ†å‰²æˆ {len(docs)} å€‹æ–‡æª”å€å¡Š")
    
    # å»ºç«‹å‘é‡è³‡æ–™åº«
    db = Chroma.from_documents(
        docs,
        embeddings,
        persist_directory=db_path
    )
    
    print("âœ… å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆ")
    return db

# åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
try:
    vectorstore = create_vector_store()
except Exception as e:
    print(f"âŒ å‘é‡è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
    vectorstore = None

# RAG Prompt æ¨¡æ¿
# ğŸ’¡ AI æç¤ºï¼šå®¢è£½åŒ– Prompt
# Prompt: "ä¿®æ”¹ RAG promptï¼Œè®“å›ç­”æ›´åŠ è©³ç´°ï¼Œä¸¦è¦æ±‚ AI èªªæ˜è³‡è¨Šä¾†æºçš„å¯é æ€§"
rag_template = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯å°ˆæ¥­çš„æ–‡æª”åŠ©æ‰‹ï¼Œå°ˆé–€å›ç­”ä½¿ç”¨æ‰‹å†Šç›¸é—œå•é¡Œã€‚

è«‹æ ¹æ“šä»¥ä¸‹åƒè€ƒè³‡æ–™å›ç­”å•é¡Œï¼š
{context}

å›ç­”è¦æ±‚ï¼š
1. åŸºæ–¼æä¾›çš„åƒè€ƒè³‡æ–™å›ç­”
2. å›ç­”è¦æ¸…æ¥šã€å…·é«”ã€æ˜“æ‡‚
3. å¦‚æœåƒè€ƒè³‡æ–™ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºå‘ŠçŸ¥
4. ä½¿ç”¨ç¹é«”ä¸­æ–‡å›ç­”
5. å¯ä»¥é©ç•¶å¼•ç”¨åƒè€ƒè³‡æ–™ä¸­çš„å…§å®¹

"""),
    ("human", "å•é¡Œï¼š{question}")
])

# ğŸ’¡ AI æç¤ºï¼šå¼·åŒ–æ ¼å¼åŒ–åŠŸèƒ½
# Prompt: "ç‚ºæ ¼å¼åŒ–å‡½æ•¸æ·»åŠ ä¾†æºæ–‡æª”åç¨±é¡¯ç¤ºã€ç›¸é—œåº¦è©•åˆ†ã€ä»¥åŠå¼•ç”¨çš„åŸæ–‡ç‰‡æ®µ"
def format_docs(docs):
    """æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ–‡æª”"""
    if not docs:
        return "ç„¡ç›¸é—œè³‡æ–™"
    
    formatted = []
    for i, doc in enumerate(docs, 1):
        source = doc.metadata.get('source_name', 'æœªçŸ¥ä¾†æº')
        content = doc.page_content.strip()
        formatted.append(f"ã€åƒè€ƒè³‡æ–™ {i} - {source}ã€‘\n{content}")
    
    return "\n\n" + "\n\n".join(formatted)

# ğŸ’¡ AI æç¤ºï¼šåŠ å…¥é€²éšåŠŸèƒ½
# Prompt: "ç‚ºå•ç­”ç³»çµ±åŠ å…¥å°è©±æ­·å²è¨˜éŒ„åŠŸèƒ½ï¼Œä½¿ç”¨ ChatMessageHistory ä¿å­˜å¤šè¼ªå°è©±"
def answer_question(question, doc_filter, num_results, search_type):
    """å›ç­”ä½¿ç”¨è€…å•é¡Œ"""
    if not vectorstore:
        return "âŒ å‘é‡è³‡æ–™åº«æœªåˆå§‹åŒ–ï¼Œè«‹æª¢æŸ¥ç³»çµ±è¨­å®š"
    
    if not question.strip():
        return "âš ï¸ è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ"
    
    try:
        # è¨­å®šæª¢ç´¢å™¨
        search_kwargs = {"k": num_results}
        
        # å¦‚æœæœ‰æŒ‡å®šæ–‡æª”ï¼Œæ·»åŠ  metadata éæ¿¾
        if doc_filter != "å…¨éƒ¨æ–‡æª”":
            search_kwargs["filter"] = {"source_name": doc_filter}
        
        retriever = vectorstore.as_retriever(
            search_type=search_type,
            search_kwargs=search_kwargs
        )
        
        # å»ºç«‹ RAG Chain
        rag_chain = (
            {
                "context": retriever | format_docs,
                "question": RunnablePassthrough()
            }
            | rag_template
            | model
            | StrOutputParser()
        )
        
        # åŸ·è¡Œå•ç­”
        answer = rag_chain.invoke(question)
        
        # å–å¾—ä¾†æºæ–‡æª”
        source_docs = retriever.invoke(question)
        
        # æ ¼å¼åŒ–è¼¸å‡º
        output = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                        æ™ºæ…§æ–‡æª”å•ç­”ç³»çµ±                          â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ æ‚¨çš„å•é¡Œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{question}

ğŸ’¡ AI å›ç­”
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{answer}

ğŸ“š åƒè€ƒè³‡æ–™ä¾†æº
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
"""
        
        for i, doc in enumerate(source_docs, 1):
            source = doc.metadata.get('source_name', 'æœªçŸ¥ä¾†æº')
            content_preview = doc.page_content.strip()[:150] + "..."
            output += f"\n{i}. ã€{source}ã€‘\n   {content_preview}\n"
        
        output += """
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” æª¢ç´¢è¨­å®š
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ æœå°‹ç¯„åœï¼š{doc_filter}
â€¢ æª¢ç´¢ç­–ç•¥ï¼š{search_type_name}
â€¢ çµæœæ•¸é‡ï¼š{num_results} å€‹æ–‡æª”å€å¡Š

ğŸ’¡ æŠ€è¡“èªªæ˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… å‘é‡è³‡æ–™åº«ï¼šChroma
âœ… Embedding æ¨¡å‹ï¼šjina-embeddings-v2-base-zh
âœ… LLM æ¨¡å‹ï¼šLlama 3.2
âœ… æª¢ç´¢ç­–ç•¥ï¼š{search_type}
""".format(
            doc_filter=doc_filter,
            search_type_name="ç›¸ä¼¼åº¦æœå°‹" if search_type == "similarity" else "æœ€å¤§é‚Šéš›ç›¸é—œæ€§ (MMR)",
            num_results=num_results,
            search_type=search_type
        )
        
        return output.strip()
        
    except Exception as e:
        return f"âŒ ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"

# é è¨­ç¯„ä¾‹å•é¡Œ
examples = [
    ["å¦‚ä½•è¨­å®š WiFiï¼Ÿ", "è·¯ç”±å™¨è¨­å®šæ‰‹å†Š", 3, "similarity"],
    ["æ‰‹æ©Ÿå¦‚ä½•çœé›»ï¼Ÿ", "æ™ºæ…§å‹æ‰‹æ©Ÿä½¿ç”¨æ‰‹å†Š", 3, "similarity"],
    ["æ´—è¡£æ©Ÿå¦‚ä½•æ¸…æ½”ï¼Ÿ", "æ´—è¡£æ©Ÿä½¿ç”¨èªªæ˜", 3, "similarity"],
    ["å†·æ°£æ©Ÿå¦‚ä½•ä¿é¤Šï¼Ÿ", "å†·æ°£æ©Ÿå®‰è£ç¶­è­·æ‰‹å†Š", 3, "similarity"],
    ["ä¿¡ç”¨å¡æœ‰ä»€éº¼å„ªæƒ ï¼Ÿ", "ä¿¡ç”¨å¡æ¬Šç›Šèªªæ˜", 3, "similarity"],
]

# å»ºç«‹ Gradio ä»‹é¢
# ğŸ’¡ AI æç¤ºï¼šåŠ å…¥é€²éšåŠŸèƒ½
# Prompt: "ç‚ºä»‹é¢åŠ å…¥æ–‡æª”ä¸Šå‚³åŠŸèƒ½ï¼Œå…è¨±ä½¿ç”¨è€…ä¸Šå‚³æ–°çš„ PDF æˆ– TXT æª”æ¡ˆåˆ°å‘é‡è³‡æ–™åº«"
with gr.Blocks(title="æ™ºæ…§æ–‡æª”å•ç­”ç³»çµ±", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ“š æ™ºæ…§æ–‡æª”å•ç­”ç³»çµ±
    
    ## ç³»çµ±åŠŸèƒ½
    æœ¬ç³»çµ±ä½¿ç”¨ **RAG (Retrieval Augmented Generation)** æŠ€è¡“ï¼š
    - ğŸ” **æ™ºèƒ½æª¢ç´¢**ï¼šå¾ 8 æœ¬ä½¿ç”¨æ‰‹å†Šä¸­å¿«é€Ÿæ‰¾åˆ°ç›¸é—œè³‡è¨Š
    - ğŸ¯ **ç²¾æº–å›ç­”**ï¼šåŸºæ–¼å¯¦éš›æ–‡æª”å…§å®¹ç”Ÿæˆç­”æ¡ˆ
    - ğŸ“Š **ä¾†æºè¿½æº¯**ï¼šé¡¯ç¤ºç­”æ¡ˆçš„åƒè€ƒè³‡æ–™ä¾†æº
    - âš™ï¸ **å½ˆæ€§è¨­å®š**ï¼šå¯èª¿æ•´æª¢ç´¢ç¯„åœã€ç­–ç•¥å’Œçµæœæ•¸é‡
    
    ## å¯æŸ¥è©¢çš„æ–‡æª”
    æ™ºæ…§å‹æ‰‹æ©Ÿã€æ´—è¡£æ©Ÿã€å†·æ°£æ©Ÿã€è·¯ç”±å™¨ã€é›»å‹•æ©Ÿè»Šã€ä¿¡ç”¨å¡ã€å¥ä¿ã€ç§Ÿå±‹å¥‘ç´„
    
    ## ä½¿ç”¨èªªæ˜
    1. é¸æ“‡è¦æœå°‹çš„æ–‡æª”ç¯„åœï¼ˆæˆ–é¸æ“‡ã€Œå…¨éƒ¨æ–‡æª”ã€ï¼‰
    2. è¼¸å…¥æ‚¨çš„å•é¡Œ
    3. èª¿æ•´æª¢ç´¢è¨­å®šï¼ˆå¯é¸ï¼‰
    4. é»æ“Šã€ŒğŸ” æœå°‹ç­”æ¡ˆã€
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            question_input = gr.Textbox(
                label="ğŸ’¬ è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ",
                placeholder="ä¾‹å¦‚ï¼šå¦‚ä½•è¨­å®š WiFiï¼Ÿ",
                lines=3
            )
            
            doc_filter = gr.Dropdown(
                label="ğŸ“ é¸æ“‡æ–‡æª”ç¯„åœ",
                choices=list(AVAILABLE_DOCS.keys()),
                value="å…¨éƒ¨æ–‡æª”"
            )
            
            with gr.Accordion("âš™ï¸ é€²éšè¨­å®š", open=False):
                num_results = gr.Slider(
                    label="æª¢ç´¢çµæœæ•¸é‡",
                    minimum=1,
                    maximum=10,
                    value=3,
                    step=1
                )
                
                search_type = gr.Radio(
                    label="æª¢ç´¢ç­–ç•¥",
                    choices=[
                        ("ç›¸ä¼¼åº¦æœå°‹ (Similarity)", "similarity"),
                        ("æœ€å¤§é‚Šéš›ç›¸é—œæ€§ (MMR)", "mmr")
                    ],
                    value="similarity"
                )
            
            submit_btn = gr.Button("ğŸ” æœå°‹ç­”æ¡ˆ", variant="primary", size="lg")
            
            gr.Examples(
                examples=examples,
                inputs=[question_input, doc_filter, num_results, search_type],
                label="ğŸ“‹ ç¯„ä¾‹å•é¡Œï¼ˆé»æ“Šä½¿ç”¨ï¼‰"
            )
        
        with gr.Column(scale=1):
            answer_output = gr.Textbox(
                label="ğŸ“– å›ç­”çµæœ",
                lines=25,
                show_copy_button=True
            )
    
    submit_btn.click(
        fn=answer_question,
        inputs=[question_input, doc_filter, num_results, search_type],
        outputs=answer_output
    )
    
    question_input.submit(
        fn=answer_question,
        inputs=[question_input, doc_filter, num_results, search_type],
        outputs=answer_output
    )
    
    gr.Markdown("""
    ---
    ### ğŸ’¡ æŠ€è¡“èªªæ˜
    
    **ä½¿ç”¨çš„ RAG æŠ€è¡“**ï¼š
    1. **å‘é‡è³‡æ–™åº« (Chroma)**
       - å„²å­˜æ‰€æœ‰æ–‡æª”çš„å‘é‡è¡¨ç¤º
       - æ”¯æ´é«˜æ•ˆçš„ç›¸ä¼¼åº¦æœå°‹
    
    2. **Embedding æ¨¡å‹ (Jina v2 Base ZH)**
       - å°ˆç‚ºä¸­æ–‡å„ªåŒ–çš„åµŒå…¥æ¨¡å‹
       - æ”¯æ´æœ€é•· 8192 tokens
    
    3. **æª¢ç´¢ç­–ç•¥**
       - **Similarity**: åŸºæ–¼é¤˜å¼¦ç›¸ä¼¼åº¦çš„æª¢ç´¢
       - **MMR**: æœ€å¤§é‚Šéš›ç›¸é—œæ€§ï¼Œå¢åŠ çµæœå¤šæ¨£æ€§
    
    4. **RAG Chain**
       - Retriever â†’ Prompt Template â†’ LLM â†’ Answer
    
    **å¯¦éš›æ‡‰ç”¨å ´æ™¯**ï¼š
    - ä¼æ¥­çŸ¥è­˜åº«å•ç­”
    - å®¢æˆ¶æœå‹™è‡ªå‹•åŒ–
    - æŠ€è¡“æ–‡æª”æŸ¥è©¢
    - æ³•è¦æ”¿ç­–è«®è©¢
    """)

if __name__ == "__main__":
    demo.launch(share=False, server_name="0.0.0.0", server_port=7860)

