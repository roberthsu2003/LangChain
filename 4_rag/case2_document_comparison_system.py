"""
æ¡ˆä¾‹ 2: å¤šæ–‡æª”æ™ºèƒ½æ¯”è¼ƒåˆ†æç³»çµ±
åŠŸèƒ½ï¼šåŒæ™‚æª¢ç´¢å¤šå€‹æ–‡æª”ï¼Œæ¯”è¼ƒä¸åŒç”¢å“/æœå‹™çš„ç‰¹é»èˆ‡å·®ç•°
ä½¿ç”¨æŠ€è¡“ï¼šå¤šæª¢ç´¢å™¨ (Multiple Retrievers) + ä¸¦è¡Œéˆ (RunnableParallel) + RAG

ğŸ¤– AI è¼”åŠ©æç¤ºï¼š
ä½ å¯ä»¥ä½¿ç”¨ AI å”åŠ©å®Œæˆä»¥ä¸‹ä»»å‹™ï¼š

1. å»ºç«‹åŸºç¤æ¶æ§‹
   Prompt: "å¹«æˆ‘å»ºç«‹ä¸€å€‹å¤šæ–‡æª”æ¯”è¼ƒç³»çµ±ï¼Œä½¿ç”¨ langchainã€gradio å’Œ Chromaï¼Œéœ€è¦èƒ½å¤ åŒæ™‚æª¢ç´¢å¤šå€‹æ–‡æª”ä¸¦é€²è¡Œæ¯”è¼ƒ"

2. å¤šæª¢ç´¢å™¨è¨­è¨ˆ
   Prompt: "è¨­è¨ˆå¤šæª¢ç´¢å™¨æ¶æ§‹ï¼Œç‚ºæ¯å€‹æ–‡æª”é¡åˆ¥å»ºç«‹ç¨ç«‹çš„ retrieverï¼Œä¸¦ä½¿ç”¨ metadata éæ¿¾"

3. ä¸¦è¡Œæª¢ç´¢å¯¦ä½œ
   Prompt: "ä½¿ç”¨ RunnableParallel å»ºç«‹ä¸¦è¡Œæª¢ç´¢éˆï¼ŒåŒæ™‚å¾ 2-3 å€‹æ–‡æª”ä¸­æª¢ç´¢ç›¸é—œå…§å®¹"

4. æ¯”è¼ƒ Prompt è¨­è¨ˆ
   Prompt: "è¨­è¨ˆå°ˆé–€çš„æ¯”è¼ƒ promptï¼Œè®“ AI èƒ½å¤ åˆ†æå¤šå€‹ç”¢å“/æœå‹™çš„ç•°åŒé»ï¼Œä¸¦ä»¥è¡¨æ ¼æˆ–åˆ—è¡¨æ–¹å¼å‘ˆç¾"

5. è¦–è¦ºåŒ–è¼¸å‡º
   Prompt: "è¨­è¨ˆæ¸…æ™°çš„æ¯”è¼ƒå ±å‘Šæ ¼å¼ï¼ŒåŒ…å«ç›¸ä¼¼é»ã€å·®ç•°é»ã€å„ªç¼ºé»åˆ†æï¼Œä»¥åŠæ¨è–¦å»ºè­°"
"""

import gradio as gr
from dotenv import load_dotenv
from langchain_community.document_loaders import TextLoader
from langchain_text_splitters import CharacterTextSplitter
from langchain_community.vectorstores import Chroma
from langchain_community.embeddings import HuggingFaceEmbeddings
from langchain.prompts import ChatPromptTemplate
from langchain.schema.output_parser import StrOutputParser
from langchain.schema.runnable import RunnableParallel, RunnablePassthrough, RunnableLambda
from langchain_ollama.llms import OllamaLLM
import os
from pathlib import Path

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# å»ºç«‹æ¨¡å‹
model = OllamaLLM(model="llama3.2:latest")

# æ–‡æª”ç›®éŒ„
BOOKS_DIR = "books"

# å¯æ¯”è¼ƒçš„æ–‡æª”çµ„åˆ
COMPARISON_GROUPS = {
    "å®¶é›»ç”¢å“": {
        "æ™ºæ…§å‹æ‰‹æ©Ÿä½¿ç”¨æ‰‹å†Š": "æ™ºæ…§å‹æ‰‹æ©Ÿä½¿ç”¨æ‰‹å†Š.txt",
        "æ´—è¡£æ©Ÿä½¿ç”¨èªªæ˜": "æ´—è¡£æ©Ÿä½¿ç”¨èªªæ˜.txt",
        "å†·æ°£æ©Ÿå®‰è£ç¶­è­·æ‰‹å†Š": "å†·æ°£æ©Ÿå®‰è£ç¶­è­·æ‰‹å†Š.txt"
    },
    "ç¶²è·¯è¨­å‚™": {
        "è·¯ç”±å™¨è¨­å®šæ‰‹å†Š": "è·¯ç”±å™¨è¨­å®šæ‰‹å†Š.txt",
        "æ™ºæ…§å‹æ‰‹æ©Ÿä½¿ç”¨æ‰‹å†Š": "æ™ºæ…§å‹æ‰‹æ©Ÿä½¿ç”¨æ‰‹å†Š.txt"
    },
    "é‡‘èæœå‹™": {
        "ä¿¡ç”¨å¡æ¬Šç›Šèªªæ˜": "ä¿¡ç”¨å¡æ¬Šç›Šèªªæ˜.txt"
    },
    "ç”Ÿæ´»æœå‹™": {
        "å¥ä¿å°±é†«æŒ‡å—": "å¥ä¿å°±é†«æŒ‡å—.txt",
        "ç§Ÿå±‹å¥‘ç´„ç¯„æœ¬èˆ‡èªªæ˜": "ç§Ÿå±‹å¥‘ç´„ç¯„æœ¬èˆ‡èªªæ˜.txt"
    }
}

# ğŸ’¡ AI æç¤ºï¼šå„ªåŒ–æ–‡æª”è¼‰å…¥
# Prompt: "ç‚ºæ–‡æª”è¼‰å…¥æ·»åŠ å¿«å–æ©Ÿåˆ¶ï¼Œé¿å…é‡è¤‡è¼‰å…¥ç›¸åŒçš„æ–‡æª”"
def load_and_split_document(filename, doc_name):
    """è¼‰å…¥ä¸¦åˆ†å‰²å–®ä¸€æ–‡æª”"""
    file_path = os.path.join(BOOKS_DIR, filename)
    
    if not os.path.exists(file_path):
        return []
    
    try:
        loader = TextLoader(file_path, encoding='utf-8')
        documents = loader.load()
        
        # æ·»åŠ  metadata
        for doc in documents:
            doc.metadata["source_name"] = doc_name
            doc.metadata["filename"] = filename
        
        # åˆ†å‰²æ–‡æª”
        text_splitter = CharacterTextSplitter(
            chunk_size=1000,
            chunk_overlap=200,
            separator="\n"
        )
        docs = text_splitter.split_documents(documents)
        
        return docs
    except Exception as e:
        print(f"âŒ è¼‰å…¥å¤±æ•— {doc_name}: {e}")
        return []

# ğŸ’¡ AI æç¤ºï¼šå„ªåŒ–å‘é‡è³‡æ–™åº«
# Prompt: "ç‚ºæ¯å€‹æ–‡æª”é¡åˆ¥å»ºç«‹ç¨ç«‹çš„ collectionï¼Œé¿å…æª¢ç´¢æ™‚çš„ç›¸äº’å¹²æ“¾"
def get_or_create_vectorstore():
    """å–å¾—æˆ–å»ºç«‹å‘é‡è³‡æ–™åº«"""
    db_path = os.path.abspath("./db")
    
    embeddings = HuggingFaceEmbeddings(
        model_name='jinaai/jina-embeddings-v2-base-zh'
    )
    
    # å¦‚æœè³‡æ–™åº«å·²å­˜åœ¨ï¼Œç›´æ¥è¼‰å…¥
    if Path(db_path).exists():
        print("ğŸ“‚ è¼‰å…¥ç¾æœ‰å‘é‡è³‡æ–™åº«...")
        return Chroma(persist_directory=db_path, embedding_function=embeddings)
    
    print("ğŸ”¨ å»ºç«‹æ–°çš„å‘é‡è³‡æ–™åº«...")
    
    # è¼‰å…¥æ‰€æœ‰æ–‡æª”
    all_docs = []
    for group_name, docs in COMPARISON_GROUPS.items():
        for doc_name, filename in docs.items():
            docs_chunks = load_and_split_document(filename, doc_name)
            all_docs.extend(docs_chunks)
            print(f"âœ… è¼‰å…¥ {doc_name}: {len(docs_chunks)} å€‹å€å¡Š")
    
    if not all_docs:
        raise ValueError("æ²’æœ‰æˆåŠŸè¼‰å…¥ä»»ä½•æ–‡æª”")
    
    # å»ºç«‹å‘é‡è³‡æ–™åº«
    db = Chroma.from_documents(
        all_docs,
        embeddings,
        persist_directory=db_path
    )
    
    print(f"âœ… å‘é‡è³‡æ–™åº«å»ºç«‹å®Œæˆï¼Œå…± {len(all_docs)} å€‹æ–‡æª”å€å¡Š")
    return db

# åˆå§‹åŒ–å‘é‡è³‡æ–™åº«
try:
    vectorstore = get_or_create_vectorstore()
except Exception as e:
    print(f"âŒ å‘é‡è³‡æ–™åº«åˆå§‹åŒ–å¤±æ•—: {e}")
    vectorstore = None

# ğŸ’¡ AI æç¤ºï¼šå„ªåŒ–æª¢ç´¢å‡½æ•¸
# Prompt: "ç‚ºæª¢ç´¢å™¨æ·»åŠ ç›¸é—œåº¦éæ¿¾ï¼Œåªè¿”å›ç›¸ä¼¼åº¦è¶…éé–¾å€¼çš„çµæœ"
def create_retriever_for_doc(doc_name, k=3):
    """ç‚ºç‰¹å®šæ–‡æª”å»ºç«‹æª¢ç´¢å™¨"""
    if not vectorstore:
        return None
    
    return vectorstore.as_retriever(
        search_type="similarity",
        search_kwargs={
            "k": k,
            "filter": {"source_name": doc_name}
        }
    )

def format_docs(docs):
    """æ ¼å¼åŒ–æ–‡æª”å…§å®¹"""
    if not docs:
        return "ç„¡ç›¸é—œè³‡æ–™"
    
    contents = []
    for doc in docs:
        contents.append(doc.page_content.strip())
    
    return "\n\n".join(contents)

# æ¯”è¼ƒåˆ†æ Prompt æ¨¡æ¿
# ğŸ’¡ AI æç¤ºï¼šå®¢è£½åŒ–æ¯”è¼ƒ Prompt
# Prompt: "è¨­è¨ˆä¸åŒé¡å‹çš„æ¯”è¼ƒ promptï¼šåŠŸèƒ½æ¯”è¼ƒã€åƒ¹æ ¼æ¯”è¼ƒã€å„ªç¼ºé»æ¯”è¼ƒã€é©ç”¨å ´æ™¯æ¯”è¼ƒ"
comparison_template = ChatPromptTemplate.from_messages([
    ("system", """ä½ æ˜¯å°ˆæ¥­çš„ç”¢å“åˆ†æå¸«ï¼Œæ“…é•·æ¯”è¼ƒåˆ†æä¸åŒç”¢å“æˆ–æœå‹™ã€‚

ä½ å°‡æ”¶åˆ°å¤šå€‹ç”¢å“/æœå‹™çš„ç›¸é—œè³‡æ–™ï¼Œè«‹é€²è¡Œè©³ç´°çš„æ¯”è¼ƒåˆ†æã€‚

{doc1_name} çš„ç›¸é—œè³‡æ–™ï¼š
{doc1_context}

{doc2_name} çš„ç›¸é—œè³‡æ–™ï¼š
{doc2_context}

{doc3_section}

è«‹æ ¹æ“šä½¿ç”¨è€…çš„å•é¡Œé€²è¡Œæ¯”è¼ƒåˆ†æï¼š
1. åˆ—å‡ºç›¸ä¼¼é»
2. åˆ—å‡ºå·®ç•°é»
3. åˆ†æå„è‡ªçš„å„ªç¼ºé»
4. æä¾›é¸æ“‡å»ºè­°

å›ç­”è¦æ±‚ï¼š
- ä½¿ç”¨ç¹é«”ä¸­æ–‡
- æ¢ç†æ¸…æ™°ï¼Œä½¿ç”¨åˆ—è¡¨æˆ–è¡¨æ ¼æ–¹å¼å‘ˆç¾
- åŸºæ–¼æä¾›çš„è³‡æ–™é€²è¡Œåˆ†æ
- å¦‚æœæŸå€‹ç”¢å“ç¼ºå°‘ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜
"""),
    ("human", "æ¯”è¼ƒå•é¡Œï¼š{question}")
])

# ğŸ’¡ AI æç¤ºï¼šåŠ å…¥è¦–è¦ºåŒ–åŠŸèƒ½
# Prompt: "ç‚ºæ¯”è¼ƒå ±å‘ŠåŠ å…¥åœ–è¡¨è¦–è¦ºåŒ–ï¼Œä½¿ç”¨ matplotlib æˆ– plotly å‘ˆç¾æ¯”è¼ƒçµæœ"
def compare_documents(question, doc1_name, doc2_name, doc3_name=None, num_results=3):
    """æ¯”è¼ƒå¤šå€‹æ–‡æª”"""
    if not vectorstore:
        return "âŒ å‘é‡è³‡æ–™åº«æœªåˆå§‹åŒ–"
    
    if not question.strip():
        return "âš ï¸ è«‹è¼¸å…¥æ¯”è¼ƒå•é¡Œ"
    
    if not doc1_name or not doc2_name:
        return "âš ï¸ è«‹è‡³å°‘é¸æ“‡å…©å€‹æ–‡æª”é€²è¡Œæ¯”è¼ƒ"
    
    try:
        # å»ºç«‹æª¢ç´¢å™¨
        retriever1 = create_retriever_for_doc(doc1_name, num_results)
        retriever2 = create_retriever_for_doc(doc2_name, num_results)
        
        if not retriever1 or not retriever2:
            return "âŒ æª¢ç´¢å™¨å»ºç«‹å¤±æ•—"
        
        # æº–å‚™ä¸¦è¡Œæª¢ç´¢
        retrieval_dict = {
            "doc1_name": RunnablePassthrough(),
            "doc2_name": RunnablePassthrough(),
            "question": RunnablePassthrough(),
            "doc1_context": retriever1 | RunnableLambda(format_docs),
            "doc2_context": retriever2 | RunnableLambda(format_docs),
        }
        
        # è™•ç†ç¬¬ä¸‰å€‹æ–‡æª”ï¼ˆå¯é¸ï¼‰
        doc3_section = ""
        if doc3_name and doc3_name != "ä¸é¸æ“‡":
            retriever3 = create_retriever_for_doc(doc3_name, num_results)
            if retriever3:
                retrieval_dict["doc3_name"] = RunnablePassthrough()
                retrieval_dict["doc3_context"] = retriever3 | RunnableLambda(format_docs)
                doc3_section = "{doc3_name} çš„ç›¸é—œè³‡æ–™ï¼š\n{doc3_context}\n"
        
        retrieval_dict["doc3_section"] = RunnablePassthrough()
        
        # å»ºç«‹ä¸¦è¡Œæª¢ç´¢éˆ
        parallel_retrieval = RunnableParallel(retrieval_dict)
        
        # å»ºç«‹å®Œæ•´çš„æ¯”è¼ƒéˆ
        comparison_chain = (
            parallel_retrieval
            | comparison_template
            | model
            | StrOutputParser()
        )
        
        # åŸ·è¡Œæ¯”è¼ƒ
        input_data = {
            "question": question,
            "doc1_name": doc1_name,
            "doc2_name": doc2_name,
            "doc3_section": doc3_section
        }
        
        if doc3_name and doc3_name != "ä¸é¸æ“‡":
            input_data["doc3_name"] = doc3_name
        
        result = comparison_chain.invoke(input_data)
        
        # æ ¼å¼åŒ–è¼¸å‡º
        output = f"""
â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—
â•‘                    å¤šæ–‡æª”æ™ºèƒ½æ¯”è¼ƒåˆ†æç³»çµ±                        â•‘
â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•

ğŸ“‹ æ¯”è¼ƒå•é¡Œ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{question}

ğŸ“Š æ¯”è¼ƒå°è±¡
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
1. {doc1_name}
2. {doc2_name}
{doc3_info}

ğŸ’¡ æ¯”è¼ƒåˆ†æçµæœ
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
{result}

â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
ğŸ” æŠ€è¡“èªªæ˜
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
âœ… ä½¿ç”¨ä¸¦è¡Œæª¢ç´¢ (RunnableParallel) åŒæ™‚æŸ¥è©¢å¤šå€‹æ–‡æª”
âœ… åŸºæ–¼å‘é‡ç›¸ä¼¼åº¦æ‰¾å‡ºæœ€ç›¸é—œçš„è³‡è¨Š
âœ… ä½¿ç”¨å°ˆé–€çš„æ¯”è¼ƒ Prompt é€²è¡Œæ·±åº¦åˆ†æ
âœ… æ¯å€‹æ–‡æª”æª¢ç´¢ {num_results} å€‹æœ€ç›¸é—œå€å¡Š

ğŸ’¡ æ‡‰ç”¨å ´æ™¯
â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”
â€¢ ç”¢å“åŠŸèƒ½æ¯”è¼ƒ
â€¢ æœå‹™æ–¹æ¡ˆé¸æ“‡
â€¢ æ”¿ç­–è¦ç¯„å°ç…§
â€¢ æŠ€è¡“è¦æ ¼åˆ†æ
""".format(
            question=question,
            doc1_name=doc1_name,
            doc2_name=doc2_name,
            doc3_info=f"3. {doc3_name}" if doc3_name and doc3_name != "ä¸é¸æ“‡" else "",
            result=result,
            num_results=num_results
        )
        
        return output.strip()
        
    except Exception as e:
        return f"âŒ æ¯”è¼ƒåˆ†æéŒ¯èª¤ï¼š{str(e)}"

# å–å¾—æ‰€æœ‰å¯é¸æ“‡çš„æ–‡æª”
all_docs = []
for group in COMPARISON_GROUPS.values():
    all_docs.extend(group.keys())
all_docs = sorted(list(set(all_docs)))

# é è¨­ç¯„ä¾‹
examples = [
    ["å¦‚ä½•è¨­å®šç¶²è·¯é€£ç·šï¼Ÿ", "è·¯ç”±å™¨è¨­å®šæ‰‹å†Š", "æ™ºæ…§å‹æ‰‹æ©Ÿä½¿ç”¨æ‰‹å†Š", "ä¸é¸æ“‡", 3],
    ["æ¸…æ½”å’Œä¿é¤Šçš„æ–¹å¼æœ‰ä»€éº¼ä¸åŒï¼Ÿ", "æ´—è¡£æ©Ÿä½¿ç”¨èªªæ˜", "å†·æ°£æ©Ÿå®‰è£ç¶­è­·æ‰‹å†Š", "ä¸é¸æ“‡", 3],
    ["æœ‰å“ªäº›ä¿å›ºæ¢æ¬¾ï¼Ÿ", "æ™ºæ…§å‹æ‰‹æ©Ÿä½¿ç”¨æ‰‹å†Š", "æ´—è¡£æ©Ÿä½¿ç”¨èªªæ˜", "å†·æ°£æ©Ÿå®‰è£ç¶­è­·æ‰‹å†Š", 3],
]

# å»ºç«‹ Gradio ä»‹é¢
# ğŸ’¡ AI æç¤ºï¼šåŠ å…¥é€²éšåŠŸèƒ½
# Prompt: "ç‚ºä»‹é¢åŠ å…¥æ¯”è¼ƒçµæœåŒ¯å‡ºåŠŸèƒ½ï¼ˆPDFã€Wordï¼‰ï¼Œä»¥åŠæ‰¹æ¬¡æ¯”è¼ƒå¤šå€‹å•é¡Œçš„åŠŸèƒ½"
with gr.Blocks(title="å¤šæ–‡æª”æ™ºèƒ½æ¯”è¼ƒç³»çµ±", theme=gr.themes.Soft()) as demo:
    gr.Markdown("""
    # ğŸ” å¤šæ–‡æª”æ™ºèƒ½æ¯”è¼ƒåˆ†æç³»çµ±
    
    ## ç³»çµ±åŠŸèƒ½
    æœ¬ç³»çµ±çµåˆ **RAG** å’Œ **ä¸¦è¡Œæª¢ç´¢** æŠ€è¡“ï¼š
    - ğŸ”„ **ä¸¦è¡Œæª¢ç´¢**ï¼šåŒæ™‚å¾ 2-3 å€‹æ–‡æª”ä¸­æª¢ç´¢ç›¸é—œè³‡è¨Š
    - ğŸ“Š **æ™ºèƒ½æ¯”è¼ƒ**ï¼šè‡ªå‹•åˆ†æå¤šå€‹ç”¢å“/æœå‹™çš„ç•°åŒ
    - ğŸ¯ **æ·±åº¦åˆ†æ**ï¼šæä¾›å„ªç¼ºé»åˆ†æå’Œé¸æ“‡å»ºè­°
    - ğŸ“ **æ¸…æ™°å‘ˆç¾**ï¼šä»¥çµæ§‹åŒ–æ–¹å¼å±•ç¤ºæ¯”è¼ƒçµæœ
    
    ## æ¯”è¼ƒé¡å‹
    - **å®¶é›»ç”¢å“**ï¼šæ‰‹æ©Ÿã€æ´—è¡£æ©Ÿã€å†·æ°£æ©Ÿç­‰
    - **ç¶²è·¯è¨­å‚™**ï¼šè·¯ç”±å™¨ã€æ™ºæ…§å‹æ‰‹æ©Ÿé€£ç·šåŠŸèƒ½
    - **é‡‘èæœå‹™**ï¼šä¿¡ç”¨å¡æ¬Šç›Š
    - **ç”Ÿæ´»æœå‹™**ï¼šå¥ä¿ã€ç§Ÿå±‹å¥‘ç´„
    
    ## ä½¿ç”¨èªªæ˜
    1. é¸æ“‡ 2-3 å€‹è¦æ¯”è¼ƒçš„æ–‡æª”
    2. è¼¸å…¥æ¯”è¼ƒå•é¡Œ
    3. èª¿æ•´æª¢ç´¢æ•¸é‡ï¼ˆå¯é¸ï¼‰
    4. é»æ“Šã€ŒğŸ” é–‹å§‹æ¯”è¼ƒåˆ†æã€
    """)
    
    with gr.Row():
        with gr.Column(scale=1):
            question_input = gr.Textbox(
                label="ğŸ’¬ è«‹è¼¸å…¥æ¯”è¼ƒå•é¡Œ",
                placeholder="ä¾‹å¦‚ï¼šä¿å›ºæœŸé™æœ‰ä»€éº¼å·®ç•°ï¼Ÿ",
                lines=3
            )
            
            doc1_select = gr.Dropdown(
                label="ğŸ“„ é¸æ“‡ç¬¬ä¸€å€‹æ–‡æª”",
                choices=all_docs,
                value=all_docs[0] if all_docs else None
            )
            
            doc2_select = gr.Dropdown(
                label="ğŸ“„ é¸æ“‡ç¬¬äºŒå€‹æ–‡æª”",
                choices=all_docs,
                value=all_docs[1] if len(all_docs) > 1 else None
            )
            
            doc3_select = gr.Dropdown(
                label="ğŸ“„ é¸æ“‡ç¬¬ä¸‰å€‹æ–‡æª”ï¼ˆå¯é¸ï¼‰",
                choices=["ä¸é¸æ“‡"] + all_docs,
                value="ä¸é¸æ“‡"
            )
            
            with gr.Accordion("âš™ï¸ é€²éšè¨­å®š", open=False):
                num_results = gr.Slider(
                    label="æ¯å€‹æ–‡æª”çš„æª¢ç´¢æ•¸é‡",
                    minimum=1,
                    maximum=10,
                    value=3,
                    step=1,
                    info="æ¯å€‹æ–‡æª”æª¢ç´¢çš„æ–‡æœ¬å€å¡Šæ•¸é‡"
                )
            
            compare_btn = gr.Button("ğŸ” é–‹å§‹æ¯”è¼ƒåˆ†æ", variant="primary", size="lg")
            
            gr.Examples(
                examples=examples,
                inputs=[question_input, doc1_select, doc2_select, doc3_select, num_results],
                label="ğŸ“‹ ç¯„ä¾‹æ¯”è¼ƒï¼ˆé»æ“Šä½¿ç”¨ï¼‰"
            )
        
        with gr.Column(scale=1):
            comparison_output = gr.Textbox(
                label="ğŸ“Š æ¯”è¼ƒåˆ†æçµæœ",
                lines=30,
                show_copy_button=True
            )
    
    compare_btn.click(
        fn=compare_documents,
        inputs=[question_input, doc1_select, doc2_select, doc3_select, num_results],
        outputs=comparison_output
    )
    
    question_input.submit(
        fn=compare_documents,
        inputs=[question_input, doc1_select, doc2_select, doc3_select, num_results],
        outputs=comparison_output
    )
    
    gr.Markdown("""
    ---
    ### ğŸ’¡ æŠ€è¡“èªªæ˜
    
    **æ ¸å¿ƒæŠ€è¡“æ¶æ§‹**ï¼š
    
    1. **ä¸¦è¡Œæª¢ç´¢ (RunnableParallel)**
       ```python
       parallel_retrieval = RunnableParallel(
           doc1_context=retriever1,
           doc2_context=retriever2,
           doc3_context=retriever3
       )
       ```
       - åŒæ™‚æŸ¥è©¢å¤šå€‹æ–‡æª”ï¼Œæå‡æ•ˆç‡
       - æ¯å€‹æª¢ç´¢å™¨ä½¿ç”¨ metadata éæ¿¾ç‰¹å®šæ–‡æª”
    
    2. **æ¯”è¼ƒ Chain æ¶æ§‹**
       ```
       ä¸¦è¡Œæª¢ç´¢ â†’ æ¯”è¼ƒ Prompt â†’ LLM åˆ†æ â†’ çµæ§‹åŒ–è¼¸å‡º
       ```
    
    3. **Metadata éæ¿¾**
       - ä½¿ç”¨ `filter={"source_name": doc_name}` ç²¾ç¢ºæª¢ç´¢
       - é¿å…ä¸åŒæ–‡æª”é–“çš„è³‡è¨Šæ··æ·†
    
    4. **å°ˆé–€çš„æ¯”è¼ƒ Prompt**
       - å¼•å° AI å¾å¤šå€‹è§’åº¦åˆ†æ
       - ç”¢ç”Ÿçµæ§‹åŒ–çš„æ¯”è¼ƒçµæœ
    
    **èˆ‡æ¡ˆä¾‹ 1 çš„å·®ç•°**ï¼š
    - æ¡ˆä¾‹ 1ï¼šå–®ä¸€å•ç­”ï¼Œfocus åœ¨æª¢ç´¢æº–ç¢ºæ€§
    - æ¡ˆä¾‹ 2ï¼šå¤šæ–‡æª”æ¯”è¼ƒï¼Œfocus åœ¨ä¸¦è¡Œè™•ç†å’Œæ¯”è¼ƒåˆ†æ
    
    **å¯¦éš›æ‡‰ç”¨å ´æ™¯**ï¼š
    - ç”¢å“é¸è³¼æ±ºç­–æ”¯æ´
    - åˆç´„æ¢æ¬¾æ¯”å°
    - æ”¿ç­–è¦ç¯„åˆ†æ
    - ç«¶å“åˆ†æå ±å‘Š
    - æŠ€è¡“æ–¹æ¡ˆè©•ä¼°
    """)

if __name__ == "__main__":
    demo.launch(share=False, server_name="0.0.0.0", server_port=7861)

